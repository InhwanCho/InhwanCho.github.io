<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Tag: Deeplearning,Study,Pytorch - Inhwan&#039;s Digital Space</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Digital Space"><meta name="msapplication-TileImage" content="/img/favicon-32x32.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Digital Space"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="I&amp;#39;m Learning ML&amp;#x2F;DL"><meta property="og:type" content="blog"><meta property="og:title" content="Inhwan&#039;s Digital Space"><meta property="og:url" content="http://inhwancho.github.io/"><meta property="og:site_name" content="Inhwan&#039;s Digital Space"><meta property="og:description" content="I&amp;#39;m Learning ML&amp;#x2F;DL"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://inhwancho.github.io/img/og_image.png"><meta property="article:author" content="InhwanCho"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://inhwancho.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://InhwanCho.github.io"},"headline":"Inhwan's Digital Space","image":["http://inhwancho.github.io/img/og_image.png"],"author":{"@type":"Person","name":"InhwanCho"},"publisher":{"@type":"Organization","name":"Inhwan's Digital Space","logo":{"@type":"ImageObject","url":{"text":"Inhwan's Digital Space"}}},"description":"I&#39;m Learning ML&#x2F;DL"}</script><link rel="icon" href="/img/favicon-32x32.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/github.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-8RGKYVDD5B" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-8RGKYVDD5B');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/rss2.xml" title="Inhwan's Digital Space" type="application/rss+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Inhwan&#039;s Digital Space</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="external nofollow noopener noreferrer" title="Download on GitHub" href="https://github.com/InhwanCho"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-7-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">Deeplearning,Study,Pytorch</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-9 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time datetime="2023-01-08T15:00:00.000Z" title="Invalid Date">2023-01-09</time></span><span class="level-item">Updated&nbsp;<time datetime="2023-01-08T15:00:00.000Z" title="2023. 1. 9. 오전 12:00:00">2023-01-09</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">a minute read (About 141 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/01/09/Study_folder/Pytorch/2023-01-09-model-info/">파이토치에서 model 정보(summary) 확인</a></h1><div class="content"><h2 id="파이토치에서-만들어진-모델-정보-확인하기"><a href="#파이토치에서-만들어진-모델-정보-확인하기" class="headerlink" title="파이토치에서 만들어진 모델 정보 확인하기"></a>파이토치에서 만들어진 모델 정보 확인하기</h2><ul>
<li>keras에서는 <code>model.summary()</code>의 내장 함수를 이용하면 간단하게 모델 정보를 확인 가능합니다.</li>
<li>파이토치에서도 여러 가지 방법을 통해 정보를 확인할 수 있습니다.</li>
</ul>
<h3 id="1-print를-하면-summary가-출력된다"><a href="#1-print를-하면-summary가-출력된다" class="headerlink" title="1. print를 하면 summary가 출력된다"></a>1. <code>print</code>를 하면 summary가 출력된다</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>

<img width="943" alt="스크린샷 2023-01-09 오후 2 47 20" src="https://user-images.githubusercontent.com/111936229/211247463-dffbfede-2625-4d50-b1f7-e48369abcc2f.png">


<h3 id="2-torchinfo-통해-summary를-출력한다"><a href="#2-torchinfo-통해-summary를-출력한다" class="headerlink" title="2. torchinfo 통해 summary를 출력한다"></a>2. <code>torchinfo</code> 통해 summary를 출력한다</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">!pip install torchinfo</span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"></span><br><span class="line">summary(model)</span><br></pre></td></tr></table></figure>

<img width="719" alt="스크린샷 2023-01-09 오후 2 47 04" src="https://user-images.githubusercontent.com/111936229/211247503-5e2ae136-14ec-4091-b65f-1cec173f4982.png">


<h3 id="3-torchsummary를-통해-summary출력-input-size를-알아야만-출력-가능"><a href="#3-torchsummary를-통해-summary출력-input-size를-알아야만-출력-가능" class="headerlink" title="3. torchsummary를 통해 summary출력 (input_size를 알아야만 출력 가능)"></a>3. <code>torchsummary</code>를 통해 summary출력 (input_size를 알아야만 출력 가능)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">!pip install torchsummary</span><br><span class="line"><span class="keyword">from</span> torchsummary <span class="keyword">import</span> summary</span><br><span class="line"></span><br><span class="line">summary(model, input_size = (<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>), batch_size= <span class="number">6</span>)</span><br></pre></td></tr></table></figure>

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-9 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time datetime="2023-01-07T15:00:00.000Z" title="Invalid Date">2023-01-08</time></span><span class="level-item">Updated&nbsp;<time datetime="2023-01-07T15:00:00.000Z" title="2023. 1. 8. 오전 12:00:00">2023-01-08</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">a few seconds read (About 64 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/01/08/Study_folder/Pytorch/2023-01-08-boolean/">torch를 boolean값으로 변경</a></h1><div class="content"><h2 id="operation-참조"><a href="#operation-참조" class="headerlink" title="operation 참조"></a>operation 참조</h2><ul>
<li>예시를 통해 알아보겠습니다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x&gt;<span class="number">2</span>)</span><br><span class="line"><span class="comment"># tensor([False, False,  True,  True])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>((x&gt;<span class="number">2</span>).<span class="built_in">type</span>(torch.float32))</span><br><span class="line"><span class="comment"># tensor([0., 0., 1., 1.])</span></span><br></pre></td></tr></table></figure>

<h2 id="gt-함수를-활용"><a href="#gt-함수를-활용" class="headerlink" title="gt() 함수를 활용"></a>gt() 함수를 활용</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.gt(<span class="number">2</span>))</span><br><span class="line"><span class="comment"># tensor([False, False,  True,  True])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.gt(<span class="number">2</span>).to(torch.int32))</span><br><span class="line"><span class="comment"># tensor([0, 0, 1, 1], dtype=torch.int32)</span></span><br></pre></td></tr></table></figure></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-9 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time datetime="2023-01-06T15:00:00.000Z" title="Invalid Date">2023-01-07</time></span><span class="level-item">Updated&nbsp;<time datetime="2023-01-06T15:00:00.000Z" title="2023. 1. 7. 오전 12:00:00">2023-01-07</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">a minute read (About 119 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/01/07/Study_folder/Pytorch/2023-01-07-tril/">torch.tril</a></h1><div class="content"><h2 id="torch-tril"><a href="#torch-tril" class="headerlink" title="torch.tril"></a>torch.tril</h2><ul>
<li><p><code>torch.tril(input, diagonal=0, *, out=None)</code></p>
</li>
<li><p>행렬의 아래쪽 삼각형 부분 (2 차원 텐서) 또는 행렬의 배치 input 을 반환합니다 .[행렬의 오른쪽 부분을(0으로 만듬)]</p>
</li>
<li><p>attention 구조의 mask를 만들 때 많이 사용되는 함수입니다.</p>
</li>
<li><p>무슨 말인지 이해가 잘 안가실겁니다. 예제 출력 코드를 보면 바로 이해가 갈겁니다.</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones((<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">torch.tril(a)</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = torch.ones((<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">torch.tril(a, diagonal=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-9 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time datetime="2023-01-06T15:00:00.000Z" title="Invalid Date">2023-01-07</time></span><span class="level-item">Updated&nbsp;<time datetime="2023-01-06T15:00:00.000Z" title="2023. 1. 7. 오전 12:00:00">2023-01-07</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">2 minutes read (About 245 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/01/07/Study_folder/Pytorch/2023-01-07-nn.Embedding/">nn.Embedding</a></h1><div class="content"><h2 id="nn-Embedding"><a href="#nn-Embedding" class="headerlink" title="nn.Embedding"></a>nn.Embedding</h2><ul>
<li><p>출처 : &lt;<a target="_blank" rel="external nofollow noopener noreferrer" href="https://wikidocs.net/64779">위키독스</a>&gt;</p>
</li>
<li><p>임베딩 층(embedding layer)을 만들어 훈련 데이터로부터 처음부터 임베딩 벡터를 학습하는 방법을 <code>nn.Embedding</code>을 이용하여 구현합니다.</p>
</li>
<li><p>주요 파라미터는 2개입니다.</p>
<ul>
<li>num_embeddings : 임베딩을 할 단어들의 개수. (단어 집합의 크기)</li>
<li>embedding_dim : 임베딩 할 벡터의 차원입니다. (사용자 정의)</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">train_data = <span class="string">&#x27;we can do lots of things like climbing do&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 중복을 제거한 단어들의 집합인 단어 집합 생성.(num_embeddings 인자)</span></span><br><span class="line">word_set = <span class="built_in">set</span>(train_data.split())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 단어 집합의 각 단어에 고유한 정수 맵핑.</span></span><br><span class="line">vocab = &#123;tkn: i+<span class="number">2</span> <span class="keyword">for</span> i, tkn <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_set)&#125;</span><br><span class="line">vocab[<span class="string">&#x27;&lt;unk&gt;&#x27;</span>] = <span class="number">0</span></span><br><span class="line">vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>] = <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(vocab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># &#123;&#x27;things&#x27;: 2, &#x27;lots&#x27;: 3, &#x27;can&#x27;: 4, &#x27;like&#x27;: 5, &#x27;do&#x27;: 6, &#x27;climbing&#x27;: 7, &#x27;of&#x27;: 8, &#x27;we&#x27;: 9, &#x27;&lt;unk&gt;&#x27;: 0, &#x27;&lt;pad&gt;&#x27;: 1&#125;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 단어 집합의 크기의 행을 가지는 임베딩 테이블 생성</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">embedding_layer = nn.Embedding(num_embeddings=<span class="built_in">len</span>(vocab), </span><br><span class="line">                               embedding_dim=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">embedding_layer</span><br><span class="line"><span class="comment"># Embedding(10, 3, padding_idx=1)</span></span><br><span class="line">````</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="built_in">print</span>(embedding_layer.weight)</span><br><span class="line"></span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.2915</span>,  <span class="number">0.8197</span>,  <span class="number">0.1080</span>],</span><br><span class="line">        [ <span class="number">0.4103</span>,  <span class="number">1.2429</span>, -<span class="number">0.7658</span>],</span><br><span class="line">        [ <span class="number">0.4185</span>, -<span class="number">0.0410</span>,  <span class="number">2.1945</span>],</span><br><span class="line">        [-<span class="number">0.9706</span>, -<span class="number">0.6196</span>, -<span class="number">1.3778</span>],</span><br><span class="line">        [-<span class="number">1.8044</span>, -<span class="number">0.8070</span>, -<span class="number">1.0277</span>],</span><br><span class="line">        [ <span class="number">0.7752</span>, -<span class="number">0.1011</span>,  <span class="number">1.5459</span>],</span><br><span class="line">        [ <span class="number">0.2195</span>,  <span class="number">1.2008</span>,  <span class="number">0.1253</span>],</span><br><span class="line">        [ <span class="number">0.6568</span>,  <span class="number">1.3255</span>,  <span class="number">0.5347</span>],</span><br><span class="line">        [-<span class="number">1.2790</span>,  <span class="number">0.3015</span>, -<span class="number">0.2819</span>],</span><br><span class="line">        [-<span class="number">0.0371</span>, -<span class="number">0.0291</span>, -<span class="number">0.2894</span>]], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-9 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time datetime="2023-01-05T15:00:00.000Z" title="Invalid Date">2023-01-06</time></span><span class="level-item">Updated&nbsp;<time datetime="2023-01-05T15:00:00.000Z" title="2023. 1. 6. 오전 12:00:00">2023-01-06</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">a minute read (About 161 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/01/06/Study_folder/Pytorch/2023-01-06-squeeze/">Squeeze, Unsqueeze</a></h1><div class="content"><h2 id="squeeze"><a href="#squeeze" class="headerlink" title="squeeze"></a>squeeze</h2><blockquote>
<p>squeeze와 unsqueeze는 1인 차원을 제거, 생성할때 매우 유용한 함수이다.</p>
</blockquote>
<ul>
<li>squeeze는 차원이 1인 차원을 제거해준다.(default값)</li>
<li>차원을 설정해주면 그 차원만 제거한다.(1인 차원만 제거되니 참고)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.rand(<span class="number">3</span>, <span class="number">20</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"><span class="comment"># torch.Size([3, 20, 1, 1])</span></span><br><span class="line"></span><br><span class="line">x = x.squeeze()</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"><span class="comment"># torch.Size([3, 20])</span></span><br><span class="line"></span><br><span class="line">x = torch.squeeze(x, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"><span class="comment"># torch.Size([3, 20])</span></span><br></pre></td></tr></table></figure>

<h2 id="unsqueeze"><a href="#unsqueeze" class="headerlink" title="unsqueeze"></a>unsqueeze</h2><ul>
<li>unsqueeze는 차원이 1인 차원을 생성해준다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">3</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"><span class="comment"># torch.Size([3, 20, 30, 40])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = x.unsqueeze(dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"><span class="comment"># torch.Size([3, 1, 20, 30, 40])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = torch.rand(<span class="number">3</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"><span class="comment"># torch.Size([3, 20, 30, 40])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = torch.unsqueeze(x,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"><span class="comment"># torch.Size([3, 1, 20, 30, 40])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = torch.rand(<span class="number">3</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"><span class="comment"># torch.Size([3, 20, 30, 40])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = x.unsqueeze(dim = <span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"><span class="comment"># torch.Size([1, 3, 20, 30, 40])</span></span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-9 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time datetime="2023-01-01T15:00:00.000Z" title="Invalid Date">2023-01-02</time></span><span class="level-item">Updated&nbsp;<time datetime="2023-01-01T15:00:00.000Z" title="2023. 1. 2. 오전 12:00:00">2023-01-02</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">4 minutes read (About 533 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/01/02/Study_folder/Pytorch/2023-01-02-dim-shape/">파이토치 차원 변경(reshape,view,permute) 비교</a></h1><div class="content"><h2 id="3가지-함수의-공통점"><a href="#3가지-함수의-공통점" class="headerlink" title="3가지 함수의 공통점"></a>3가지 함수의 공통점</h2><ul>
<li>reshape, view, permute는 모두 텐서의 형태(차원을 바꾸는) 기능이다.</li>
</ul>
<h2 id="view와-reshape"><a href="#view와-reshape" class="headerlink" title="view와 reshape"></a>view와 reshape</h2><ul>
<li>view로 반환된 tensor는 원본 tensor와 기반이 되는 data를 공유한다. 만약 반환된 tensor의 값이 변경된다면, viewed되는 tensor에서 해당하는 값이 변경된다.</li>
<li>view와 reshape은 연산 자체는 같고 기존 tensor가 변경되냐 되지 않는지의 차이가 있다.(contiguous)</li>
<li>view는 붙어있는 차원 떼어낼 때 사용하면 용이하다. [A*2, B, C] -&gt; [A, 2, B, C]</li>
<li>view는 contiguous한 함수에서만 사용 가능하다.</li>
<li>reshape은 contiguous 하지 않는 함수에서도 작동한다.</li>
</ul>
<h2 id="permute-input-dims"><a href="#permute-input-dims" class="headerlink" title="permute(input, dims)"></a>permute(input, dims)</h2><ul>
<li>permute의 parameters에는 input&#x3D;tensor, dims &#x3D; index값을 입력한다.</li>
<li>numpy의 transpose는 두 개의 차원을 맞교환할 수 있다. 그러나 permute()는 모든 차원들을 변경할 수 있다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># tensor([[[ 0.5676,  0.4593, -0.2802, -1.9395, -1.2933],</span></span><br><span class="line"><span class="comment">#          [-0.6459, -1.0244, -0.3164,  1.5379,  0.9390],</span></span><br><span class="line"><span class="comment">#          [-0.2366, -1.4711, -0.6014, -0.4424,  0.1915]],</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         [[-1.3794, -0.4103,  0.1862,  0.2172,  0.3282],</span></span><br><span class="line"><span class="comment">#          [-1.9206,  1.1295, -0.4130, -0.5630,  0.9670],</span></span><br><span class="line"><span class="comment">#          [-1.0632,  0.7054,  0.4358, -0.4522, -0.3983]]])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.size())</span><br><span class="line"><span class="comment"># torch.Size([2, 3, 5])</span></span><br><span class="line"></span><br><span class="line">torch.permute(x, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)).size() <span class="comment">#== x.permute(2, 0, 1)</span></span><br><span class="line"><span class="comment"># torch.Size([5, 2, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># transpose는 두 개의 차원을 맞교환</span></span><br><span class="line">x.tranpose(<span class="number">0</span>, <span class="number">3</span>) <span class="comment"># 0과 3의 차원을 변경(교환)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="contiguous란"><a href="#contiguous란" class="headerlink" title="contiguous란"></a>contiguous란</h2><ul>
<li>사전적 의미는 ‘접촉하는’ 이다.</li>
<li>contiguous 란 matrix 의 눈에 보이는 형태(shape) 과 실제 matrix 의 각 데이터가 저장된 위치가 같은지의 여부이다.</li>
</ul>
<h2 id="unsqueeze-squeeze"><a href="#unsqueeze-squeeze" class="headerlink" title="unsqueeze, squeeze"></a>unsqueeze, squeeze</h2><ul>
<li>batch가 1일 때 batch차원도 없애버릴 수 있기때문에 이 경우를 유의해서 사용해야한다.</li>
</ul>
<blockquote>
<p>1차원을 생성 및 제거를 할 경우 unsqueeze, squeeze함수가 편하다.</p>
</blockquote>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li>view : tensor 에 저장된 데이터의 물리적 위치 순서와 index 순서가 일치할 때 (contiguous) shape을 재구성한다. 이 때문에 항상 contiguous 하다는 성질이 보유된다.</li>
<li>reshape : 순서가 일치하지 않아도 shape을 재구성한 이후에 강제로 일치시킨다. 이 때문에 항상 contiguous 하다는 성질이 보유된다.</li>
<li>transpose : tensor 의 index 순서는 같다는 보장이 없으므로 항상 contiguous 하지 않다.</li>
<li>permute : contiguous 하지 않다.</li>
<li>transpose와 permute는 transpose.contiguous()이런식으로 사용 가능하다.</li>
<li>einsum : 여러 연산을 통해 차원 관리를 쉽게 할 수 있다.</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-9 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time datetime="2022-12-29T15:00:00.000Z" title="Invalid Date">2022-12-30</time></span><span class="level-item">Updated&nbsp;<time datetime="2022-12-29T15:00:00.000Z" title="2022. 12. 30. 오전 12:00:00">2022-12-30</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">2 minutes read (About 293 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/12/30/Study_folder/Pytorch/2022-12-30-torch-basic/">파이토치 기본 정보들</a></h1><div class="content"><h2 id="파이토치-기본-함수들"><a href="#파이토치-기본-함수들" class="headerlink" title="파이토치 기본 함수들"></a>파이토치 기본 함수들</h2><ul>
<li><p>torch.autograd</p>
<ul>
<li>자동 미분을 위한 함수들이 포함되어져 있습니다.</li>
</ul>
</li>
<li><p>torch.nn</p>
<ul>
<li>신경망을 구축하기 위한 다양한 데이터 구조나 레이어 등이 정의되어져 있습니다.<br>RNN, LSTM과 같은 레이어, ReLU와 같은 활성화 함수, MSELoss와 같은 손실 함수들이 있습니다.</li>
</ul>
</li>
<li><p>torch.optim</p>
<ul>
<li>확률적 경사 하강법(Stochastic Gradient Descent, SGD)를 중심으로 한 파라미터 최적화 알고리즘이 구현되어져 있습니다.</li>
</ul>
</li>
<li><p>torch.utils.data</p>
<ul>
<li>Dataset, Dataloader등의 함수가 내장되어 있습니다.</li>
</ul>
</li>
<li><p>torch.onnx</p>
<ul>
<li>ONNX(Open Neural Network Exchange)의 포맷으로 모델을 익스포트(export)할 때 사용합니다. ONNX는 텐서플로같은 다른 딥러닝 프레임워크간의 교류에 필요한 함수입니다.</li>
</ul>
</li>
</ul>
<h2 id="파이토치-정보들"><a href="#파이토치-정보들" class="headerlink" title="파이토치 정보들"></a>파이토치 정보들</h2><ul>
<li>optimizer.zero_grad()가 필요한 이유<ul>
<li><p>파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시키는 특징이 있어서 값을 초기화 시켜줄 필요가 있음</p>
</li>
<li><p>backward()는 기울기를 계산하는 함수</p>
</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-9 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time datetime="2022-12-02T15:00:00.000Z" title="Invalid Date">2022-12-03</time></span><span class="level-item">Updated&nbsp;<time datetime="2022-12-03T15:00:00.000Z" title="2022. 12. 4. 오전 12:00:00">2022-12-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">5 minutes read (About 785 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/12/03/Study_folder/Pytorch/2022-12-03-nn.Module(class)/">Pytorch nn.Module 클래스(상속) 파악하기!!</a></h1><div class="content"><h2 id="Pytorch에서-클래스로-모듈-불러오기"><a href="#Pytorch에서-클래스로-모듈-불러오기" class="headerlink" title="Pytorch에서 클래스로 모듈 불러오기"></a>Pytorch에서 클래스로 모듈 불러오기</h2><ul>
<li>파이토치에서 대부분 모델을 생성할 때 클래스(Class)를 사용하고 있습니다.<ul>
<li>아래의 예시에서는 <code>nn.Module</code>클래스의 <code>BERTClassifier</code>를 상속 받는 모델입니다.</li>
<li><code>__init__()</code>에서 모델의 구조와 동작을 정의하는 생성자를 정의합니다. 이는 파이썬에서 객체가 갖는 속성값을 초기화하는 역할로, 객체가 생성될 때 자동으호 호출됩니다. <code>super()</code> 함수를 부르면 여기서 만든 클래스는 nn.Module 클래스의 속성들을 가지고 초기화 됩니다. <code>foward()</code> 함수는 모델이 학습데이터를 입력받아서 forward 연산을 진행시키는 함수입니다. 이 <code>forward()</code> 함수는 model 객체를 데이터와 함께 호출하면 자동으로 실행이됩니다. <code>nn.Module</code>클래스에서 그렇게 만들어졌기에 그렇습니다.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 간단한 예시</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="number">3</span>, <span class="number">1</span>) <span class="comment"># input_dim=3, output_dim=1.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(x)</span><br><span class="line"><span class="comment">######</span></span><br><span class="line">model = MultiLinear()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-5</span>) </span><br><span class="line">nb_epochs = <span class="number">200</span></span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure>

<h2 id="KoBERT-분류기-클래스로-생성"><a href="#KoBERT-분류기-클래스로-생성" class="headerlink" title="KoBERT 분류기 클래스로 생성"></a>KoBERT 분류기 클래스로 생성</h2><ul>
<li><code>어텐션 마스크(attention mask)</code>는 BERT가 어텐션 연산을 할 때, 불필요하게 패딩 토큰에 대해서 어텐션을 하지 않도록 실제 단어와 패딩 토큰을 구분할 수 있도록 알려주는 입력입니다. 0과 1으로 이루어져있는데, 숫자 0은 해당 토큰은 패딩 토큰이므로 마스킹을 한다는 의미이고 숫자 1은 해당 토큰은 실제 단어이므로 마스킹을 하지 않는다라는 의미입니다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#KoBERT 학습모델 생성</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">KoBERTClassifier</span>(nn.Module):    <span class="comment">## nn.Module 클래스를 상속</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 bert,              <span class="comment">#bert모델</span></span></span><br><span class="line"><span class="params">                 hidden_size = <span class="number">768</span>, <span class="comment">#히든사이즈</span></span></span><br><span class="line"><span class="params">                 num_classes=<span class="number">6</span>,     <span class="comment">#클래스 수</span></span></span><br><span class="line"><span class="params">                 dr_rate=<span class="literal">None</span></span>):     <span class="comment">#dropout</span></span><br><span class="line">        <span class="built_in">super</span>(BERTClassifier, self).__init__() <span class="comment">#super함수 생성 시 self가 뒤에 와야한다.</span></span><br><span class="line">        self.bert = bert</span><br><span class="line">        self.dr_rate = dr_rate</span><br><span class="line"></span><br><span class="line">        <span class="comment">#inputsize, outputsize의 Linearmodel 만드는 함수     </span></span><br><span class="line">        self.classifier = nn.Linear(hidden_size , num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#dropout옵션을 사용한다면 아래 함수 생성</span></span><br><span class="line">        <span class="keyword">if</span> dr_rate:</span><br><span class="line">            self.dropout = nn.Dropout(p=dr_rate)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#어텐션마스크 생성을 위한 함수</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">gen_attention_mask</span>(<span class="params">self, token_ids, valid_length</span>):</span><br><span class="line">        attention_mask = torch.zeros_like(token_ids)</span><br><span class="line">        <span class="keyword">for</span> i, v <span class="keyword">in</span> <span class="built_in">enumerate</span>(valid_length):</span><br><span class="line">            attention_mask[i][:v] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> attention_mask.<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, token_ids, valid_length, segment_ids</span>):</span><br><span class="line">        <span class="comment">#어텐션마스크 생성</span></span><br><span class="line">        attention_mask = self.gen_attention_mask(token_ids, valid_length)</span><br><span class="line">        </span><br><span class="line">        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.<span class="built_in">float</span>().to(token_ids.device))</span><br><span class="line"></span><br><span class="line">        <span class="comment">#dropout옵션을 사용한다면 아래 함수 생성</span></span><br><span class="line">        <span class="keyword">if</span> self.dr_rate:</span><br><span class="line">            out = self.dropout(pooler)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.classifier(out)</span><br><span class="line"></span><br><span class="line"><span class="comment">######## 사용 예시 ########</span></span><br><span class="line">bertmodel, vocab = get_pytorch_kobert_model()</span><br><span class="line"></span><br><span class="line">model = KoBERTClassifier(bertmodel,  dr_rate=<span class="number">0.5</span>).to(device)</span><br></pre></td></tr></table></figure>

<h2 id="코드-분해하여-원리-파악하기"><a href="#코드-분해하여-원리-파악하기" class="headerlink" title="코드 분해하여 원리 파악하기"></a>코드 분해하여 원리 파악하기</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Linear(in_features,out_features,bias = <span class="literal">True</span>, device = <span class="literal">None</span>,dtype = <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train_dataloader의 리스트는</span></span><br><span class="line"><span class="comment"># [[token_ids], [valid_length], [segment_ids], [label]] 의 순서로 이루어져 있습니다.</span></span><br><span class="line"><span class="comment"># for i ,(token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader): 이런식으로 사용.</span></span><br></pre></td></tr></table></figure></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-9 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time datetime="2022-12-01T15:00:00.000Z" title="Invalid Date">2022-12-02</time></span><span class="level-item">Updated&nbsp;<time datetime="2022-12-04T15:00:00.000Z" title="2022. 12. 5. 오전 12:00:00">2022-12-05</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">6 minutes read (About 848 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/12/02/Study_folder/Pytorch/2022-12-02-pytorch-Dataset(class)/">Pytorch Dataset 클래스(상속) 파악하기!!</a></h1><div class="content"><h2 id="BERT-Dataset"><a href="#BERT-Dataset" class="headerlink" title="BERT_Dataset"></a>BERT_Dataset</h2><ul>
<li>데이터를 정제하였다면, 각 데이터가 KoBERT 모델의 입력으로 들어갈 수 있는 형태가 되도록 토큰화, 정수 인코딩, 패딩 등을 해주어야 한다. 아래는 그를 수행할 클래스이다.</li>
<li>Dataset을 상속 받는 클래스의 구성</li>
<li>사용자 정의 Dataset 클래스는 반드시 3개 함수를 구현해야 합니다: <br><br>1.<code>__init__</code>,<br>2.<code>__len__</code>,<br>3.<code>__getitem__</code></li>
<li><code>__init__</code> 함수는 Dataset 객체가 생성(instantiate)될 때 한 번만 실행됩니다. </li>
<li><code>__len__</code> 함수는 데이터셋의 샘플 개수를 반환합니다.<br>len()함수를 사용 시 반환값이라고 생각하시면 됩니다.</li>
<li><code>__getitem__</code> 함수는 클래스의 인덱스에 접근할 때 자동으로 호출되는 메서드(함수)이다.<br>쉽게 표현하면 슬라이싱을 구현하려면 필요한 것은 <code>__getitem__</code>라는 메소드!</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> gluonnlp <span class="keyword">as</span> nlp</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BERT_Dataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># __init__ 함수는 Dataset 객체가 생성(instantiate)될 때 한 번만 실행됩니다. </span></span><br><span class="line">    <span class="comment"># 여기서는 dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair)를 불러옵니다.</span></span><br><span class="line">    <span class="comment"># 함수를 호출하게되면 nlp.data.BERTSentenceTransform을 하고, sentences와 labels의 리스트를 만들게 됩니다.</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,</span></span><br><span class="line"><span class="params">                 pad, pair</span>):</span><br><span class="line">        transform = nlp.data.BERTSentenceTransform(</span><br><span class="line">            bert_tokenizer, max_seq_length = max_len, pad = pad, pair = pair)</span><br><span class="line"></span><br><span class="line">        self.sentences = [transform([i[sent_idx]]) <span class="keyword">for</span> i <span class="keyword">in</span> dataset]</span><br><span class="line">        self.labels = [np.int32(i[label_idx]) <span class="keyword">for</span> i <span class="keyword">in</span> dataset]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 인덱스를 기반으로 문장과 라벨을 반환합니다. return에 슬라이싱 &quot;[]&quot;이 필요함.</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>):</span><br><span class="line">        <span class="keyword">return</span> (self.sentences[i] + (self.labels[i], ))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># __len__ 함수는 데이터셋의 레이블 개수를 반환합니다.</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> (<span class="built_in">len</span>(self.labels))</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#사용하기</span></span><br><span class="line">tokenizer = get_tokenizer()</span><br><span class="line">token = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 이런식으로 만든 함수를 사용합니다.</span></span><br><span class="line">data_train = BERTDataset(dataset = train_set_data, sent_idx = <span class="number">0</span>, label_idx = <span class="number">1</span>, bert_tokenizer = token, max_len = <span class="number">64</span>, pad = <span class="literal">True</span>, pair = <span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 아니면 아래와 같은 형식으로 &#x27;=&#x27; 기호를 빼고 사용하셔도 결과는 같습니다.</span></span><br><span class="line">data_train = BERTDataset(train_set_data, <span class="number">0</span>, <span class="number">1</span>, token, max_len, <span class="literal">True</span>, <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h2 id="코드-분해하여-원리-파악하기"><a href="#코드-분해하여-원리-파악하기" class="headerlink" title="코드 분해하여 원리 파악하기"></a>코드 분해하여 원리 파악하기</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> kobert.utils <span class="keyword">import</span> get_tokenizer</span><br><span class="line"><span class="keyword">import</span> gluonnlp <span class="keyword">as</span> nlp</span><br><span class="line">tokenizer = get_tokenizer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bertmodel, vocab = get_pytorch_kobert_model()</span><br><span class="line">token = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">data_train = BERTDataset(dataset = train_set_data, sent_idx = <span class="number">0</span>, label_idx = <span class="number">1</span>, bert_tokenizer = token, max_len = <span class="number">64</span>, pad = <span class="literal">True</span>, pair = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># data를 리스트[0]에는 문장을 넣고, 리스트[1]에는 감정(labels)으로 넣어서 리스트로 만들어줌.</span></span><br><span class="line">train_set_data = [[i, <span class="built_in">str</span>(j)] <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">zip</span>(train_set[<span class="string">&#x27;data&#x27;</span>], train_set[<span class="string">&#x27;label&#x27;</span>])]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(train_set_data[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 출력 결과 =&gt; [&#x27;큰아들이 결혼하는데 집을 사달라고 해서 화가 나.&#x27;, &#x27;4&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(data_train[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 출력 결과 =&gt;</span></span><br><span class="line">(array([   <span class="number">2</span>, <span class="number">4688</span>, <span class="number">6797</span>, <span class="number">5940</span>,  <span class="number">950</span>, <span class="number">7795</span>, <span class="number">4384</span>, <span class="number">7088</span>, <span class="number">2573</span>, <span class="number">5794</span>, <span class="number">5439</span>,</span><br><span class="line">        <span class="number">5007</span>, <span class="number">5112</span>, <span class="number">5330</span>, <span class="number">1370</span>,  <span class="number">517</span>,   <span class="number">54</span>,    <span class="number">3</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">           <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">           <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">           <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">           <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>], dtype=int32),</span><br><span class="line"> array(<span class="number">18</span>, dtype=int32),</span><br><span class="line"> array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,</span><br><span class="line">        <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,</span><br><span class="line">        <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       dtype=int32),</span><br><span class="line"> <span class="number">4</span>)</span><br><span class="line"><span class="comment"># array의 첫 번째는 패딩된 시퀀스, 두 번째는 길이와 타입에 대한 내용, 세 번재는 어텐션 마스크 시퀀스이다.</span></span><br></pre></td></tr></table></figure>

<ul>
<li>아래는 <code>from kobert.pytorch_kobert import get_pytorch_kobert_model</code>의 <code>get_kobert_model</code>함수 형식입니다.</li>
<li><blockquote>
<p><code>bertmodel, vocab = get_pytorch_kobert_model()</code>을<br> 실행 시 에러가 나온다면 <br><code>!pip install sentencepiece==0.1.91</code><br> <code>!pip install transformers==4.8.2</code><br>버전을 맞춰주면 해결이 될 수도 있습니다.</p>
</blockquote>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_kobert_model</span>(<span class="params">model_path, vocab_file, ctx=<span class="string">&quot;cpu&quot;</span></span>):</span><br><span class="line">    bertmodel = BertModel.from_pretrained(model_path)</span><br><span class="line">    device = torch.device(ctx)</span><br><span class="line">    bertmodel.to(device)</span><br><span class="line">    bertmodel.<span class="built_in">eval</span>()</span><br><span class="line">    vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(vocab_file,</span><br><span class="line">                                                         padding_token=<span class="string">&#x27;[PAD]&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> bertmodel, vocab_b_obj</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>KoBERT 함수 출처&lt;<a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/SKTBrain/KoBERT/blob/8df69ec6b588ae661bef98d28ec29448482bbe6e/kobert/pytorch_kobert.py#L5">KoBERT</a>&gt;</li>
</ul>
</div></article></div></div><div class="column column-left is-3-tablet is-3-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.png" alt="Inhwan Cho"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Inhwan Cho</p><p class="is-size-6 is-block">Enjoy the life</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Seoul in Korea</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">101</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">12</p></a></div></div></nav><div class="level"><a class="level-item button is-info is-outlined is-rounded" href="/" target="_self" rel="noopener">Home</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="external nofollow noopener noreferrer" title="Github" href="https://github.com/InhwanCho"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="external nofollow noopener noreferrer" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Blog/"><span class="level-start"><span class="level-item">Blog</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Colab/"><span class="level-start"><span class="level-item">Colab</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Command/"><span class="level-start"><span class="level-item">Command</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Errors/"><span class="level-start"><span class="level-item">Errors</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/Git/"><span class="level-start"><span class="level-item">Git</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Mac/"><span class="level-start"><span class="level-item">Mac</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/OpenCV/"><span class="level-start"><span class="level-item">OpenCV</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Portfolio/"><span class="level-start"><span class="level-item">Portfolio</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Project/"><span class="level-start"><span class="level-item">Project</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Review/"><span class="level-start"><span class="level-item">Review</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Samples/"><span class="level-start"><span class="level-item">Samples</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Study/"><span class="level-start"><span class="level-item">Study</span></span><span class="level-end"><span class="level-item tag">60</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time datetime="2023-01-23T01:35:35.285Z">2023-01-23</time></p><p class="title"><a href="/2023/01/23/Portfolio/2023-01-23-visual-project/">탄소 중립을 위한 기후 기술 정보 시각화(프로젝트)</a></p><p class="categories"><a href="/categories/Portfolio/">Portfolio</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2023-01-20T02:22:22.012Z">2023-01-20</time></p><p class="title"><a href="/2023/01/20/Colab_folder/2023-01-20-kaggle-colab/">코랩(colab)에서 kaggle(캐글) 데이터 바로 다운받기</a></p><p class="categories"><a href="/categories/Colab/">Colab</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2023-01-19T04:16:37.117Z">2023-01-19</time></p><p class="title"><a href="/2023/01/19/Study_folder/OpneCV/2023-01-19-face-recognizion/">OpenCV로 아는 얼굴인지 확인하기</a></p><p class="categories"><a href="/categories/OpenCV/">OpenCV</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2023-01-18T06:23:12.874Z">2023-01-18</time></p><p class="title"><a href="/2023/01/18/Study_folder/OpneCV/2023-01-18-haarscascade/">OpenCV 얼굴, 눈 등 특정 객체 검출</a></p><p class="categories"><a href="/categories/OpenCV/">OpenCV</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2023-01-18T05:11:30.071Z">2023-01-18</time></p><p class="title"><a href="/2023/01/18/Study_folder/OpneCV/2023-01-18-contour/">OpenCV 윤곽선 검출</a></p><p class="categories"><a href="/categories/OpenCV/">OpenCV</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">January 2023</span></span><span class="level-end"><span class="level-item tag">28</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">50</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">23</span></span></a></li></ul></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Inhwan&#039;s Digital Space</a><p class="is-size-7"><span>&copy; 2023 InhwanCho</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="external nofollow noopener noreferrer">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Download on GitHub" href="https://github.com/InhwanCho"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>