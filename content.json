{"posts":[{"title":"Colab Symbolic link(코랩 패키지 영구 설치)","text":"Google Colab에서 python 패키지를 영구적으로 설치하는 방법 구글드라이브로 패키지를 설치하고 읽어들이는 방법입니다. 아래와 같은 방식으로 설치를 하지 않을 경우, 런타임 초기화 마다 재설치를 해줘야 합니다. 혹시, 왼쪽 탐색기 탭을 통해 수동으로 마운트 한 경우, drive.mount('/content/drive')는 안하셔도 됩니다. 1.구글 코랩을 열고 마운트를 합니다. 1234import os, sysfrom google.colab import drive# 일단 마운트를 해줍니다(구글 드라이브와 연동)drive.mount('/content/drive') 2.새로운 폴더를 생성합니다. (my_env) 123if not os.path.exists('drive/My Drive/Colab Notebooks/my_env'): print('create directory.....') os.mkdir('drive/My Drive/Colab Notebooks/my_env') 3.새로운 경로를 설정합니다. 12345# 마운트 된 상태에서 해당 명령어를 실행합니다.my_path = '/content/notebooks'# Notebooks 폴더의 my_env 폴더에 패키지 저장os.symlink('/content/drive/My Drive/Colab Notebooks/my_env', my_path)sys.path.insert(0, my_path) 4.코랩에 원하는 패키지를 my_path 경로로 설치합니다. 12# konlpy를 설치하려면 이런식으로 설치하시면 됩니다.!pip install --target=$my_path konlpy (필수)재 실행 시 아래의 코드를 다시 실행합니다 테스트할 경우 새 탭이 아닌 새로운 윈도우에서 테스트해야 제대로 됩니다. 12345678910import os, sysfrom google.colab import drivedrive.mount('/content/drive')my_path = '/content/notebooks'os.symlink('/content/drive/My Drive/Colab Notebooks/my_env', my_path)sys.path.insert(0, my_path)import konlpy 코랩의 왼쪽 탐색기 탭의 /content/drive/My Drive/Colab Notebooks/my_env 폴더에서 다운 받은 파일이 잘 설치되어있는지 확인 가능합니다.","link":"/2022/11/22/Colab_folder/2022-11-22-Symbolic-Link/"},{"title":"Konply설치 in colab","text":"1. bash 셸로 명령어를 입력하여 설치(기본 설정이 zsh라도 bash 상관 없음)12345%%bashapt-get updateapt-get install g++ openjdk-8-jdk python-dev python3-devpip3 install JPype1pip3 install konlpy 12345### 또는 아래처럼 1개씩 입력해도 가능(위에거 문제없이 실행 시 스킵하세요)!apt-get update!apt-get install g++ openjdk-8-jdk python-dev python3-dev!pip3 install JPype1!pip3 install konlpy 2. 환경변수 설정1%env JAVA_HOME &quot;/usr/lib/jvm/java-8-openjdk-amd64&quot; 3. mecab 설치123%%bashbash &lt;(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)pip3 install /tmp/mecab-python-0.996 4. 확인12345678910111213import konlpyfrom konlpy.tag import Kkma, Komoran, Hannanum, Oktfrom konlpy.utils import pprintfrom konlpy.tag import Mecab###################mecab = Mecab()sentence = '안녕하세요 테스트용 텍스트 입니다.'temp_X = mecab.morphs(sentence)temp_X# 출력 결과['안녕', '하', '세요', '테스트', '용', '텍스트', '입니다', '.']","link":"/2022/11/22/Colab_folder/2022-11-22-konlpy-install/"},{"title":"nltk.download() in colab","text":"nltk.download()에서 리스트의 파일을 모두 다운 받고 싶은 경우123import nltknltk.download() 12345678910NLTK Downloader--------------------------------------------------------------------------- d) Download l) List u) Update c) Config h) Help q) Quit---------------------------------------------------------------------------Downloader&gt; dDownload which package (l=list; x=cancel)? Identifier&gt; alldownload which package에서 all을 입력하면 모두 설치가 가능합니다. 또는 1234# 런타임 초기화 될때마다 까는게 부담될때는(특정 파일만 설치를 원하는 경우)nltk.download('popular')# 다 깔고 싶을 경우nltk.download('all')","link":"/2022/11/22/Colab_folder/2022-11-22-nltk-download/"},{"title":"주피터노트북처럼 코랩에서 Tap키 사용하기","text":"Google Colab에서 Tap키 사용하기(자동완성) 도구 -&gt; 설정 -&gt; 편집기 -&gt; 코드 완성 제안을 자동으로 표시 클릭하여 해제하면 주피터노트북처럼 자동 완성 기능을 사용할 수 있습니다.","link":"/2022/11/24/Colab_folder/2022-11-23-setting-tap/"},{"title":"코랩(colab)에서 kaggle(캐글) 데이터 바로 다운받기","text":"코랩에서 캐글 데이터를 바로 다운받는 방법 kaggle 홈페이지의 오른쪽 프로필 -&gt; account -&gt; create new api token 누른 후 다운로드 kaggle.json 파일을 업로드 맥북 로컬은 ~/.kaggle에 파일을 옮겨서 사용하면 됩니다. in colab1234567# import kaggle # 요즘은 import kaggle을 안해도 됩니다.from google.colab import filesfiles.upload()# api token(kaggle.json 파일)을 '파일 선택' 눌러서 업로드 in colab12345678# .kaggle 폴더 생성!mkdir -p ~/.kaggle# json파일 .kaggle로 복사!cp kaggle.json ~/.kaggle/# Permission Warning이 발생하지 않도록 해줍니다.!chmod 600 ~/.kaggle/kaggle.json# 내가 참가한 대회 리스트 확인(옵션)# !kaggle competitions list 다운 받고 싶은 데이터의 API 주소를 복사하려면 밑의 스크린샷의 UTKFace 같은 데이터 셋 주소를 클릭합니다. 그 후 오른쪽 ...을 누르고 copy api command를 누릅니다. 12!kaggle datasets download -d jangedoo/utkface-new!ls","link":"/2023/01/20/Colab_folder/2023-01-20-kaggle-colab/"},{"title":"[Github Blog] 블로그 생성 시 에러 모음","text":"Github.io Blog Errors error : The process '/opt/hostedtoolcache/Ruby/3.1.2/x64/bin/bundle' failed with exit code 16 bundle lock –add-platform x86_64-linux &lt;터미널에 입력 error : --- layout: home # Index page --- (깃허브 페이지 Actions) .github/workflows 폴더 -&gt; pages-deploy.yml파일 열기 -&gt; branches에서 자기가 사용하는 브랜치로 변경 그래도 안되면 github -&gt; repo -&gt; setting -&gt; pages -&gt; source에서 GithubActions로 변경 error : Process completed with exit code 1. 코드 1개가 잘못되었다는 뜻이다. 직전에 commit했던 파일을 다시 수정해보자. 저 에러 위에 약간의 설명이 나와있을텐데 그것을 수정하면 된다. error : keychain errors terminal에 $git config –global credential.helper osxkeychain 입력 그래도 안되면 깃허브 -&gt; settings -&gt; developer settings -&gt; Personal Access Token -&gt; Token(Classic) 발급 $git config –global user.password ghp_abcabcabcabc (발급받은 토큰 입력) $git config –list 입력 후 제대로 입력 되었는지 확인. ERROR: Invalid first code point of tag name U+BC1C. 포스트.md파일에 &lt;html 문법이 아닌 이런 한글 입력값&gt;이 있으면 에러가 나온다. 1234567Error: The process '/opt/hostedtoolcache/Ruby/3.2.0/x64/bin/bundle' failed with exit code 5 at ExecState._setResult (/home/runner/work/_actions/ruby/setup-ruby/v1/dist/index.js:5371:25) at ExecState.CheckComplete (/home/runner/work/_actions/ruby/setup-ruby/v1/dist/index.js:5354:18) at ChildProcess.&lt;anonymous&gt; (/home/runner/work/_actions/ruby/setup-ruby/v1/dist/index.js:5248:27) at ChildProcess.emit (node:events:390:28) at maybeClose (node:internal/child_process:1064:16) at Process.ChildProcess._handle.onexit (node:internal/child_process:301:5) 이와 같은 메시지가 출력 된다면 ruby의 버전에 오류가 생겼을 확률이 높다. .pages-delpy.yml에서 ruby버전을 3.1.2로 수정하니 해결되었다.","link":"/2022/11/19/Error_fix/2022-11-19-bloging_errors/"},{"title":"No module named &quot;torch&quot;","text":"pytorch설치 후 주피터 노트북에서 import안될 경우 mac vantura OS를 사용중이며, 아나콘다의 주피터 노트북 사용중에 pip혹은 conda로 설치했지만(혹은 오류) 파이토치 설치 후 모듈이 없다는 에러가 나오는 경우 터미널에서 가상 환경을 파이썬 3.6버전으로 변경 (이름 : env_pytorch) 그 가상 환경을 활성화 시키기 torchvision 설치 주피터 노트북에서 import하여 작동 확인해보기 123conda create -n env_pytorch python=3.6conda activate env_pytorchpip install torchvision 123# 주피터 노트북에서 import하여 확인import torchimport torchvision 그래도 안되는 경우 경로 수정 필요 &lt;PATH&gt; 여기 페이지 눌러서 확인해보세요.","link":"/2022/12/03/Error_fix/2022-12-03-pytorch-installerror/"},{"title":"VScode에서 한글 버벅임 오류","text":"Visual Studio Code에서 한글 작성 시 버벅일 경우(버퍼링)123보기 -&gt; 명령팔레트 (Shift + Command + P) -&gt; 표시 언어 구성 -&gt; '한글' 선택컴퓨터 재부팅 시 같은 오류 발생 시 다시 '한글' 선택해주시면 됩니다. Terminal 창에서 글자가 깨지거나 작게 보일 경우1234567iTerm -&gt; profile -&gt; text 에서 서체를 확인 &gt; 제 경우는 'SourceCodePro+Powerline+Awesome Regular'로 설정되어 있었습니다VScode에서 설정으로 들어가서 terminal.integrated.fontFamily을defalt -&gt; &quot;'SourceCodePro+Powerline+Awesome Regular'&quot;로 설정- 홑따옴표를 저런식으로 2번을 해줘야 적용이 됩니다!!&quot;terminal.integrated.fontFamily&quot;: &quot;'SourceCodePro+Powerline+Awesome Regular'&quot; #터미널 폰트에 \\'사용해줘야 적용됨","link":"/2022/12/10/Error_fix/2022-12-10-VScode(input_Korean)/"},{"title":"python error logs","text":"파이썬 error log123AttributeError: Can't get attribute 'Sports_Dataset' on &lt;module '__main__' (built-in)&gt;# 파이토치의 torch.utils.data.DataLoader를 불러올때 옵션에 num_workers의 옵션을 ipykenel파일로 실행 하게 될 경우 오류가 난다.# 코랩 환경에서 돌리거나 num_workers옵션을 제거하면 실행은 된다(느리다) 123ValueError: 'a' cannot be empty unless no samples are taken# random.choice(range(train_data.__len__())) 이런 명령어를 수행 시 오류# print(train_data.__len__())하면 0값이 나올것이다. 클래스 또는 함수에서 오타를 수정해보자 openCV error log opencv는 맥에서 4.xx버전에서는 trackbar기능을 사용 시 에러가 나는 경우가 있습니다. 방법을 찾아봤지만 아직 오류 해결중 인듯으로 보이네요. 찾아본 유일한 해결책은 다운그레이드 (23-01-13기준) 123456789# conda 환경 설정 후에 진행하셔도 됩니다.# opencv 제거python -m pip uninstall opencv-python# pip upgrade(이거 안하면 에러 나올 확률 있음)python -m pip install --upgrade pip# opencv를 다운 그레이드하기python -m pip install opencv-python==3.4.14.51 실행하여 잘 되는지 확인! openCV의 findContours에서 해당 에러 발생 시 ValueError: too many values to unpack (expected 2) 12#3.버전에서는 결과 값이 3개가 나오기 때문에 images를 붙여주면 에러가 나오지 않습니다.images, contours, hierachy = cv2.findContours(image, mode, method)","link":"/2022/12/18/Error_fix/2022-12-18-error-logs/"},{"title":"Dacon code review","text":"코드 리뷰(데이콘, 전처리) 마감된 대회에서 데이터를 다운 받고, 코드 공유에서 간단하게 살펴 본 후 자신의 스타일이 맞는 코드(너무 어렵지 않은 코드)를 찾습니다. ppt파일로 간단하게 코드를 왜 이런식(공유된 방식)으로 짰는지 보통 설명이 되있습니다. ppt바로 아래 코드가 공유되있는데 오른쪽에 보면 다운로드가 있는데 클릭하여 다운받습니다. jupyternotebook을 이용하여 해당 파일을 열어줍니다. 일단 간단하게 왜 이런식으로 짰고, 코드 방식이 어떤식으로 흘러가는지 이해합니다. 처음부터 끝까지 코드를 안보고 비슷하게 구동될 때까지 리뷰를 합니다. 데이콘(제주도 도로 교통량 예측 AI 경진대회)1.함수 선언에서 모르는 함수가 있는지 확인. 모르면 구글링하여 찾아보기. 2.csv to parquet(파켓)에서 파켓 파일을 왜 사용하는지 구글링 및 함수 다시 만들어보기 123456789101112def csv_to_parquet(csv_path, save_name): df = pd.read_csv(csv_path) df.to_parquet(f'./{save_name}.parquet') del df gc.collect() print(save_name, 'Done.')# 초기 1회만 수행(csv-&gt;parquet)csv_to_parquet('./train.csv', 'train')csv_to_parquet('./test.csv', 'test')# parquet불러오기train = pd.read_parquet('./train.parquet')test = pd.read_parquet('./test.parquet') 3.EDA는 코드 리뷰 전에 개인적으로 해보고 남들이 사용한 방식 보기. 4.시계열 데이터(datetime)을 라벨화 할 수 있게 추가 열 생성. 1234train['base_date'] = train['base_date'].astype('str') train['year'] = train['base_date'].apply(lambda x: x[:4]).astype('int') train['month'] = train['base_date'].apply(lambda x: x[4:6]).astype('int') train['day'] = train['base_date'].apply(lambda x: x[6:8]).astype('int') 5.이번 데이터는 train데이터(21년7월~22년7월)와 test데이터(22년8월)의 날짜 차이가 크기 때문에 12test = train.query('month==7 and year==2022 and day&gt;15').reset_index(drop=True)train = train.query('month!=7 or year!=2022 or day&lt;=15').reset_index(drop=True) 이런식으로 데이터를 나눈 것을 확인 (참고하여 추후에 비슷한 문제 나와도 이런 방식으로 생각해보기) 6.라벨 인코딩 할 수 있는 열들을 한 번에 인코딩하기 123456789101112str_col = ['day_of_week','start_turn_restricted','end_turn_restricted', 'road_name','road_type','road_rating','start_node_name','end_node_name', 'start_latitude','end_latitude','start_longitude','end_longitude']for i in str_col: le = LabelEncoder() le=le.fit(train[i]) train[i]=le.transform(train[i]) for label in np.unique(test[i]): if label not in le.classes_: le.classes_ = np.append(le.classes_, label) test[i]=le.transform(test[i]) 7.파생 변수 생성(Feature Engineering) - 매우 중요 제가 리뷰하는 코드를 작성한 팀은 다음과 같은 변수를 생성했음을 파악. 주요 항목별 target의 train 데이터셋 평균과 표준편차를 파생변수로 생성! [시간, 제한속도]별 target 평균은 별도 파생변수로 생성 [시간]별 targe 평균에 대한 [시간,제한속도]별 target 평균의 비율 추가 파생 변수는 train 데이터셋 만으로 생성한 이후, test 데이터셋으로 merge train &amp; target을 합친 후 데이터를 구해서 어떤게 더 좋은지 파악해봐도 좋을듯함 (아마 train이 더 좋아서 train만 한듯합니다. 이와 같은 파생 변수 생성은 문제들을 많이 풀어보고 경험이 축적되어야 좋은 모델을 만들 수 있을 듯합니다. 일단, 제 경우는 이러한 코드들에서 이러한 변수를 어떤식으로 왜 만들었는지 분석해보고, 추후 적용할 수 있을지 파악하는 편입니다. 123456789101112names = ['day_of_week', 'base_hour','road_name','start_node_name','end_node_name','maximum_speed_limit','start_latitude','end_latitude','start_longitude','end_longitude']for name in names: print(name) df1 = train.groupby(name).mean().reset_index()[[name,'target']].rename(columns={'target':f'{name}_mean_target'}) train = pd.merge(train,df1,on=name,how='left') test = pd.merge(test,df1,on=name,how='left') df1 = train.groupby(['base_hour','maximum_speed_limit']).mean().reset_index()[[ 'base_hour','maximum_speed_limit','target']].rename(columns={'target':'whs_mean_target'})train = pd.merge(train,df1,on=[ 'base_hour','maximum_speed_limit'],how='left')test = pd.merge(test,df1,on=[ 'base_hour','maximum_speed_limit'],how='left') 8.null 처리, 카테고리(라벨) 처리 123456# null 처리 train = train.fillna(0)test = test.fillna(0)# 카테고리 변수 선언train[str_col] = train[str_col].astype('category')test[str_col] = test[str_col].astype('category') 9.시간(hour)열 처리 123456# 시간 cos/sin 변환 추가train['cos_time'] = np.cos(2*np.pi*(train['base_hour']/24))train['sin_time'] = np.sin(2*np.pi*(train['base_hour']/24))test['cos_time'] = np.cos(2*np.pi*(test['base_hour']/24))test['sin_time'] = np.sin(2*np.pi*(test['base_hour']/24)) 여기까지 전처리 코드 리뷰였습니다. 데이터 출처 : 데이콘_코드공유","link":"/2022/11/20/Personal_folder/2022-11-20-dacon-codereview/"},{"title":"Sample_codes","text":"Sample_codesparquet(메모리 줄여줌)12345678910111213import gcdef csv_to_parquet(csv_path, save_name): df = pd.read_csv(csv_path) df.to_parquet(f'./{save_name}.parquet') # df.to_parquet('train.parquet', engine='fastparquet', compression='snappy') del df gc.collect() print(save_name, 'Done.')csv_to_parquet('./train.csv', 'train')train = pd.read_parquet('./train.parquet').drop('road_in_use',axis=1)# from google.colab import files 코랩인 경우# files.download(&quot;train.parquet&quot;) query함수 (필터거는 함수) 열이름 입력1train.query('month==7 and year==2022 and day&gt;15') Labelencoding(train과 test데이터의 값이 다를 경우)12345678910for i in str_col: le = LabelEncoder() le=le.fit(train[i]) train[i]=le.transform(train[i]) for label in np.unique(test[i]): if label not in le.classes_: le.classes_ = np.append(le.classes_, label) #np.append하면 값이 추가되어 추가된 값이 라벨클래스에 추가되어 라벨링되는 구조 test[i]=le.transform(test[i]) global 변수명 생성1globals()['data_{}'.format(i)]","link":"/2022/11/20/Personal_folder/2022-11-20-sample-codes/"},{"title":"미니프로젝트 part 2","text":"Classification using Swin Transformer 이번 미니 프로젝트의 목표는 구글 이미지를 웹크롤링을 이용하여 이미지 저장 -&gt; 이미지 분류 모델(Swin Transformer모델을 이용한 파인 튜닝) -&gt; 모델 불러오기 + 구글 이미지를 분류 &amp; 삭제(+정렬) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251import torchvisionimport torchfrom PIL import Image, ImageFilterimport osimport numpy as npimport matplotlib.pyplot as pltimport randomfrom torch.utils.data import Dataset, DataLoaderimport torch.nn as nnimport torchvision.transforms as transformsimport cv2import globimport mathfrom einops import rearrange #차원 관리 모듈import timm # 파인튜닝모듈from tqdm.notebook import tqdm# configurationdevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')num_epochs = 10lr = 0.001batch_size = 32num_workers = 4 # ipykenel에서는 주석처리를 해야됩니다(멀티프로세싱 오류)# zip파일(archive) 해제하기import zipfilezip_file=zipfile.ZipFile('/content/drive/MyDrive/lesson_data/archive.zip')#파일 이름zip_file.extractall(path='/content/data')#압축 해제 경로, default(path=None)# 클래스(타겟) 리스트 만들기class_names = os.listdir('./data/train/') #폴더 이름 == 클래스(target) 네임class_names.sort()class_len = len(class_names) # Dataset 클래스 만들기class Sports_Dataset(Dataset): def __init__(self, data_name): #data_name will be set 'train' or 'valid'(the folder names) self.dataname = data_name #train파일 경로리스트 생성 self.img_path = [] #경로가 .jpg 확장자인 파일들을 img_path에 리스트화 시켜줌 for name in class_names: self.img_path.append(glob.glob(f'./data/{data_name}/{name}/*jpg')) #2차원 리스트 -&gt; 1차원 리스트 self.img_path = sum(self.img_path, []) #train파일의 labels 생성 self.labels = [] for path in self.img_path: self.labels.append(class_names.index(path.split('/')[3])) #텐서타입으로 변경하는 변수 생성 self.img_transpose = transforms.Compose([transforms.ToTensor()]) def __getitem__(self, index): img = Image.open(self.img_path[index]) # swin_base_patch4_window7_224모델이 224,224로 트레이닝 된 모델이라서 # 이미지 보간법을 이용하여 (224,224)사이즈로 변경 if img.size != (224,224): img = img.resize((224,224),Image.Resampling.BILINEAR) # Data augmentation with PIL when only 'train set' if self.dataname == 'train': if random.uniform(0,1) &lt; 0.3 or img.getbands() == 'L': img = img.convert('L').convert('RGB') # Random crop with size (64,64) from 30% if random.uniform(0,1) &lt; 0.3 : img = img.resize((224+64,224+64), Image.Resampling.BILINEAR) x = random.randrange(0,64) y = random.randrange(0,64) img = img.crop((x,y,x+224, y+224)) # Random Gaussian blur from 20% if random.uniform(0,1) &lt; 0.2: img = img.filter(ImageFilter.GaussianBlur(random.uniform(0.5,1.2))) # Random flip from 30% if random.uniform(0,1) &lt; 0.3: img = img.transpose(Image.Transpose.FLIP_LEFT_RIGHT) else : img = img.convert('RGB') lbl = self.labels[index] lbl = torch.tensor(lbl) img = self.img_transpose(img) return img, lbl #Sports_Dataset[0] == img, Sports_Dataset[1] == lbl(데이터 사용 방식) def __len__(self): return len(self.img_path)# 이미지 8개 확인 및 데이터셋 클래스 작동 유무 확인(실행하지 않아도 모델링에는 지장은 없음)train_dataset = Sports_Dataset('train')print(train_dataset.__len__())_, ax = plt.subplots(2, 4, figsize=(16,10))for i in range(8): data = train_dataset.__getitem__(random.choice(range(train_dataset.__len__()))) image = data[0].cpu().detach().numpy().transpose(1, 2, 0) * 255 # .cpu() == GPU 메모리에 올려져 있는 tensor를 cpu 메모리로 복사하는 method # .detach() == Returns a new Tensor # detach,cpu 순서는 별로 상관 없다. image = image.astype(np.uint32) #uint는 0을포함한 양수로된 정수타입 label = data[1] ax[i//4][i-(i//4)*4].imshow(image) ax[i//4][i-(i//4)*4].set_title(class_names[label])# 학습된 모델 중 swin_base_patch4_window7_224를 사용(파인튜닝)model = timm.create_model('swin_base_patch4_window7_224', pretrained=True)model.head = nn.Sequential(nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(0.3), nn.Linear(512, class_len)) model = model.to(device)criterion = timm.loss.LabelSmoothingCrossEntropy() # this is better than nn.CrossEntropyLosscriterion = criterion.to(device)optimizer = torch.optim.AdamW(model.head.parameters(), lr=lr) # Setting for transfer learning#데이터 정제train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)val_dataset = Sports_Dataset('valid')val_loader = DataLoader(dataset=val_dataset, batch_size=1, shuffle=False)#training (실행 전에 net폴더가 없으면 생성해주기!)def update_lr(optimizer, lr): for param_group in optimizer.param_groups: param_group['lr'] = lrmodel.train()total_step = len(train_loader)curr_lr = lrbest_score = 0for epoch in range(2): total_loss = 0 for i, (images,labels) in enumerate(tqdm(train_loader)): images = images.to(device) labels = labels.to(device) g_labels = model(images) loss = criterion(g_labels,labels) optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss.item() if (i+1) % 100 == 0: print(f'{batch_size*(i+1)} / {train_dataset.__len__()}') model.eval() score = 0 for i, (images, labels) in enumerate(valid_loader): images = images.to(device) labels = labels.to(device) g_labels = model(images) score += int(torch.max(g_labels, 1)[1][0] == labels[0]) print(f'Epoch : {epoch+1}, Loss : {total_loss/total_step}') avg = score / len(val_dataset) print(f'Accuracy : {avg :.2f}\\n') model.train() if best_score &lt; avg: best_score = avg if not os.path.exists('./nets'): os.mkdir('./nets') torch.save(model.state_dict(), 'nets/SwinTransformer.ckpt') #net 폴더를 만들어야함 if (epoch+1) %2 == 0: curr_lr = lr * 0.8 update_lr(optimizer, curr_lr)#모델 불러오기model.eval()model.load_state_dict(torch.load('nets/SwinTransformer.ckpt', map_location=device))test_dataset = Sports_Dataset('test')test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)# accuracy 확인 하기preds = []gts = []score = 0for i, (images, labels) in enumerate(test_loader): images = images.to(device) labels = labels.to(device) g_labels = model(images) pred = torch.max(g_labels, 1)[1][0].item() preds.append(pred) gt = labels[0].item() gts.append(gt) score += int(pred == gt)avg = score / len(val_dataset)print('Accuracy: {:.4f}\\n'.format(avg))#테스트 데이터 평가(랜덤 8개)test_dataset = Sports_data('test')test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)_, ax = plt.subplots(2, 4, figsize=(16,10))for i in range(8): data = test_dataset.__getitem__(np.random.choice(range(test_dataset.__len__()))) image = data[0].cpu().detach().numpy().transpose(1, 2, 0) * 255 image = image.astype(np.uint32) label = data[1] idx = torch.max(model(data[0].unsqueeze(0).to(device)), 1)[1][0].item() ax[i//4][i-(i//4)*4].imshow(image) ax[i//4][i-(i//4)*4].set_title('Predict: {}\\nGT: {}'.format(class_names[idx], class_names[label]))# 녹색: perfect score / 빨간색: imperfect score(어떤 종목을 못맞추었는지 확인하기)for i in range(class_len): score_sum = 0 for j in range(5): score_sum += int(gts[i*5+j] == preds[i*5+j]) if score_sum == 5: print('\\033[92m' + '{}: {} / 5'.format(class_names[i], score_sum)) else: print('\\033[91m' + '{}: {} / 5'.format(class_names[i], score_sum))","link":"/2022/12/17/Portfolio/2022-12-17-%08img-cl-swin/"},{"title":"미니프로젝트 part 1","text":"google image save using Chrome driver 이번 미니 프로젝트의 목표는 구글 이미지를 웹크롤링을 이용하여 이미지 저장 -&gt; 이미지 분류 모델(Swin Transformer모델을 이용한 파인 튜닝) -&gt; 모델 불러오기 + 구글 이미지를 분류 &amp; 삭제(+정렬) 123456789101112131415161718192021222324252627282930import timeimport urllib.requestfrom urllib.request import urlopen, urlparse, urlunparse, urlretrievefrom selenium import webdriverfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver.common.by import Byimport osimport pandas as pdfrom tqdm.notebook import tqdmchrome_path ='../Desktop/chromedriver' #크롬드라이버 경로(각자 환경에 맞춰 수정)chrome_options = webdriver.ChromeOptions()chrome_options.add_argument(&quot;lang=ko_KR&quot;) # 한국어chrome_options.add_argument('window-size=1920x1080') #윈도우 창크기를 키움def selenium_scroll_option(): # 스크롤 높이 가져옴 last_height = driver.execute_script('return document.body.scrollHeight') while True: # 끝까지 스크롤 다운 driver.execute_script('window.scrollTo(0, document.body.scrollHeight);') time.sleep(3) # 스크롤 다운 후 스크롤 높이 다시 가져옴 new_height = driver.execute_script('return document.body.scrollHeight') if new_height == last_height: break last_height = new_height 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 키워드 검색하기keyword = input(&quot;검색할 키워드를 입력 : &quot;)image_name = input(&quot;저장할 이미지(+폴더) 이름 : &quot;)#폴더가 없으면 폴더 생성if not os.path.exists(f'./{image_name}'): print(f'create directory ... {image_name}') os.mkdir(f'./{image_name}')driver = webdriver.Chrome(chrome_path)driver.get('http://www.google.co.kr/imghp?hl=ko')browser = driver.find_element(By.CSS_SELECTOR, 'body &gt; div.L3eUgb &gt; div.o3j99.ikrT4e.om7nvf &gt; form &gt; div:nth-child(1) &gt; div.A8SBwf &gt; div.RNNXgb &gt; div &gt; div.a4bIc &gt; input' )browser.click()browser.send_keys(keyword)browser.send_keys(Keys.RETURN)###### 이 부분의 bb함수 부분은 '검색 결과 더보기'버튼을 누르는 함수인데 버전마다 차이가 있으니 수정해서 사용해야됩니다. 본인은 mac/ 4.6.0 사용selenium_scroll_option() # 스크롤하여 이미지를 많이 확보bb = driver.find_element(By.CSS_SELECTOR, '#islmp &gt; div &gt; div &gt; div &gt; div &gt; div.gBPM8 &gt; div.qvfT1 &gt; div.YstHxe &gt; input')bb.click() # 이미지 더보기 클릭selenium_scroll_option()#이미지 src요소를 리스트업해서 이미지 url 저장images = driver.find_elements(By.CSS_SELECTOR, &quot;.rg_i.Q4LuWd&quot;) # 클래스 네임에서 공백은 .을 찍어줌images_url = []for i in images: if i.get_attribute('src')!= None : images_url.append(i.get_attribute('src')) else : images_url.append(i.get_attribute('data-src'))# driver.close()# 겹치는 이미지 url 제거print(&quot;전체 다운로드한 이미지 개수: {}\\n동일한 이미지를 제거한 이미지 개수: {}&quot;.format(len(images_url), len(pd.DataFrame(images_url)[0].unique())))images_url=pd.DataFrame(images_url)[0].unique()# 이미지 저장# 저장되는 규칙은 folder_name(경로)/이미지 이름(크롤링 시 폴더 생성 input 이름)_번호.jpgfolder_name = (f'./{image_name}/')for i, url in enumerate(tqdm(images_url), 0): urlretrieve(url, folder_name + image_name + '_' + str(i) + '.jpg')driver.close()","link":"/2022/12/19/Portfolio/2022-12-19-img-webcrawling/"},{"title":"미니프로젝트 part 3","text":"이번 미니 프로젝트의 목표는 구글 이미지를 웹크롤링을 이용하여 이미지 저장 -&gt; 이미지 분류 모델(Swin Transformer모델을 이용한 파인 튜닝) -&gt; 모델 불러오기 + 구글 이미지를 분류 &amp; 삭제(+정렬) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import torchvisionimport torchfrom PIL import Image, ImageFilterimport osimport numpy as npimport matplotlib.pyplot as pltimport randomfrom torch.utils.data import Dataset, DataLoaderimport torch.nn as nnimport torchvision.transforms as transformsimport cv2import globimport mathfrom einops import rearrange #차원 관리 모듈import timm # 파인튜닝모듈from tqdm.notebook import tqdm# configurationdevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')num_epochs = 10lr = 0.001batch_size = 32num_workers = 4 # ipykenel에서는 주석처리를 해야됩니다(멀티프로세싱 오류)# 로컬 환경class_names = os.listdir('./data/train/')class_names = sorted(class_names)class_len = len(class_names)# Dataset 클래스 만들기class Sports_Dataset(Dataset): def __init__(self, data_name): #data_name will be set 'train' or 'valid'(the folder names) self.dataname = data_name #train파일 경로리스트 생성 self.img_path = [] #경로가 .jpg 확장자인 파일들을 img_path에 리스트화 시켜줌 for name in class_names: self.img_path.append(glob.glob(f'./data/{data_name}/{name}/*jpg')) #2차원 리스트 -&gt; 1차원 리스트 self.img_path = sum(self.img_path, []) #train파일의 labels 생성 self.labels = [] for path in self.img_path: self.labels.append(class_names.index(path.split('/')[3])) #텐서타입으로 변경하는 변수 생성 self.img_transpose = transforms.Compose([transforms.ToTensor()]) def __getitem__(self, index): img = Image.open(self.img_path[index]) # swin_base_patch4_window7_224모델이 224,224로 트레이닝 된 모델이라서 # 이미지 보간법을 이용하여 (224,224)사이즈로 변경 if img.size != (224,224): img = img.resize((224,224),Image.Resampling.BILINEAR) # Data augmentation with PIL when only 'train set' if self.dataname == 'train': if random.uniform(0,1) &lt; 0.3 or img.getbands() == 'L': img = img.convert('L').convert('RGB') # Random crop with size (64,64) from 30% if random.uniform(0,1) &lt; 0.3 : img = img.resize((224+64,224+64), Image.Resampling.BILINEAR) x = random.randrange(0,64) y = random.randrange(0,64) img = img.crop((x,y,x+224, y+224)) # Random Gaussian blur from 20% if random.uniform(0,1) &lt; 0.2: img = img.filter(ImageFilter.GaussianBlur(random.uniform(0.5,1.2))) # Random flip from 30% if random.uniform(0,1) &lt; 0.3: img = img.transpose(Image.Transpose.FLIP_LEFT_RIGHT) # 왜인지 모르겠지만 구글이미지 중 컬러인데 1채널이 존재해서 무조건 3채널로 바꿔줘야합니다. else : img = img.convert('RGB') lbl = self.labels[index] lbl = torch.tensor(lbl) img = self.img_transpose(img) return img, lbl #Sports_Dataset[0] == img, Sports_Dataset[1] == lbl(데이터 사용 방식) def __len__(self): return len(self.img_path) 12345678910# 모델 로드model = timm.create_model('swin_base_patch4_window7_224',pretrained=True)model.head = nn.Sequential( nn.Linear(1024,512), nn.ReLU(), nn.Dropout(0.3), nn.Linear(512,class_len))model = model.to(device) 12345model.eval()model.load_state_dict(torch.load('./nets/SwinTransformer (1).ckpt', map_location=device))test_dataset = Sports_Dataset('test')test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False) 123456789101112131415161718192021#불러온 모델(.ckpt파일)의 성능 검사preds = []gts = []score = 0for i, (images, labels) in enumerate(test_loader): images = images.to(device) labels = labels.to(device) g_labels = model(images) pred = torch.max(g_labels, 1)[1][0].item() a = torch.max(g_labels, 1) preds.append(pred) gt = labels[0].item() gts.append(gt) score += int(pred == gt)avg = score / len(test_dataset)print('Accuracy: {:.4f}\\n'.format(avg)) Accuracy: 0.9840 12345test_dataset = Sports_Dataset('base_test')test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)len(test_loader) 12345678910111213141516ori_len = len(test_dataset)_, ax = plt.subplots(2, 4, figsize=(16,10))for i in range(8): data = test_dataset.__getitem__(np.random.choice(range(test_dataset.__len__()))) label = data[1] image = data[0].cpu().detach().numpy().transpose(1, 2, 0) * 255 image = image.astype(np.uint32) label = data[1] idx = torch.max(model(data[0].unsqueeze(0).to(device)), 1)[1][0].item() ax[i//4][i-(i//4)*4].imshow(image) ax[i//4][i-(i//4)*4].set_title('Predict: {}\\nGT: {}'.format(class_names[idx], class_names[label])) 여기서 원본과 예상이 다른 파일들을 삭제할 겁니다. 123456789101112131415161718192021222324folder_name = 'base_test'target_folder = 'baseball'test_dataset = Sports_Dataset(folder_name)test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)# 원래 : baseball, 예측 : chuckwagon racing 13ori_len = len(test_dataset)for i in range(ori_len): try: data = test_dataset.__getitem__(i) label = data[1] idx = torch.max(model(data[0].unsqueeze(0).to(device)), 1)[1][0].item() print(f'원래 사진 : {class_names[label]}, 예측 : {class_names[idx]}', i) if class_names[label] != class_names[idx]: path = f'data/{folder_name}/{target_folder}/{class_names[label]}_{i}.jpg' os.remove(path) print('\\033[31m'+f'removed file {path}'+'\\033[30m') # if i = : # break except: print('\\033[31m'+f'경로 {path}에 {class_names[label]}_{i}.jpg 사진이 없습니다'+'\\033[30m') 1234567891011121314#파일 이름 초기화 및 정렬 !!!!!!!(1회만 실행)!!!!!!!!folder_name = 'base_test'target_folder = 'baseball'#폴더 경로folder_path = 'data/base_test/baseball'file_names = os.listdir(folder_path)#파일 이름 초기화 후 재정렬(1회만 실행해야함 여러번 실행 시 중복된 이름이 삭제됨)for i, name in enumerate(file_names): newname = f'{folder_path}/{target_folder}_{i}.jpg' print(newname) src = os.path.join(folder_path, name) os.rename(src, newname)","link":"/2022/12/23/Portfolio/2022-12-23-img-delete/"},{"title":"개인 프로젝트(포트폴리오)","text":"이미지 프로젝트 프로젝트 기간 : 2022.12.09 ~ 2022.12.23 작성자, 발표자 : 조인환 이미지 분류를 통해 무엇인가 활용하고 싶어서 시작한 주제입니다. 이번 프로젝트는 총 3개의 파일로 구성되어 있습니다. Part 1. 구글 이미지를 웹크롤링을 이용하여 이미지 저장 Part 2. 이미지 분류 모델(Swin Transformer)을 이용한 파인 튜닝 Part 3. 모델 불러오기 + 구글 이미지를 분류 =&gt; 삭제 &amp; 리네임하고 재정렬 Part 1. 구글 이미지 웹크롤링 -&gt; 이미지 저장하는 코드123456789101112131415161718192021222324252627282930import timeimport urllib.requestfrom urllib.request import urlopen, urlparse, urlunparse, urlretrievefrom selenium import webdriverfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver.common.by import Byimport osimport pandas as pdfrom tqdm.notebook import tqdmchrome_path ='../Desktop/chromedriver' #크롬드라이버 경로(각자 환경에 맞춰 수정)chrome_options = webdriver.ChromeOptions()chrome_options.add_argument(&quot;lang=ko_KR&quot;) # 구글.kr이기때문에 한국어로 설정chrome_options.add_argument('window-size=1920x1080') #윈도우 창크기를 키움def selenium_scroll_option(): # 스크롤 높이 가져옴 last_height = driver.execute_script('return document.body.scrollHeight') while True: # 끝까지 스크롤 다운 driver.execute_script('window.scrollTo(0, document.body.scrollHeight);') time.sleep(3) # 스크롤 다운 후 스크롤 높이 다시 가져옴 new_height = driver.execute_script('return document.body.scrollHeight') if new_height == last_height: break last_height = new_height 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 키워드 검색하기keyword = input(&quot;검색할 키워드를 입력 : &quot;)image_name = input(&quot;저장할 이미지(+폴더) 이름 : &quot;)#폴더가 없으면 폴더 생성if not os.path.exists(f'./{image_name}'): print(f'create directory ... {image_name}') os.mkdir(f'./{image_name}')driver = webdriver.Chrome(chrome_path)driver.get('http://www.google.co.kr/imghp?hl=ko')browser = driver.find_element(By.CSS_SELECTOR, 'body &gt; div.L3eUgb &gt; div.o3j99.ikrT4e.om7nvf &gt; form &gt; div:nth-child(1) &gt; div.A8SBwf &gt; div.RNNXgb &gt; div &gt; div.a4bIc &gt; input' )browser.click()browser.send_keys(keyword)browser.send_keys(Keys.RETURN)# 이 부분의 bb함수 부분은 '검색 결과 더보기'버튼을 누르는 함수인데,# 버전마다 차이가 있으니 수정해서 사용해야됩니다. 본인은 mac/ 4.6.0 사용selenium_scroll_option() # 스크롤하여 이미지를 많이 확보bb = driver.find_element(By.CSS_SELECTOR, '#islmp &gt; div &gt; div &gt; div &gt; div &gt; div.gBPM8 &gt; div.qvfT1 &gt; div.YstHxe &gt; input')bb.click() # 이미지 더보기 클릭selenium_scroll_option()#이미지 src요소를 리스트업해서 이미지 url 저장images = driver.find_elements(By.CSS_SELECTOR, &quot;.rg_i.Q4LuWd&quot;) # 클래스 네임에서 공백은 .을 찍어줌images_url = []for i in images: if i.get_attribute('src')!= None : images_url.append(i.get_attribute('src')) else : images_url.append(i.get_attribute('data-src'))# 겹치는 이미지 url 제거images_url=pd.DataFrame(images_url)[0].unique()# 이미지 저장. 700장 기준 약 3~5분 소요# 저장되는 규칙은 folder_name(경로)/이미지 이름(크롤링 시 폴더 생성 input 이름)_번호.jpgprint(f'전체 다운로드한 이미지 개수: {len(images_url)}\\n 동일한 이미지를 제거한 이미지 개수: {len(pd.DataFrame(images_url)[0].unique())}\\n *다운로드 중입니다.(약 3~5분 소요)')folder_name = (f'./{image_name}/')for i, url in enumerate(tqdm(images_url),0): urlretrieve(url, folder_name + image_name + '_' + str(i) + '.jpg')print('Done')driver.close() Part 2. 이미지 분류 모델(Swin Transformer)을 이용한 파인 튜닝Swin Transformer란 ? Swin Transformer는 2021년 3월에 마이크로소프트(아시아)에서 발표한 Transformer이다. 해당 논문에서는 ViT에서 모든 patch가 self attention을 하는 것에 대한 단점을 지적하면서각 patch를 window로 나누어 해당 윈도우 안에서만 self attention을 수행하고 그 윈도우를 한번 shift하고, 다시 self attention을 하는 구조 일반적인 Transformer와 달리 마치 Feature Pyramid Network같은 Hierarchical 구조를 제시하면서classification은 물론 Object Detection, Segmentation에서 backbone으로 사용되어 좋은 성능을 내게 된다. Swin Transformer == (shifted windows) Transformer 123456789101112131415161718!pip install pillow==9.1.0 import torchvisionimport torchfrom PIL import Image, ImageFilter #버전 맞춰주세요 9.1.0 이상import osimport numpy as npimport matplotlib.pyplot as pltimport randomfrom torch.utils.data import Dataset, DataLoaderimport torch.nn as nnimport torchvision.transforms as transformsimport cv2import globimport mathimport timm # 파인튜닝모듈from timm.loss import LabelSmoothingCrossEntropyfrom tqdm.notebook import tqdm 123456789101112# configurationdevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')num_epochs = 10lr = 0.001batch_size = 32num_workers = 2 # ipykenel에서는 주석처리를 해야됩니다(멀티프로세싱 오류)# 클래스(타겟) 리스트 만들기class_names = os.listdir('./data/train/') #폴더 이름 == 클래스(target) 네임class_names.sort()class_len = len(class_names) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# Dataset 클래스 만들기class Sports_Dataset(Dataset): def __init__(self, data_name): #data_name will be set 'train' or 'valid'(the folder names) self.dataname = data_name #train파일 경로리스트 생성 self.img_path = [] #경로가 .jpg 확장자인 파일들을 img_path에 리스트화 시켜줌 for name in class_names: self.img_path.append(glob.glob(f'./data/{data_name}/{name}/*jpg')) #2차원 리스트 -&gt; 1차원 리스트 self.img_path = sum(self.img_path, []) #train파일의 labels 생성 self.labels = [] for path in self.img_path: self.labels.append(class_names.index(path.split('/')[3])) #텐서타입으로 변경하는 변수 생성 self.img_transpose = transforms.Compose([transforms.ToTensor()]) def __getitem__(self, index): img = Image.open(self.img_path[index]) # swin_base_patch4_window7_224모델이 224,224로 트레이닝 된 모델이라서 # 이미지 보간법을 이용하여 (224,224)사이즈로 변경 if img.size != (224,224): img = img.resize((224,224),Image.Resampling.BILINEAR) # Data augmentation with PIL when only 'train set' if self.dataname == 'train': if random.uniform(0,1) &lt; 0.3 or img.getbands() == 'L': img = img.convert('L').convert('RGB') # Random crop with size (64,64) from 30% if random.uniform(0,1) &lt; 0.3 : img = img.resize((224+64,224+64), Image.Resampling.BILINEAR) x = random.randrange(0,64) y = random.randrange(0,64) img = img.crop((x,y,x+224, y+224)) # Random Gaussian blur from 20% if random.uniform(0,1) &lt; 0.2: img = img.filter(ImageFilter.GaussianBlur(random.uniform(0.5,1.2))) # Random flip from 30% if random.uniform(0,1) &lt; 0.3: img = img.transpose(Image.Transpose.FLIP_LEFT_RIGHT) # 왜인지 모르겠지만 구글이미지 중 컬러인데 1채널이 존재해서 무조건 3채널로 바꿔줘야합니다. else : img = img.convert('RGB') lbl = self.labels[index] lbl = torch.tensor(lbl) img = self.img_transpose(img) return img, lbl #Sports_Dataset[0] == img, Sports_Dataset[1] == lbl(데이터 사용 방식) def __len__(self): return len(self.img_path) 1234567891011121314151617181920# 이미지 8개 확인 및 데이터셋 클래스 작동 유무 확인(실행하지 않아도 모델링에는 지장은 없음)train_dataset = Sports_Dataset('train')print(train_dataset.__len__())_, ax = plt.subplots(2, 4, figsize=(16,10))for i in range(8): data = train_dataset.__getitem__(random.choice(range(train_dataset.__len__()))) image = data[0].cpu().detach().numpy().transpose(1, 2, 0) * 255 # .cpu() == GPU 메모리에 올려져 있는 tensor를 cpu 메모리로 복사하는 method # .detach() == Returns a new Tensor # detach,cpu 순서는 별로 상관 없다. image = image.astype(np.uint32) #uint는 0을포함한 양수로된 정수타입 label = data[1] ax[i//4][i-(i//4)*4].imshow(image) ax[i//4][i-(i//4)*4].set_title(class_names[label]) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102# 학습된 모델 중 swin_base_patch4_window7_224를 사용(파인튜닝)model = timm.create_model('swin_base_patch4_window7_224', pretrained=True)model.head = nn.Sequential(nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(0.3), nn.Linear(512, class_len)) model = model.to(device)criterion = timm.loss.LabelSmoothingCrossEntropy() # this is better than nn.CrossEntropyLosscriterion = criterion.to(device)optimizer = torch.optim.AdamW(model.head.parameters(), lr=lr) # Setting for transfer learning#데이터 정제train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)val_dataset = Sports_Dataset('valid')val_loader = DataLoader(dataset=val_dataset, batch_size=1, shuffle=False)#training def update_lr(optimizer, lr): for param_group in optimizer.param_groups: param_group['lr'] = lrmodel.train()total_step = len(train_loader)curr_lr = lrbest_score = 0for epoch in range(2): total_loss = 0 for i, (images,labels) in enumerate(tqdm(train_loader)): images = images.to(device) labels = labels.to(device) g_labels = model(images) loss = criterion(g_labels,labels) optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss.item() if (i+1) % 100 == 0: print(f'{batch_size*(i+1)} / {train_dataset.__len__()}') model.eval() score = 0 for i, (images, labels) in enumerate(valid_loader): images = images.to(device) labels = labels.to(device) g_labels = model(images) score += int(torch.max(g_labels, 1)[1][0] == labels[0]) print(f'Epoch : {epoch+1}, Loss : {total_loss/total_step}') avg = score / len(val_dataset) print(f'Accuracy : {avg :.2f}\\n') model.train() if best_score &lt; avg: best_score = avg if not os.path.exists('./nets'): os.mkdir('./nets') torch.save(model.state_dict(), 'nets/SwinTransformer.ckpt') if (epoch+1) %2 == 0: curr_lr = lr * 0.8 update_lr(optimizer, curr_lr)#모델 불러오기model.eval()model.load_state_dict(torch.load('nets/SwinTransformer.ckpt', map_location=device))test_dataset = Sports_Dataset('test')test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)# accuracy 확인 하기preds = []gts = []score = 0for i, (images, labels) in enumerate(test_loader): images = images.to(device) labels = labels.to(device) g_labels = model(images) pred = torch.max(g_labels, 1)[1][0].item() preds.append(pred) gt = labels[0].item() gts.append(gt) score += int(pred == gt)avg = score / len(val_dataset)print('Accuracy: {:.4f}\\n'.format(avg)) 12345678910111213141516171819#테스트 데이터 평가(랜덤 8개)test_dataset = Sports_Dataset('test')test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)_, ax = plt.subplots(2, 4, figsize=(16,10))for i in range(8): data = test_dataset.__getitem__(np.random.choice(range(test_dataset.__len__()))) image = data[0].cpu().detach().numpy().transpose(1, 2, 0) * 255 image = image.astype(np.uint32) label = data[1] idx = torch.max(model(data[0].unsqueeze(0).to(device)), 1)[1][0].item() ax[i//4][i-(i//4)*4].imshow(image) ax[i//4][i-(i//4)*4].set_title('Predict: {}\\nGT: {}'.format(class_names[idx], class_names[label])) 123456789# 녹색: perfect score / 빨간색: imperfect score(어떤 종목을 못맞추었는지 확인하기)for i in range(class_len): score_sum = 0 for j in range(5): score_sum += int(gts[i*5+j] == preds[i*5+j]) if score_sum == 5: print('\\033[92m' + '{}: {} / 5'.format(class_names[i], score_sum)) else: print('\\033[91m' + '{}: {} / 5'.format(class_names[i], score_sum)) Part 3. 모델 불러오기 + 구글 이미지를 분류 &amp; 삭제 &amp; 리네임1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import torchvisionimport torchfrom PIL import Image, ImageFilterimport osimport numpy as npimport matplotlib.pyplot as pltimport randomfrom torch.utils.data import Dataset, DataLoaderimport torch.nn as nnimport torchvision.transforms as transformsimport cv2import globimport mathfrom einops import rearrange #차원 관리 모듈import timm # 파인튜닝모듈from tqdm.notebook import tqdm# configurationdevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')num_epochs = 10lr = 0.001batch_size = 32num_workers = 4 # ipykenel에서는 주석처리를 해야됩니다(멀티프로세싱 오류)# 로컬 환경class_names = os.listdir('./data/train/')class_names = sorted(class_names)class_len = len(class_names)# Dataset 클래스 만들기class Sports_Dataset(Dataset): def __init__(self, data_name): #data_name will be set 'train' or 'valid'(the folder names) self.dataname = data_name #train파일 경로리스트 생성 self.img_path = [] #경로가 .jpg 확장자인 파일들을 img_path에 리스트화 시켜줌 for name in class_names: self.img_path.append(glob.glob(f'./data/{data_name}/{name}/*jpg')) #2차원 리스트 -&gt; 1차원 리스트 self.img_path = sum(self.img_path, []) #train파일의 labels 생성 self.labels = [] for path in self.img_path: self.labels.append(class_names.index(path.split('/')[3])) #텐서타입으로 변경하는 변수 생성 self.img_transpose = transforms.Compose([transforms.ToTensor()]) def __getitem__(self, index): img = Image.open(self.img_path[index]) # swin_base_patch4_window7_224모델이 224,224로 트레이닝 된 모델이라서 # 이미지 보간법을 이용하여 (224,224)사이즈로 변경 if img.size != (224,224): img = img.resize((224,224),Image.Resampling.BILINEAR) # Data augmentation with PIL when only 'train set' if self.dataname == 'train': if random.uniform(0,1) &lt; 0.3 or img.getbands() == 'L': img = img.convert('L').convert('RGB') # Random crop with size (64,64) from 30% if random.uniform(0,1) &lt; 0.3 : img = img.resize((224+64,224+64), Image.Resampling.BILINEAR) x = random.randrange(0,64) y = random.randrange(0,64) img = img.crop((x,y,x+224, y+224)) # Random Gaussian blur from 20% if random.uniform(0,1) &lt; 0.2: img = img.filter(ImageFilter.GaussianBlur(random.uniform(0.5,1.2))) # Random flip from 30% if random.uniform(0,1) &lt; 0.3: img = img.transpose(Image.Transpose.FLIP_LEFT_RIGHT) # 왜인지 모르겠지만 구글이미지 중 컬러인데 1채널이 존재해서 무조건 3채널로 바꿔줘야합니다. else : img = img.convert('RGB') lbl = self.labels[index] lbl = torch.tensor(lbl) img = self.img_transpose(img) return img, lbl #Sports_Dataset[0] == img, Sports_Dataset[1] == lbl(데이터 사용 방식) def __len__(self): return len(self.img_path) 12345678910# 모델 로드model = timm.create_model('swin_base_patch4_window7_224',pretrained=True)model.head = nn.Sequential( nn.Linear(1024,512), nn.ReLU(), nn.Dropout(0.3), nn.Linear(512,class_len))model = model.to(device) 12345model.eval()model.load_state_dict(torch.load('./nets/SwinTransformer (1).ckpt', map_location=device))test_dataset = Sports_Dataset('test')test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False) 123456789101112131415161718192021#불러온 모델(.ckpt파일)의 성능 검사preds = []gts = []score = 0for i, (images, labels) in enumerate(test_loader): images = images.to(device) labels = labels.to(device) g_labels = model(images) pred = torch.max(g_labels, 1)[1][0].item() a = torch.max(g_labels, 1) preds.append(pred) gt = labels[0].item() gts.append(gt) score += int(pred == gt)avg = score / len(test_dataset)print('Accuracy: {:.4f}\\n'.format(avg)) Accuracy: 0.9840 12345test_dataset = Sports_Dataset('base_test')test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)len(test_loader) 12345678910111213141516ori_len = len(test_dataset)_, ax = plt.subplots(2, 4, figsize=(16,10))for i in range(8): data = test_dataset.__getitem__(np.random.choice(range(test_dataset.__len__()))) label = data[1] image = data[0].cpu().detach().numpy().transpose(1, 2, 0) * 255 image = image.astype(np.uint32) label = data[1] idx = torch.max(model(data[0].unsqueeze(0).to(device)), 1)[1][0].item() ax[i//4][i-(i//4)*4].imshow(image) ax[i//4][i-(i//4)*4].set_title('Predict: {}\\nGT: {}'.format(class_names[idx], class_names[label])) 여기서 원본과 예상이 다른 파일들을 삭제할 겁니다. 123456789101112131415161718192021222324folder_name = 'base_test'target_folder = 'baseball'test_dataset = Sports_Dataset(folder_name)test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)# 원래 : baseball, 예측 : chuckwagon racing 13ori_len = len(test_dataset)for i in range(ori_len): try: data = test_dataset.__getitem__(i) label = data[1] idx = torch.max(model(data[0].unsqueeze(0).to(device)), 1)[1][0].item() print(f'원래 사진 : {class_names[label]}, 예측 : {class_names[idx]}', i) if class_names[label] != class_names[idx]: path = f'data/{folder_name}/{target_folder}/{class_names[label]}_{i}.jpg' os.remove(path) print('\\033[31m'+f'removed file {path}'+'\\033[30m') # if i = : # break except: print('\\033[31m'+f'경로 {path}에 {class_names[label]}_{i}.jpg 사진이 없습니다'+'\\033[30m') 1234567891011121314#파일 이름 초기화 및 정렬 !!!!!!!(1회만 실행)!!!!!!!!folder_name = 'base_test'target_folder = 'baseball'#폴더 경로folder_path = 'data/base_test/baseball'file_names = os.listdir(folder_path)#파일 이름 초기화 후 재정렬(1회만 실행해야함 여러번 실행 시 중복된 이름이 삭제됨)for i, name in enumerate(file_names): newname = f'{folder_path}/{target_folder}_{i}.jpg' print(newname) src = os.path.join(folder_path, name) os.rename(src, newname) 그동안 이미지를 분류만 해보고 이를 이용해서 다른 것을 하는 작업은 한 적이 없었는데, 이번 프로젝트 때 다양한 방법을 시도해 보며 파이토치를 다루는 부분, OS를 사용하는 부분 등 다양한 사용 방법을 배웠습니다. 이번 프로젝트를 하면서 가장 난도가 높았던 작업은 구글에서 다운받은 이미지들을 예측값에 맞게 다시 형식을 맞추고 다루는 부분이었습니다.","link":"/2022/12/23/Portfolio/2022-12-23-img-project/"},{"title":"Vim 주요 단축키","text":"개요 맥북/리눅스의 에러들을 검색하다보면 vim 혹은 vi ~/.zshrc 같은 명령어들을 입력하라는 게시글들이 많이 있습니다. vim/vi/nvim은 텍스트 에디터로 텍스트파일을 해당 도구를 활용하여 편집하겠다는 명령어입니다. 하지만 처음 사용해보면 어떻게 사용하는지 감이 안오고 그 전화면(메인화면)으로 나갈 수도 없습니다. 주요 명령어를 활용하여 이 에디터를 조금 파악한다면 보다 쉬운 개발을 할 수 있을겁니다. command기본적으로 exc버튼을 누른 상태에서 활용(기본 상태) 이동 h, j, k, l: 좌,하,상,우 커서 이동 - : 줄의 처음 위치로 커서 이동 gg: 맨 위로 커서 이동 [shift + g]: 맨 아래로 커서 이동 단어 검색 /누르고 검색 ex)/conda 입력 시 에디터에서 conda화면으로 커서 이동enter한번 입력 후 다음 단어 검색 시 enter가 아닌 n입력, 전으로 가려면 N입력 현재 보이는 페이지에서 커서 이동 [shift + h]: 현재 보이는 페이지를 기준으로 맨 위로 커서 이동 [shift + m]: 현재 보이는 페이지를 기준으로 중간 라인으로 커서 이동 [shift + l]: 현재 보이는 페이지를 기준으로 맨 아래로 커서 이동 전체 페이지 이동 \\}\\} : 입력 시 페이지 맨 아래로 이동 \\{\\{ : 입력 시 페이지 맨 위로 이동 (참고) 역슬레시 없이 입력해야합니다. insert 커맨드 i: 현재 커서가 위치한 문자의 앞에 Insert 하기 I: 현재 커서가 위치한 줄 맨 앞에 Insert 하기 a: 현재 커서가 위치한 문자의 뒤에 Insert 하기 A: 현재 커서가 위치한 줄 맨 뒤에 Insert 하기 O: 현재 커서가 위치한 줄 바로 윗줄에 Insert 하기 o: 현재 커서가 위치한 줄 바로 아랫줄에 Insert 하기 삭제 커맨드 dd : 커서가 있는 줄 삭제 저장 및 종료 커맨드 exc누르고 : 를 입력시 맨아래에 코맨드 입력 가능 :w 저장 :wq 저장 후 텍스트에디터 종료(터미널로 이동) :q 저장 하지 않고 종료 :wq! 저장 후 텍스트에디터를 강제 종료","link":"/2022/11/20/Mac_Fundamental_Concept/2022-11-20-vim-command/"},{"title":"JupyterNotebook 주요 단축키","text":"주피터 노트북 주요 단축키 a == 위에 셀 생성 b == 아래 셀 생성 c == 셀 복사 x == 셀 자르기 v == 셀 붙여넣기 control + shift + ‘-‘ == 셀 나누기 shift + m == 셀 합치기 dd == 셀 삭제 y == code 모드(셀) m == markdown 모드(셀) o == 코드 결과 접기(열기) %lgmagic == 입력하면 입력이 가능한 목록 확인 가능 % 입력 후 command 입력 가능(주피터 환경 내부에서 터미널 사용) ex) %lgmagic, %ls, %pwd, %cd 이를 활용하여 import gc를 하지 않고 작업가능 %% 입력하면 셀 전체 command 입력 가능 ! 입력하고 입력하면 command(터미널) 사용가능 (터미널과 완전 동일) ex) !pip3 install pandas 다운로드 받은 파일 쉽게 현재폴더(주피터노트북)으로 옮기기 terminal을 cd명령어로 주피터노트북의 폴더로 이동해둔다. $mv ~/Dow 까지 입력 후 Tap키를 눌르고 다운받은 파일을 탭키를 이용하여 입력한다.-수정필요 스페이스바 후 (./)를 입력한다. ex) mv ~/Downloads/testfile.text ./ 이렇게 입력하면 해당 파일이 현재폴더로 옮겨진다.","link":"/2022/11/20/Mac_Fundamental_Concept/2022-11-20-jupyter-command/"},{"title":"Terminal 주요 단축키","text":"맥북 terminal 명령어 현재 디렉토리 확인 pwd/Users/inhwan/ 현재 디렉토리 파일리스트 보기 ls, 숨김 파일까지 확인 할 경우 ls -la lsLICENSE _config.ymlls -la-rw-r–r– 1 inhwan staff 227 11 19 09:40 .editorconfigdrwxr-xr-x 15 inhwan staff 480 11 21 23:02 .git 디렉토리(폴더) 이동 cd cd/Users/inhwan/ &lt;- 으로 이동됨(기본 디렉토리)cd Music~/Music &lt;-으로 폴더 이동cd ../Users/inhwan/ &lt;- 전 폴더로 이동함 디렉토리(폴더) 생성 mkdir test_foldertouch test_folder/ &lt;- 위와 동일test_folder가 현위치(pwd)에 생성 파일 생성 touch test_file.mdtouch test_folder/test_file.txt &lt;- 이런 방식으로 사용가능 디렉토리(폴더) 및 파일 삭제 rmdir test_folder &lt;- 빈 폴더만 삭제 가능rm -R test_folder &lt;- 빈 폴더&amp;파일 강제 삭제rm -rf test_folder &lt;- 위와 강제 동일rm test_file.md &lt;- 파일 삭제 디렉토리(폴더) 및 파일 이동 mv test_file.md test_folder/ 디렉토리(폴더) 및 파일 이름 변경 mv test_file.md test_folder/test_rename.mdmv test_folder/ test_folder_rename 파일 열기 open test_file.mdopen test_folder/test_rename.md 파일 복사 cp test_file.md new_file.md 명령어 입력 기록 확인 history 2645 ls 2646 cd Music 2647 cd 파일 내용 확인 cat test_file.md 파일 내용 수정(텍스트 에디터) vim test_file.txtnano test_file.txt이것들에 대해서는 따로 포스팅을 하였으며 참고.(생각보다 초기에 접근이 쉽지 않음) 디렉토리(폴더) 및 파일 위치 찾기 which python/opt/anaconda3/bin/python 파일 위치 찾기 find .(현재 디렉토리) -type f -name 'test_1.text'만약 정확한 파일 이름을 모르면 *(와일드카드) 사용 가능ex)find . -type f -name 'test*' 터미널 화면 초기화 clear혹은 command + 'k' 입력 터미널에서 작성 중인 글 지우기 control + 'U' 입력","link":"/2022/11/22/Mac_Fundamental_Concept/2022-11-22-mac-command/"},{"title":"Terminal 사용 Tips","text":"터미널 명령어 Tips 다운로드(finder)파일을 누르고 cd 뒤로 드레그로 옮겨서 여는것도 가능(추후 스크린샷) 화면 세로 분할 command + d, 분할 해제 command + shift + d 새로운 터미널 command + n 속성 보기 command + i (화면 색상 및 정보 확인) 터미널 탭 닫기 command + w 터미널 모든 탭 닫기 command + q 모든 탭 보기 또는 탭 개요 종료 command + shift + \\ ‘↑’ 화살표 누르면 전에 입력했던 명령어 불러오기 가능 몇 개의 단어 입력 후 Tap키 누르면 자동 완성 기능이 수행되어 작업이 편해짐 ex) cd 같은 명령어 뒤에 tap키 누르면 선택지에서 선택 가능 control + ‘u’ &lt; 터미널에 쓰고 있던 타이핑 전체 지우기 . current directory(현재 폴더) .. previous directory(전 폴더) ~ 폴더 자유롭게 참조할때 바탕화면 파일 전체 숨김 (배경 깔끔한거 좋아하는 사람. 기본 배경만 나옴) Defaults write com.apple.finder CreateDesktop false &amp;&amp; killall Finder(해제하려면 false에서 true로 변경) Control + ‘a’ == 맨 왼쪽으로 커서 이동 Control + ‘e’ == 맨 오른쪽으로 커서 이동","link":"/2022/11/22/Mac_Fundamental_Concept/2022-11-22-termianl-tips/"},{"title":"터미널에서 pip으로 설치한 파일이 열리지 않는 경우","text":"Shell(터미널)에서 pip으로 설치한 파일이 열리지 않는 경우 리눅스와 맥에서만 사용 가능합니다. 1.주피터 노트북(랩)에서 해당 명령어를 실행 합니다. 123456import syssys.executable# 출력 결과 예시'/opt/homebrew/Cellar/jupyterlab/3.4.8/libexec/bin/python3.10' 이 결과가 pip 명령어로 설치(실행) 시의 경로입니다. 2.터미널을 실행합니다. 3.해당 명령어를 추가합니다. 12bash사용자면 vim ~/.bashrc zsh라면 zshrc vim ~/.zshrc 4.키보드 화살표 아래 버튼을 맨 아래 화면이 나올때까지 내린 후 i를 누르고 해당 명령어를 붙여줍니다. 아래의 명령어는 주피터 노트북에서 나온 경로를 PATH로 설정하는겁니다. 12345export PATH='/opt/homebrew/Cellar/jupyterlab/3.4.8/libexec/bin/python3.10:PATH'또는export PATH=/opt/homebrew/Cellar/jupyterlab/3.4.8/libexec/bin/python3.10:$PATH 5.작성 후:를 입력 후 wq를 입력하면 됩니다. vim명령어를 통해 vim text editor를 사용하여 경로 설정을 해준겁니다. vim 에디터가 아닌 다른 에디터로(nano, code 등) 수정해주셔도 무방합니다. 중복 경로를 제거하고 싶다면 아래 명령어 실행 1PATH=$(echo -n $PATH | awk -v RS=: '!($0 in a) {a[$0]; printf(&quot;%s%s&quot;, length(a) &gt; 1 ? &quot;:&quot; : &quot;&quot;, $0)}')","link":"/2022/12/07/Mac_Fundamental_Concept/2022-12-07-PATH(shell)/"},{"title":"Anaconda 가상 환경","text":"터미널로 아나콘다 가상환경 설정하기 가상환경 리스트 확인하기 1conda env list 가상환경 이름 ‘name_of_conda_env’ 실행 1conda env activate name_of_conda_env 가상환경 종료(기본으로 돌아옴) 1conda env deactivate 가상환경 name_of_conda_env에 설치된 목록 확인 1conda list -n name_of_conda_env 가상환경 생성 1234conda create --name name_of_conda_env# 특정 버전의 환경 생성conda create -n name_of_conda_env_3.6 python=3.6 가상환경 제거 1conda remove --name name_of_conda_env 만약 iterm2를 세팅해뒀으면 터미널 오른쪽에 가상환경이 어디로 셋되어 있는지 확인이 가능하다.","link":"/2022/12/10/Mac_Fundamental_Concept/2022-12-10-conda-env/"},{"title":"Mac 사용 Tips","text":"Mac에서 윈도우 화면을 여러 창으로 띄어서 사용하는 경우 보통 Command + Tap 사용하면 다른 창을 불러올 수 있지만, 같은 프로그램일 경우 불가능하다. 이 경우 Command + ₩ Command 버튼 + 숫자 1 왼쪽의 버튼을 누르면 같은 프로그램의 다른 창이 열립니다.(보통 듀얼모니터 사용 시 매우 유용) 터미널을 자주 사용한다면 Iterm2 설치를 추천 Iterm2와 powerlevel10k그리고 oh my zsh를 설치하면 명령어 사용이 무척 편해진다.(관련 자료 검색해보세요) 윈도우의 delete 를 사용하고 싶다면 fn + backspace(뒤로가기 버튼) 를 입력하면 됩니다. 제거한 인터넷(크롬, 사파리) 창의 탭을 복구 command + shift + 't' 을 누르면 됩니다. 경로 설정1234. 현재 디렉토리.. 부모 디렉토리/ 최상위 root~ 홈(HOME, 메인)","link":"/2022/12/14/Mac_Fundamental_Concept/2022-12-14-Mac-basic/"},{"title":"wget 사용법","text":"wget 사용법1234567# 옵션과 url의 위치는 바꿔서 사용해도 상관 없음.$wget [options] [url]# 예시$wget https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EXVy7_7pF5FIsPp6WfXXfWgBNfUKx8N1VrTisN8FbGYG9w?download=1 -O Flickr8k_dataset.zip==$wget -O Flickr8k_dataset.zip https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EXVy7_7pF5FIsPp6WfXXfWgBNfUKx8N1VrTisN8FbGYG9w?download=1 wget의 옵션 신뢰할 수 없는 사이트 건너뛰기 --no-check-certificate 옵션을 사용 12$wget --no-check-certificate https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EXVy7_7pF5FIsPp6WfXXfWgBNfUKx8N1VrTisN8FbGYG9w?download=1 전체 웹 사이트를 미러링하는 방법 -m옵션으로 미러링을 설정합니다. 이를 이용해 모든 웹사이트의 파일을 다운로드할 수 있습니다. 1$wget -m https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EXVy7_7pF5FIsPp6WfXXfWgBNfUKx8N1VrTisN8FbGYG9w?download=1 파일의 저장 이름 설정(파일을 덮어 씌움) -O 옵션으로 결과 파일을 지정하면 기존에 존재하는 모든 파일에 덮어쓰기 합니다. 파일 이름 앞에 경로를 설정하면 경로에 데이터가 다운받아집니다. 1$wget https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EXVy7_7pF5FIsPp6WfXXfWgBNfUKx8N1VrTisN8FbGYG9w?download=1 -O Flickr8k_dataset.zip 저장 위치 설정 -P 옵션으로 저장 위치를 설정합니다. 다만 -O옵션에 경로를 설정하는게 일반적입니다. 1$wget -O data/Flickr8k_dataset.zip https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EXVy7_7pF5FIsPp6WfXXfWgBNfUKx8N1VrTisN8FbGYG9w?download=1 이어서 다운로드 옵션 -c 옵션으로 이어서 다운로드가 가능하게 설정 가능합니다. 새로운 받기에서는 파일이름 뒤에 .1이 추가됩니다. 여기서 .1이 이미 있으면 .2가 추가됩니다. 1$wget -c https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EXVy7_7pF5FIsPp6WfXXfWgBNfUKx8N1VrTisN8FbGYG9w?download=1","link":"/2022/12/27/Mac_Fundamental_Concept/2022-12-27-wget/"},{"title":"제 PC의 세팅","text":"VScode 설정 command + ,누르면 Setting으로 들어갈 수 있음 제 세팅은 아래와 같습니다. 123456789101112131415161718192021{ &quot;git.autofetch&quot;: true, #깃허브 자동 &quot;workbench.colorTheme&quot;: &quot;One Dark Pro Mix&quot;, #화면 컬러 &quot;workbench.iconTheme&quot;: &quot;ayu&quot;, #탐색기 아이콘 &quot;workbench.settings.editor&quot;: &quot;json&quot;, #설정을 json으로 바꾸는 기능(현재) &quot;workbench.settings.openDefaultSettings&quot;: true, &quot;workbench.settings.useSplitJSON&quot;: true, &quot;editor.fontSize&quot;: 13, &quot;editor.fontWeight&quot;: &quot;normal&quot;, &quot;debug.console.fontSize&quot;: 13, &quot;terminal.integrated.fontSize&quot;: 13, &quot;markdown.preview.fontSize&quot;: 13, &quot;editor.codeActionsOnSave&quot;: { &quot;source.fixAll.markdownlint&quot;: true } &quot;editor.minimap.enabled&quot;: false, #VScode 오른쪽 위편에 맵처럼 보이는 미니맵 제거 &quot;terminal.integrated.fontFamily&quot;: &quot;'SourceCodePro+Powerline+Awesome Regular'&quot;, #터미널 폰트에 \\'사용해줘야 적용됨 &quot;terminal.external.osxExec&quot;: &quot;iTerm.app&quot;, #터미널 -&gt; iTerm &quot;terminal.explorerKind&quot;: &quot;external&quot;, #internal이 아니라 외부(iTerm) &quot;terminal.integrated.shell.osx&quot;: '/bin/zsh', #zsh라고 해도됨 Colab 설정 High Contrast Dark가 가장 보기 편한거 같습니다. 12345678910111213{ '테마' : 'Dark', '편집기 색상' : 'High Contrast Dark', '편집기 키바인딩' : 'default', '글꼴 크기' : 14, '코드 랜더링' : 'monospace', '세로 눈금자 열' : 80, '코드 자동완성' : false, '행번호 표시' : true, '들여쓰기 가이드' : true, '코드셀 자동으로 괄호 닫기' : true, 'enter로 제안 수락' : true,} zshrc 기록(백업용) 123456789101112131415161718192021222324252627282930313233343536373839404142434445if [[ -r &quot;${XDG_CACHE_HOME:-$HOME/.cache}/p10k-instant-prompt-${(%):-%n}.zsh&quot; ]]; then source &quot;${XDG_CACHE_HOME:-$HOME/.cache}/p10k-instant-prompt-${(%):-%n}.zsh&quot;fiexport ZSH=&quot;$HOME/.oh-my-zsh&quot;ZSH_THEME=&quot;powerlevel10k/powerlevel10k&quot;plugins=( git zsh-autosuggestions )source $ZSH/oh-my-zsh.sh[[ ! -f ~/.p10k.zsh ]] || source ~/.p10k.zsh[[ -d ~/.rbenv ]] &amp;&amp; \\ export PATH=${HOME}/.rbenv/bin:${PATH} &amp;&amp; \\ eval &quot;$(rbenv init -)&quot;# Install Ruby Gems to ~/gemsexport GEM_HOME=&quot;$HOME/gems&quot;export PATH=&quot;$HOME/gems/bin:$PATH&quot;export PATH=&quot;$PATH:/Applications/Visual Studio Code.app/Contents/Resources/app/bin&quot;code () { VSCODE_CWD=&quot;$PWD&quot; open -n -b &quot;com.microsoft.VSCode&quot; --args $* ;}export PATH=&quot;/opt/anaconda3/bin:PATH&quot;# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;# !! Contents within this block are managed by 'conda init' !!__conda_setup=&quot;$('/opt/anaconda3/bin/conda' 'shell.zsh' 'hook' 2&gt; /dev/null)&quot;if [ $? -eq 0 ]; then eval &quot;$__conda_setup&quot;else if [ -f &quot;/opt/anaconda3/etc/profile.d/conda.sh&quot; ]; then . &quot;/opt/anaconda3/etc/profile.d/conda.sh&quot; else export PATH=&quot;/opt/anaconda3/bin:$PATH&quot; fifiunset __conda_setup# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;export CONDA_AUTO_ACTIVATE_BASE=false","link":"/2022/12/29/Mac_Fundamental_Concept/2022-12-29-editor-setting/"},{"title":"[Github Blog] 마크다운(Markdown) 문법","text":"Mathematics jekyll의 md 포스트 파일 상단에 math : true로 설정해야 됩니다. hexo 의 경우 mathjax: true`로 설정 (config파일에서 자동으로 설정 되있으면 안해도 됩니다) $$ \\sum_{n=1}^\\infty 1/n^2 = \\frac{\\pi^2}{6} $$ When $a \\ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are $$ x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a} $$ 12345$$ \\sum_{n=1}^\\infty 1/n^2 = \\frac{\\pi^2}{6} $$When $a \\ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are$$ x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a} $$ 밑줄 넣기 &lt;u&gt; 입력 &lt;/u&gt; 글자 색(노란색) 넣기 `입력` 맥북에서는 option + ₩(~) 버튼 줄바꿈을 하고 싶다면 문장 뒤에 스페이스바를 두번 + Enter 해준다. copy가 가능한 셀(코드블럭) 만들기```markdown입력은 여기에 해주세요.``` 12안녕하세요. 저는 조인환 이라고 합니다. 안녕하세요.저는 조인환 이라고 합니다. &lt;br&gt; 또한 줄바꿈을 해주는 HTML 태그이다. 1안녕하세요. &lt;br&gt; 저는 조인환 이라고 합니다. 안녕하세요. 저는 조인환 이라고 합니다. 문단 나누기한 줄의 공백을 두어 작성 하면 된다. (총 두 줄) 123안녕하세요.저는 조인환 합니다. 안녕하세요. 저는 조인환 합니다. 중첩된 구조아래와 같이 중첩된 구조를 만드려면 두번째 줄을 스페이스바 2번 눌러 띄어준 후 작성한다. 세번 중첩된 줄을 만드려면 스페이스바 4번. 123- hi - hello - 안녕 hi hello 안녕 마크다운 문법을 그대로 보여주고 싶을 때마크다운 문법 앞에 \\를 붙여준다. 1\\&lt;u&gt;안녕&lt;/u&gt; &lt;u&gt;안녕원래 같으면 밑줄 그어진 형태로 안녕으로 보일텐데 \\를 앞에 붙여주어 문법 그대로 &lt;u&gt;가 보여진다. Header글의 제목이 된다. 각 제목마다 permalink가 있는 것이 특징! # ~ ###### 로 제목 크기에 따라 h1 ~ h6을 나타낸다. 123456# h1## h2### h3#### h4##### h5###### h6 h1h2h3h4h5h6텍스트강조1**강조된 텍스트입니다** 기울임12*기울여진 텍스트입니다****굵고 기울여진 텍스트입니다*** 기울여진 텍스트입니다굵고 기울여진 텍스트입니다 취소선1~~취소된 텍스트입니다~~ 취소된 텍스트입니다 밑줄1&lt;u&gt;밑줄 있는 텍스트입니다&lt;/u&gt; 밑줄 있는 텍스트입니다 글씨 색123456789101112131415&lt;span style=&quot;color:yellow&quot;&gt;노란 글씨입니다.&lt;/span&gt;``` &lt;span style=&quot;color:yellow&quot;&gt;노란 글씨입니다.&lt;/span&gt;&lt;br&gt;## 링크### 링크만 있는 inline 링크\\&lt;링크주소&gt;```html&lt;https://www.google.com&gt; https://www.google.com 설명 있는 inline 링크[링크설명](링크주소) 1[구글 홈페이지](https://www.google.com) 구글 홈페이지 동일 파일 내에서의 문단(헤더) 이동 링크[설명어](문단의 주소) 문단의 주소 따는 방법 theorydb님 블로그 참고 헤더 제목 문자열을 복사하고 (문단의 주소)에 복사한다. 특수 문자를 제거한다. 공백을 -로 변경한다. 대문자는 소문자로 변경한다.예시) “#Markdown! 장점” &gt; “#markdown-장점” 1[마크다운 문법을 그대로 보여주고 싶을 때](#마크다운-문법을-그대로-보여주고-싶을-때) 마크다운 문법을 그대로 보여주고 싶을 때 동영상 삽입 123&lt;video controls width=&quot;800&quot;&gt; &lt;source src=&quot;/assets/videos/attention.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/video&gt; 그림 링크 삽입![image](이미지주소)로컬 파일 경로도 가능하다. {: width=”70%” height=”70%”}{: .align-center} *사진 출처 : 우리집 꾸미 ^^ 그림 자체에 링크 걸기![image](이미지주소)](이동하려는 링크 주소) 인용문&gt;로 표현할 수 있다. &gt;&gt; 두개 쓰면 중첩된 인용문.중첩시킬땐 앞에 스페이스바 2번 ! 12&gt; 이건 인용문이에요. &gt;&gt; 이건 인용문 속 인용문이에요. 이건 인용문이에요. 이건 인용문 속 인용문이에요. &lt;cite&gt; --- 태그와 {{: .small}}를 함께 써서 인용문 출처 남기기 12&lt;cite&gt;Steve Jobs&lt;/cite&gt; --- Apple Worldwide Developers' Conference, 1997{% raw %}{: .small}{% endraw %} Steve Jobs — Apple Worldwide Developers’ Conference, 1997{: .small} 리스트unordered list12345678910111213141516171819202122232425- 순서가 * 없는 + 목록 * 순서가- 없어용``` - 순서가 - 없는 - 목록 - 순서가- 없어용### ordered list```html1. 순서가2. 있는 1. 목록 - 하나 - 둘 2. 목록 - 하나 - 둘3. 목록 순서가 있는 목록 하나 둘 목록 하나 둘 목록 check list12- [ ] 체크 안됨- [X] 체크 됨 체크 안됨 체크 됨 구분선***와 ---로 나타낼 수 있다. 12***--- 테이블|와 - (3개 이상)의 조합으로 테이블을 만들 수 있다. 정렬 왼쪽 정렬 |:—| 오른쪽 정렬 |—:| 가운데 정렬 |:—:| 12345678910111213141516171819202122232425262728|**제목**|평점|감상평||:---:|---:|---||Avatar1|⭐⭐⭐⭐⭐|나비||Avatar2|⭐⭐⭐⭐⭐|물||아바타|⭐⭐⭐⭐⭐|안녕|``` |**제목**|평점|감상평||:---:|---:|---||Avatar1|⭐⭐⭐⭐⭐|나비||Avatar2|⭐⭐⭐⭐⭐|물||아바타|⭐⭐⭐⭐⭐|안녕|&lt;br&gt;## 토글 리스트 (접기/펼치기)마크다운에선 지원하지 않고 HTML의 `details` 태그로 사용 가능하다. `div markdown=”1”` 은 jekyll에서 html사이에 markdown을 인식 하기 위한 코드이다.```html&lt;details&gt;&lt;summary&gt;여기를 눌러주세요&lt;/summary&gt;&lt;div markdown=&quot;1&quot;&gt; 숨겨진 내용&lt;/div&gt;&lt;/details&gt; 여기를 눌러주세요 숨겨진 내용 버튼1&lt;a href=&quot;#&quot; class=&quot;btn--success&quot;&gt;Success Button&lt;/a&gt; Success Button 1[Default Button](#){% raw %}{: .btn .btn--primary }{% endraw %} Default Button{: .btn .btn--primary } 맨 위로 이동하기{: .btn .btn–primary }{: .align-right}","link":"/2022/11/19/Blogs_folder/2022-11-19-markdown/"},{"title":"Github blog info","text":"깃허브 포스팅 정보 아래와 같은 양식의 정보를 최상단에 입력을 해야한다. 1234567891011121314---title: 'Github blog info'categories: - Blogtags: - [Blog] date: 2022-12-30updated: 2022-12-30--- 오른쪽에 Contents(목차)를 지우고 싶으면 toc : false을 적거나, _config.yml에서 설정하면 모든 포스팅에 적용이 된다. toc_sticky : true를 적용하면 페이지(스크롤)이 넘어가도 화면 오른쪽 상단쪽에 고정되는 기능이 적용된다. 맨 밑의 Comment도 마찬가지이다. 최상단에 comments : false또는 _config.yml에서 설정 특정 포스트를 메인화면에 고정시키고 싶으면 pin : true를 입력하면 된다.(최근날짜 우선) 이미지 삽입 이미지를 삽입하기 위해서는 image: /assets/img_folder/file.jpg 이런식으로 넣는다.(데이터가 이미 그곳에 있는 경우) 또는, 깃허브 issue에 파일을 드레그하면 주소가 생성되는데 그 주소를 입력을 한다. {: width=’400’ class=’left’} 이미지 주소 옆에 이런식으로 크기, 위치를 설정 가능하다.(normal,left,right)","link":"/2022/12/30/Blogs_folder/2022-12-30-blog-info/"},{"title":"Github commands","text":"Github 연결 맥 OS에는 기본적으로 git이 설치되어있습니다. 123$git --version# 이 명령어를 수행하면 버전이 나온다면 git이 설치가 되어있는 겁니다.# git version 2.37.1 (Apple Git-137.1) Github이랑 컴퓨터랑 연동(처음 등록할 경우) 및 기본 명령어123# (명령어 확인(메뉴얼 호출))$git --help# 나가려면 q누르면 됨 git config (컴퓨터에 깃허브 아이디,이메일,페스워드(토큰)을 등록하는 명령어)123456$git config --global user.name YOUR_NAME$git config --global user.email YOUR_EMAIL$git config --global user.password YOUR_TOKEN# 유저,이메일,토큰을 등록한 이후 등록이 잘 되었는지 확인(화면 나가려면 q누르기)$git config --list git init (컴퓨터에 github repo를 연결하는 명령어)(git clone으로 할 경우 이 명령어를 사용할 필요 없음) 깃에 연결할 폴더에 cd 명령어로 이동 후, 해당 명령어 실행 시 .git파일 생성(숨김 파일)됨 동시에 깃명령어 수행하는 폴더로 변경(iterm2를 사용할 경우(커스터마이징) 브런치네임이 나옴) 1234$git init$git remote add origin https://github.com/InhwanCho/Study.git# 깃이랑 폴더를 연결# 파일 처음 커밋하려면 푸시 안됨 이거 먼저 해줘야 함 git clone (보통 url.git) git clone ../StudyforGit(repository name) 상대방, 혹은 자신의 깃에 연결하여 다운을 받음 폴더나 url에 연결 url에 보통 연결 12#맨 뒤에 보통 .git을 붙여줘야 한다.$git clone https://github.com/InhwanCho/Study.git git status 파일 수정 후 상태 확인 창.(확인 할 필요는 없지만 초반에는 도움이 많이 되는 명령어) 상대방이 수정 했을 경우에도 도움이 되니 초반에 많이 사용하도록 해보자. 1$git status git add ‘filename’ or . or ‘-A’ 파일 수정 후 페이지에 변경할 파일명 등록. ‘.’이나 ‘-A’입력 시 폴더 전체를 등록함 12345$git add .$git add -A# 만약 하나씩 등록을 원할 경우$git add test.text git commit -m ‘message’12# add한 파일들을 홈페이지에 올림.$git commit -m 'this is a test message' git push123456# commit한 파일을 홈페이지에 확정으로 올림.$git push# 파일이 처음 생성된 경우(깃허브 자체 연동 처음한 경우)$git push origin master# 위의 명령어로 실행해야 함(이후에는 git push만 하여도 작동) git pull12# 홈페이지에서 수정된 정보를 디렉토리와 비교해서 업데이트(혼자 작업할때는 거의 사용 안함)$git pull 변경 로그 확인(log, diff)12345$git log #전체 로그 출력$git log --oneline -n 3 #3개만 출력을 원할 경우# 파일 수정 후 변경된 부분을 표시하는 명령어(나오려면 q)$git diff add된 명령 취소하기(커밋 전)1$git reset HEAD^ git revert(push된 명령 취소하기)(로그 남음)123456789#log에서 커밋된 로그(ex.abdc0123abdc0123)를 되돌리기$git log --oneline -n 3 #3개의 로그 출력$git reset --hard abdc0123abdc0123# -&gt; git revert abdc0123abdc0123 --no-edit # -&gt;$git push git reset(push된 명령 취소하기) revert랑 유사하지만 reset은 로그를 남기지 않기때문에 revert가 보다 유용함 12345$git reset --hard abdc0123abdc0123 #-&gt;$git reset abdc0123abdc0123 --no-edit#-&gt;$git push revert한것을 다시 되돌리고 싶을 경우12# revert한 로그를 다시 revert하기 (1으로 머지)$git revert -m 1 abcd0321abcd0321 branch 사용하기branch 리스트 확인12# 현재 연결된 브렌치를 확인 *로 표시됨.$git branch -a branch 생성 및 선택1234567891011121314# 생성(작업 후 add, commit까진 작업이 동일함) # 다만, push를 git push 'branch name'으로 해야함$git branch test_branch_name#git 2.23버전 부터 git checkout을 대신하여 switch와 restore가 나오게 되었다.#checkout의 기능이 너무 많아 분리하였다고 볼 수 있다. - checkout: Switch branches or restore working tree files - switch: Switch branches - restore: Restore working tree files# 선택$git checkout test_branch_name$git switch test_branch_name$git restore test_branch_name 새로운 branch로 푸쉬하기12345$git push --set-upstream origin test_branch_name#또는$git push -u origin test_branch_name#1회만 해주면 다음부터는 그냥 push로 진행 가능 branch 끼리 푸쉬된 항목 합치기12345678#다시 메인 업스트림 branch로 이동(보통 main)$git checkout main#병합$git merge test_branch_name$git add .$git commit -m 'we have just merged the branch'$git push branch 삭제12345# (merge안하고 하면 경고뜸. 무시해도 됨)$git branch -d test_branch_name# branch를 메모리에서 완전히 제거$git push origin --delete test_branch_name","link":"/2022/12/11/Blogs_folder/2022-12-11-Git-command/"},{"title":"Github info","text":"HEAD란? 깃허브를 사용하다보면 HEAD라는게 자주 보입니다. HEAD란 해당 branch의 마지막 commit을 의미합니다. 깃허브에 올리지 말아야 할 파일이 있을 경우 깃허브를 사용하다보면 개인적으로 작업 또는 불필요한 파일들이 깃허브 폴더에 들어있을 경우가 있습니다. 이를 제어하려면 .ignore파일이 필요합니다. 123456# 디렉토리에있는 파일 중 깃에 올리고 싶지 않은 파일이 필요할 경우 생성 후 편집# 깃허브와 연동된 파일로 cd로 이동 후$touch .gitignore$open .gitignore# ex) .gitignore 파일에 abc.text, *.csv (이런식으로 와일드카드로 설정 가능)# vim/code 같은 텍스트 에디터로 편집 주의할 점은 이미 stage/repo에 올라간 파일이 .gitignore파일에 있으면 오류가 난다. 1234# 이런 경우 파일을 제거한 후 커밋해준다.$git rm file_name.md$git commit -m 'just deledted file_name.md'$git push","link":"/2022/12/11/Blogs_folder/2022-12-11-Git-info/"},{"title":"Github page 맨 위(탭) 아이콘(파비콘) 변경하기","text":"깃허브 블로그 파비콘 변경하기 변경을 하고 싶은 아이콘 이미지를 다운받는다. _config.yml파일의 favicon 경로를 살펴본다. https://favicon.io/이 사이트 png-&gt;ico를 눌러서 convert한다. unzip하고 site.webmanifest파일을 제거 하고 6개의 이미지 파일을 yml파일의 경로에다 옮겨준다. 변경되는데 시간이 오래 걸립니다.","link":"/2023/01/03/Blogs_folder/2023-01-03-favicon/"},{"title":"Github 메인페이지 README.md 배지(badge)로 꾸미기","text":"깃허브 메인페이지 README파일 노출 시키기 위의 사진의 Hi there... 가 적힌 문구는 제 깃허브 아이디(InhwanCho)와 동일한 이름의 repo(저장소) InhwanChoREADME.md 파일입니다.(더블 클릭된 부분) 오른쪽 상단에 +를 눌러서 New Repository를 클릭해서 아이디(Owner)와 동일한 이름의 repo를 생성합니다. 아이디와 동일한 이름의 repo에 들어가서 README.md 파일을 생성합니다. 마크다운 문법을 사용하여 메인 페이지를 꾸며줍니다. 배지(badge) 사용 방법 일단 위의 사진의 README.md 파일의 데이터입니다. 123456789101112131415161718### Hi there, I'm Inhwan Cho 👋&lt;h3 align=&quot;center&quot;&gt;📚 my STACKS📚&lt;/h3&gt;&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/Python-3766AB?style=flat-square&amp;logo=Python&amp;logoColor=white&quot;/&gt;&lt;/a&gt;&amp;nbsp &lt;br&gt; &lt;img src=&quot;https://img.shields.io/badge/TensorFlow-FF6F00?style=flat-square&amp;logo=TensorFlow&amp;logoColor=white&quot;/&gt;&lt;/a&gt;&amp;nbsp &lt;img src=&quot;https://img.shields.io/badge/PyTorch-EE4C2C?style=flat-square&amp;logo=PyTorch&amp;logoColor=white&quot;/&gt;&lt;/a&gt;&amp;nbsp &lt;img src=&quot;https://img.shields.io/badge/Linux-FCC624?style=flat-square&amp;logo=Linux&amp;logoColor=white&quot;&gt;&lt;/a&gt;&amp;nbsp &lt;br&gt; &lt;img src=&quot;https://img.shields.io/badge/Visual Studio Code-007ACC?style=flat-square&amp;logo=Visual Studio Code&amp;logoColor=white&quot;&gt;&lt;/a&gt;&amp;nbsp &lt;img src=&quot;https://img.shields.io/badge/GitHub-181717?style=flat-square&amp;logo=GitHub&amp;logoColor=white&quot;&gt;&lt;/a&gt;&amp;nbsp &lt;img src=&quot;https://img.shields.io/badge/GitHub Pages-222222?style=flat-square&amp;logo=GitHub Pages&amp;logoColor=white&quot;&gt;&lt;/a&gt;&amp;nbsp &lt;br&gt; &lt;img src=&quot;https://img.shields.io/badge/Google Sheets-34A853?style=flat-square&amp;logo=Google Sheets&amp;logoColor=white&quot;&gt;&lt;/a&gt;&amp;nbsp &lt;/p&gt; 맨위는 ###의 H3문법, 그 아래는 html 문법이 적용되었습니다. 아이콘 및 컬러에 대한 정보는 &lt;simpleicons&gt; 를 들어가셔서 참고하시면 됩니다. 예를들어 스텍스에 파이썬을 입력하고 싶다면 아래 대괄호 [] 부분만 변경하여 사용하시면 됩니다. 1https://img.shields.io/badge/[파이썬 &lt;아이콘&gt;]-[색상넘버 &lt;#빼고 입력&gt;]?style=flat-square&amp;logo=[파이썬 &lt;글자&gt;]&amp;logoColor=[글자 색(보통 white or black)]","link":"/2023/01/02/Blogs_folder/2023-01-02-gitbadges/"},{"title":"깃허브블로그를 변경 후 기존 페이지 잔존 시","text":"깃허브 블로그 기존 페이지 잔존 시깃허브 블로그를 jekyll에서 hexo로 변경 후로컬 서버에서는 잘 나오지만, 배포후에 메인 페이지는 출력이 잘 되는데, About, Archives, Tags, Categories가 기존의 블로그가 출력이 되었습니다.결론은 캐시를 지우지 않아서 그렇습니다.css를 수정하고 새로 고침을 해도 서버에서 새로운 css를 받아오는것이 아닌 캐시에 저장된 이미 있는 캐시 파일만을 계속 받아오므로 이런 현상이 나올 수 있습니다. 맥에서는 해당 페이지에서 Command + Shift + R 혹은 캐시 삭제로 해결 가능합니다.","link":"/2023/01/13/Blogs_folder/2023-01-13-blogmoving/"},{"title":"Icarus 테마 커스터마이징","text":"테마의 프로필 영역 사이즈 변경1234&lt;!-- layout/widget/profile.jsx --&gt;{/* &lt;figure class=&quot;image is-128x128 mx-auto mb-2&quot;&gt; */}&lt;figure class=&quot;image mx-auto mb-2&quot;&gt; font 변경12345678910// 51 lineconst fontCssUrl = { default: fontcdn(&quot;Ubuntu:wght@400;600&amp;family=Source+Code+Pro&quot;, &quot;css2&quot;), cyberpunk: fontcdn(&quot;Oxanium:wght@300;400;600&amp;family=Roboto+Mono&quot;, &quot;css2&quot;), nanumgothic: fontcdn(&quot;Nanum+Gothic:wght@400&amp;family=Roboto&quot;, &quot;css2&quot;),};// 151 line&lt;link rel=&quot;preconnect&quot; href=&quot;https://fonts.gstatic.com&quot; /&gt;&lt;link href={fontCssUrl['nanumgothic']} rel=&quot;stylesheet&quot; /&gt; 8 line12$family-sans-serif ?= 'Nanum Gothic', Ubuntu, Roboto, 'Open Sans', 'Microsoft YaHei', sans-serif// $family-sans-serif ?= Ubuntu, Roboto,'Nanum Gothic Coding', 'Open Sans', 'Microsoft YaHei', sans-serif 4 line1$article-font-size ?= 1.3rem disqus(댓글) 설정1234comment: type: disqus # Disqus shortname shortname: inhwancho-github-io #disqus아이디 생성 후 shortname이 생성되는데 그거 입력하면 됩니다. google_analytics123# id칸에 입력해야됩니다google_analytics: tracking_id: G-8RGKYVDD5B 버튼(follow 버튼) 기존에 follow 버튼 -&gt; home으로 돌아가는 버튼으로 변경 1234567&lt;/nav&gt;{followLink ? &lt;div class=&quot;level&quot;&gt;// 재수정 &lt;a class=&quot;level-item button is-round is-info is-outlined is-rounded is-light&quot; href=&quot;/&quot; target=&quot;_self&quot; rel=&quot;noopener&quot;&gt;Home&lt;/a&gt;// 기존꺼(수정된거) {/* &lt;a class=&quot;level-item button is-round is-link is-outlined&quot; href={followLink} target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;{followTitle}&lt;/a&gt; */}&lt;/div&gt; : null} 맨 밑에 총 방문자수 설정1busuanzi: true donate 부분 주석 처리 adsense 위젯 주석 처리 column 변경12345678&lt;div class={classname({ column: true, 'order-2': true, 'column-main': true, 'is-12': columnCount === 1, 'is-8-tablet is-8-desktop is-9-widescreen': columnCount === 2, 'is-8-tablet is-8-desktop is-6-widescreen': columnCount === 3})} dangerouslySetInnerHTML={{ __html: body }}&gt;&lt;/div&gt; navbar font 설정12345.navbar-logo img max-height: $logo-height font-size: 1.5rem 제목 설명 폰트 7 -&gt; 9 1{page.layout !== 'page' ? &lt;div class=&quot;article-meta is-size-9 is-uppercase level is-mobile&quot;&gt; 코드블럭 테스트 중_.compact 112Underscore.js123안녕_.compact([0, 1, false, 2, '', 3]);=&gt; [1, 2, 3] Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque hendrerit lacus ut purus iaculis feugiat. Sed nec tempor elit, quis aliquam neque. Curabitur sed diam eget dolor fermentum semper at eu lorem. Do not just seek happiness for yourself. Seek happiness for all. Through kindness. Through mercy. David LevithanWide Awake NEW: DevDocs now comes with syntax highlighting. http://devdocs.io @DevDocs Every interaction is both precious and an opportunity to delight. Seth Godin Welcome to Island Marketing","link":"/2023/01/16/Blogs_folder/2023-01-16-customizeing/"},{"title":"img_Captioning 코드 분해 Part 1","text":"코드 리뷰 이미지 캡셔닝(with attention)을 위해 불러오기용 Class 정의 py파일입니다. 코드 출처 : 캐글 코드 공유 &lt;kaggle&gt; 1234567891011#모듈 importimport osfrom collections import Counterimport numpy as npimport pandas as pdimport spacyimport torchfrom torch.nn.utils.rnn import pad_sequencefrom torch.utils.data import DataLoader,Datasetimport torchvision.transforms as Tfrom PIL import Image Vocabulary 클래스 정의 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class Vocabulary: #tokenizer(spacy는 영어에 최적화된 모델) spacy_eng = spacy.load(&quot;en_core_web_sm&quot;) def __init__(self,freq_threshold): #스페셜 토큰(int -&gt; str(token)) self.itos = {0:&quot;&lt;PAD&gt;&quot;,1:&quot;&lt;SOS&gt;&quot;,2:&quot;&lt;EOS&gt;&quot;,3:&quot;&lt;UNK&gt;&quot;} #string to int tokens #str -&gt; int(위에꺼 다시 변환, {str : int}형태의 딕셔너리로 바꿈) self.stoi = {v:k for k,v in self.itos.items()} self.freq_threshold = freq_threshold def __len__(self): return len(self.itos) # 정적인 메소드. self인자를 받지 않고 별개의 함수처럼 사용할 경우 사용 @staticmethod def tokenize(text): return [token.text.lower() for token in Vocabulary.spacy_eng.tokenizer(text)] # 토큰화된 값이 리스트로 만들어짐 # ex) text에 'this is a goo place'를 넣었다면 # ['this', 'is', 'a', 'good', 'place'] 이런식으로 출력 #vocab 생성 함수 def build_vocab(self, sentence_list): frequencies = Counter() #staring index 4(스페셜토큰이 0,1,2,3으로 지정되어있기 때문에 4부터 시작!) idx = 4 for sentence in sentence_list: for word in self.tokenize(sentence): frequencies[word] += 1 #freq_threshold이 넘어가면 그 다음 번호의 vocab(idx)을 추가 if frequencies[word] == self.freq_threshold: self.stoi[word] = idx self.itos[idx] = word idx += 1 # 실행 테스트용(실제로는 사용하지 않음) def numericalize(self,text): &quot;&quot;&quot; For each word in the text corresponding index token for that word form the vocab built as list &quot;&quot;&quot; tokenized_text = self.tokenize(text) return [ self.stoi[token] if token in self.stoi else self.stoi[&quot;&lt;UNK&gt;&quot;] for token in tokenized_text ] 클래스가 잘 작동하는지 확인 123456789101112#testing the vicab class v = Vocabulary(freq_threshold=1)v.build_vocab([&quot;This is a good place to find a city&quot;])print(v.stoi) #vocab이 만들어짐{'&lt;PAD&gt;': 0, '&lt;SOS&gt;': 1, '&lt;EOS&gt;': 2, '&lt;UNK&gt;': 3, 'this': 4, 'is': 5, 'a': 6, 'good': 7, 'place': 8, 'to': 9, 'find': 10, 'city': 11}#vocab 인덱스가 잘 나오는지 출력print(v.numericalize(&quot;This is a good place to find a city here!!&quot;))[4, 5, 6, 7, 8, 9, 10, 6, 11, 3, 3, 3] Dataset 클래스 정의 123456789101112131415161718192021222324252627282930313233343536class FlickrDataset(Dataset): def __init__(self,root_dir,caption_file,transform=None,freq_threshold=5): self.root_dir = root_dir self.df = pd.read_csv(caption_file) self.transform = transform #Get image and caption colum from the dataframe self.imgs = self.df[&quot;image&quot;] #타입 : 시리즈 self.captions = self.df[&quot;caption&quot;] #타입 : 시리즈 #Initialize vocabulary and build vocab self.vocab = Vocabulary(freq_threshold) self.vocab.build_vocab(self.captions.tolist()) #시리즈 -&gt; 리스트 def __len__(self): return len(self.df) def __getitem__(self,idx): caption = self.captions[idx] img_name = self.imgs[idx] img_location = os.path.join(self.root_dir,img_name) img = Image.open(img_location).convert(&quot;RGB&quot;) #apply the transfromation to the image if self.transform is not None: img = self.transform(img) #numericalize the caption text caption_vec = [] caption_vec += [self.vocab.stoi[&quot;&lt;SOS&gt;&quot;]] #시작 토큰 caption_vec += self.vocab.numericalize(caption) #vocab idx 예) [4, 5, 6, ...] caption_vec += [self.vocab.stoi[&quot;&lt;EOS&gt;&quot;]] #끝 토큰 return img, torch.tensor(caption_vec) FlickrDataset이 작동을 잘 하는지 테스트 123456789101112131415161718192021222324#이미지 사이즈를 파인튜닝용 (224,224)로 변경, 텐서로 변경transforms = T.Compose([ T.Resize((224,224)), T.ToTensor()])def show_image(inp, title=None): inp = inp.numpy().transpose((1, 2, 0)) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots areupdatedimport matplotlib.pyplot as plt#testing the dataset classdataset = FlickrDataset( root_dir = data_location+&quot;/Images&quot;, caption_file = data_location+&quot;/captions.txt&quot;, transform=transforms)img, caps = dataset[3]show_image(img,&quot;Image&quot;)print(&quot;Token:&quot;,caps)print(&quot;Sentence:&quot;, [dataset.vocab.itos[token] for token in caps.tolist()]) batches가 1이 아닌 경우 DataLoader의 collate_fn 옵션에 넣기 위해 생성이 필요 123456789101112131415class CapsCollate: def __init__(self,pad_idx,batch_first=False): self.pad_idx = pad_idx self.batch_first = batch_first def __call__(self,batch): imgs = [item[0].unsqueeze(0) for item in batch] imgs = torch.cat(imgs,dim=0) targets = [item[1] for item in batch] targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx) #batch [0]은 img, [1]은 target return imgs,targets dataloader작동 확인(Capscollate) 12345678910111213141516171819202122232425262728293031BATCH_SIZE = 4NUM_WORKER = 1#dataset은 앞에 만든 FlickrDataset, 패딩 토큰 지정(padding_value 옵션)pad_idx = dataset.vocab.stoi[&quot;&lt;PAD&gt;&quot;]data_loader = DataLoader( dataset=dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKER, shuffle=True, collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True))#데이터 로더에서 batch로 보기위해 iter,next함수 호출dataiter = iter(data_loader)batch = next(dataiter)#배치를 분해images, captions = batch#싱글 배치 단위의 정보 출력for i in range(BATCH_SIZE): img,cap = images[i],captions[i] caption_label = [dataset.vocab.itos[token] for token in cap.tolist()] print(caption_label) eos_index = caption_label.index('&lt;EOS&gt;') caption_label = caption_label[1:eos_index] caption_label = ' '.join(caption_label) show_image(img,caption_label) plt.show() 아래는 img_captioning_v2에 불러오기 위한 함수 1234567891011121314def get_data_loader(dataset,batch_size,shuffle=False,num_workers=1): pad_idx = dataset.vocab.stoi[&quot;&lt;PAD&gt;&quot;] collate_fn = CapsCollate(pad_idx=pad_idx,batch_first=True) data_loader = DataLoader( dataset=dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn ) return data_loader","link":"/2022/12/29/Personal_folder/for_project/2022-12-29-captioning_v1/"},{"title":"img_Captioning 코드 분해 Part 2","text":"이미지 캡셔닝(with attention) 현재 작업중인 파일입니다. 데이터는 캐글의 Flickr8k를 사용하였습니다. 코드 출처 : &lt;kaggle&gt; custom 파일(data_loader.py)는 Part 1에 있습니다. 파일 다운, 압축 해제 다운받고 압축을 해제하는 명령어입니다.(따로 구해서 편집함) 123456789101112# 파일 다운!wget -O Flickr8k_dataset.zip https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EXVy7_7pF5FIsPp6WfXXfWgBNfUKx8N1VrTisN8FbGYG9w?download=1 -q# 압축 해제import zipfilezipfile.ZipFile('Flickr8k_dataset.zip').extractall(path ='/content/dataset')# 로케이션 지정data_location = './dataset'# 아래와 같은 방법도 가능(magic 명령어)# !unzip -q Flickr8k_dataset.zip -d ./dataset 123456789101112#importsimport numpy as npimport torchimport torchvision.transforms as Timport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimimport torchvision.models as modelsfrom torch.utils.data import DataLoader,Dataset#custom imports (Part 1 참고)from data_loader import FlickrDataset,get_data_loader 1234567891011121314151617181920#show the tensor imageimport matplotlib.pyplot as pltdef show_image(img, title=None): &quot;&quot;&quot;Imshow for Tensor.&quot;&quot;&quot; #unnormalize img[0] = img[0] * 0.229 img[1] = img[1] * 0.224 img[2] = img[2] * 0.225 img[0] += 0.485 img[1] += 0.456 img[2] += 0.406 img = img.numpy().transpose((1, 2, 0)) plt.imshow(img) if title is not None: plt.title(title) plt.pause(0.001) “All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224” 이 특정 수치들은 pretraining에 사용된 모델들의 ImageNet 데이터셋의 학습 시에 얻어낸 값이고, ImageNet 데이터셋은 질 좋은 이미지들을 다량 포함하고 있기에 이런 데이터셋에서 얻어낸 값이라면 어떤 이미지 데이터 셋에서도 잘 작동할 것이라는 가정하에 이 값들을 기본 값으로 세팅해 놓은 것이다. 1234567891011121314151617181920212223242526272829303132333435363738#데이터셋, 데이터로더 만들기 (part1 파일 참고하세요)data_location = './dataset'# BATCH_SIZE = 256BATCH_SIZE = 6NUM_WORKER = 1 # 작업 환경에 따라 수정#transformstransforms = T.Compose([ T.Resize(226), T.RandomCrop(224), T.ToTensor(), T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))])#datasetdataset = FlickrDataset( root_dir = data_location+&quot;/Images&quot;, caption_file = data_location+&quot;/captions.txt&quot;, transform=transforms)#writing the dataloaderdata_loader = get_data_loader( dataset=dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKER, shuffle=True, batch_first=True)#vocab_sizevocab_size = len(dataset.vocab)print(vocab_size)# 2994device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) 전체적 모델은 seq2seq 모델입니다. encoder는 resnet50(CNN)을 사용하고, 디코더에는 바다나우 어텐션을 사용(RNN(LSTM)) 12345678910111213141516171819202122class EncoderCNN(nn.Module): def __init__(self): super(EncoderCNN, self).__init__() resnet = models.resnet50(pretrained=True) # 신경망의 모든 매개변수를 고정합니다(파인 튜닝 시 보통 사용) for param in resnet.parameters(): param.requires_grad_(False) modules = list(resnet.children())[:-2] #마지막 학습된 2개의 층은 사용하지 않겠다 self.resnet = nn.Sequential(*modules) def forward(self, images): features = self.resnet(images) #(batch_size,2048,7,7) features = features.permute(0, 2, 3, 1) # 차원 변경 #(batch_size,7,7,2048) features = features.view(features.size(0), -1, features.size(-1)) #(batch_size,49,2048) return features 미세조정(finetuning)을 하는 과정에서, 새로운 정답(label)을 예측할 수 있도록 모델의 대부분을 고정한 뒤 일반적으로 분류 계층(classifier layer)만 변경합니다.또한 캡셔닝(captioning)에서는 일반적으로 마지막 학습된 2개의 층(layer)은 사용하지 않습니다. 디코더 모델을 정의하기 전에 모델을 어떻게 쌓을지 보면 이해가 잘 될겁니다 변수값으로 vocab_size=2994, embedding=300, attention_dim=256, encoder_dim=2048, decoder_dim=512이 입력됩니다. 모델은 크게보면 embedding, attention, 나머지 모델들로 구성됩니다. 1234567891011121314151617(decoder): DecoderRNN( (embedding): Embedding(2994, 300) (attention): Attention( (W): Linear(in_features=512, out_features=256, bias=True) (U): Linear(in_features=2048, out_features=256, bias=True) (A): Linear(in_features=256, out_features=1, bias=True) ) (init_h): Linear(in_features=2048, out_features=512, bias=True) (init_c): Linear(in_features=2048, out_features=512, bias=True) (lstm_cell): LSTMCell(2348, 512) (f_beta): Linear(in_features=512, out_features=2048, bias=True) (fcn): Linear(in_features=512, out_features=2994, bias=True) (drop): Dropout(p=0.3, inplace=False) ) 그럼 코드를 보면서 해석해 봅시다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 디코더에서 사용되는 어텐션입니다.# Bahdanau Attentionclass Attention(nn.Module): def __init__(self, encoder_dim,decoder_dim,attention_dim): super(Attention, self).__init__() self.attention_dim = attention_dim self.U = nn.Linear(encoder_dim,attention_dim) self.W = nn.Linear(decoder_dim,attention_dim) self.A = nn.Linear(attention_dim,1) def forward(self, features, hidden_state): u_hs = self.U(features) #@@@인코더의 은닉상태?(query?) #(batch_size,num_layers,attention_dim) w_ah = self.W(hidden_state) #@@@디코더의 은닉상태?(key?혹은 value?) #(batch_size,attention_dim) combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1)) #(batch_size,num_layers,attemtion_dim) # w_ah에만 unsqueeze(1)하는 이유는 num_layers가 1이고 u_hs와 차원을 맞추기 위해 attention_scores = self.A(combined_states) #(batch_size,num_layers,1) attention_scores = attention_scores.squeeze(2) #(batch_size,num_layers) #어텐션 값 alpha = F.softmax(attention_scores,dim=1) #(batch_size,num_layers) attention_weights = features * alpha.unsqueeze(2) #(batch_size,num_layers,features_dim) #features와 연산을 위해 차원을 맞춰줌 attention_weights = attention_weights.sum(dim=1) #(batch_size,num_layers) return alpha,attention_weights 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107#Attention Decoderclass DecoderRNN(nn.Module): def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3): super().__init__() #save the model param self.vocab_size = vocab_size self.attention_dim = attention_dim self.decoder_dim = decoder_dim self.embedding = nn.Embedding(vocab_size,embed_size) self.attention = Attention(encoder_dim,decoder_dim,attention_dim) self.init_h = nn.Linear(encoder_dim, decoder_dim) #hidden self.init_c = nn.Linear(encoder_dim, decoder_dim) #cell self.lstm_cell = nn.LSTMCell(embed_size+encoder_dim,decoder_dim,bias=True) self.f_beta = nn.Linear(decoder_dim, encoder_dim) self.fcn = nn.Linear(decoder_dim,vocab_size) self.drop = nn.Dropout(drop_prob) def forward(self, features, captions): #vectorize the caption embeds = self.embedding(captions) # Initialize LSTM state h, c = self.init_hidden_state(features) # (batch_size, decoder_dim) #@@@ self.init_hidden_state는 init에 정의 안되있는데 어떤식으로 작동될 수 있는건지? #get the seq length to iterate seq_length = len(captions[0])-1 #Exclude the last one batch_size = captions.size(0) num_features = features.size(1) preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device) alphas = torch.zeros(batch_size, seq_length,num_features).to(device) for s in range(seq_length): alpha,context = self.attention(features, h) lstm_input = torch.cat((embeds[:, s], context), dim=1) h, c = self.lstm_cell(lstm_input, (h, c)) output = self.fcn(self.drop(h)) preds[:,s] = output alphas[:,s] = alpha return preds, alphas def generate_caption(self,features,max_len=20,vocab=None): # Inference part # Given the image features generate the captions batch_size = features.size(0) h, c = self.init_hidden_state(features) # (batch_size, decoder_dim) alphas = [] #starting input word = torch.tensor(vocab.stoi['&lt;SOS&gt;']).view(1,-1).to(device) embeds = self.embedding(word) captions = [] for i in range(max_len): alpha,context = self.attention(features, h) #store the apla score alphas.append(alpha.cpu().detach().numpy()) lstm_input = torch.cat((embeds[:, 0], context), dim=1) h, c = self.lstm_cell(lstm_input, (h, c)) output = self.fcn(self.drop(h)) output = output.view(batch_size,-1) #select the word with most val predicted_word_idx = output.argmax(dim=1) #save the generated word captions.append(predicted_word_idx.item()) #end if &lt;EOS detected&gt; if vocab.itos[predicted_word_idx.item()] == &quot;&lt;EOS&gt;&quot;: break #send generated word as the next caption embeds = self.embedding(predicted_word_idx.unsqueeze(0)) #covert the vocab idx to words and return sentence return [vocab.itos[idx] for idx in captions],alphas def init_hidden_state(self, encoder_out): mean_encoder_out = encoder_out.mean(dim=1) h = self.init_h(mean_encoder_out) # (batch_size, decoder_dim) c = self.init_c(mean_encoder_out) return h, c 12345678910111213141516class EncoderDecoder(nn.Module): def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3): super().__init__() self.encoder = EncoderCNN() self.decoder = DecoderRNN( embed_size=embed_size, vocab_size = len(dataset.vocab), attention_dim=attention_dim, encoder_dim=encoder_dim, decoder_dim=decoder_dim ) def forward(self, images, captions): features = self.encoder(images) outputs = self.decoder(features, captions) return outputs 1234567#Hyperparamsembed_size=300vocab_size = len(dataset.vocab)attention_dim=256encoder_dim=2048decoder_dim=512learning_rate = 3e-4 1234567891011#init modelmodel = EncoderDecoder( embed_size=300, vocab_size = len(dataset.vocab), attention_dim=256, encoder_dim=2048, decoder_dim=512).to(device)criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[&quot;&lt;PAD&gt;&quot;])optimizer = optim.Adam(model.parameters(), lr=learning_rate) 12345678910111213#helper function to save the modeldef save_model(model,num_epochs): model_state = { 'num_epochs':num_epochs, 'embed_size':embed_size, 'vocab_size':len(dataset.vocab), 'attention_dim':attention_dim, 'encoder_dim':encoder_dim, 'decoder_dim':decoder_dim, 'state_dict':model.state_dict() } torch.save(model_state,'attention_model_state.pth') Training Job from above configs 1234567891011121314151617181920212223242526272829303132333435363738394041num_epochs = 25print_every = 100for epoch in range(1,num_epochs+1): for idx, (image, captions) in enumerate(iter(data_loader)): image,captions = image.to(device), captions.to(device) # Zero the gradients. optimizer.zero_grad() # Feed forward outputs,attentions = model(image, captions) # Calculate the batch loss. targets = captions[:,1:] loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1)) # Backward pass. loss.backward() # Update the parameters in the optimizer. optimizer.step() if (idx+1)%print_every == 0: print(&quot;Epoch: {} loss: {:.5f}&quot;.format(epoch,loss.item())) #generate the caption model.eval() with torch.no_grad(): dataiter = iter(data_loader) img,_ = next(dataiter) features = model.encoder(img[0:1].to(device)) caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab) caption = ' '.join(caps) show_image(img[0],title=caption) model.train() #save the latest model save_model(model,epoch) Visualizing the attentions123456789101112131415161718192021222324252627282930313233343536373839#generate captiondef get_caps_from(features_tensors): #generate the caption model.eval() with torch.no_grad(): features = model.encoder(features_tensors.to(device)) caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab) caption = ' '.join(caps) show_image(features_tensors[0],title=caption) return caps,alphas#Show attentiondef plot_attention(img, result, attention_plot): #untransform img[0] = img[0] * 0.229 img[1] = img[1] * 0.224 img[2] = img[2] * 0.225 img[0] += 0.485 img[1] += 0.456 img[2] += 0.406 img = img.numpy().transpose((1, 2, 0)) temp_image = img fig = plt.figure(figsize=(15, 15)) len_result = len(result) for l in range(len_result): temp_att = attention_plot[l].reshape(7,7) ax = fig.add_subplot(len_result//2,len_result//2, l+1) ax.set_title(result[l]) img = ax.imshow(temp_image) ax.imshow(temp_att, cmap='gray', alpha=0.7, extent=img.get_extent()) plt.tight_layout() plt.show() 123456789#show any 1dataiter = iter(data_loader)images,_ = next(dataiter)img = images[0].detach().clone()img1 = images[0].detach().clone()caps,alphas = get_caps_from(img.unsqueeze(0))plot_attention(img1, caps, alphas)","link":"/2022/12/30/Personal_folder/for_project/2022-12-29-captioning_v2/"},{"title":"과적적합(Overfitting)을 막는 방법","text":"과적합을 막는 방법1.데이터의 양을 늘리기 만약, 데이터의 양이 적을 경우에는 의도적으로 기존의 데이터를 조금씩 변형하고 추가하여 데이터의 양을 늘리기도 하는데 이를 데이터 증식 또는 증강(Data Augmentation)이라고 합니다. 이미지의 경우에는 데이터 증식이 많이 사용되는데 이미지를 돌리거나 노이즈를 추가하고, 일부분을 수정하는 등으로 데이터를 증식시킵니다. 텍스트 데이터의 경우에는 데이터를 증강하는 방법으로 번역 후 재번역을 통해 새로운 데이터를 만들어내는 역번역(Back Translation) 등의 방법이 있습니다. 2.모델의 복잡도를 줄이기 3.가중치 규제(Regularization) 적용하기 4.드롭아웃(Dropout) 123456789101112131415#예시from tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Dropout, Densemax_words = 10000num_classes = 46model = Sequential()model.add(Dense(256, input_shape=(max_words,), activation='relu'))model.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50% &lt;- 여기서 드롭아웃 50%만 사용한다는 의미(랜덤으로)model.add(Dense(128, activation='relu'))model.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%model.add(Dense(num_classes, activation='softmax'))# 코드 출처 : 위키독스(wikidocs) pad_sequences 사용하는 이유 샘플의 길이가 서로 다를 수 있는데, 모델의 입력으로 사용하려면 모든 샘플의 길이를 통일해주는 함수 maxlen = 모든 데이터에 대해서 정규화 할 길이 padding = ‘post’로 입력 시 뒤에께 잘림, 보통 ‘post’를 사용함 1234567891011121314from tensorflow.keras.preprocessing.sequence import pad_sequencespad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]], maxlen=3, padding='pre') #결과array([[1, 2, 3], [4, 5, 6], [0, 7, 8]], dtype=int32)pad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]], maxlen=3, padding='post')#결과array([[1, 2, 3], [4, 5, 6], [7, 8, 0]], dtype=int32)","link":"/2022/12/01/Study_folder/DL(Deep_Learning)/2022-12-01-Overfitting/"},{"title":"callbacks - EarlyStopping, ModelCheckpoint","text":"1234567891011121314from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint# earlystopping == 더이상 개선이 안될 경우 스탑하는 기능.# 관심 가지는 변수 monitor = '변수'# 연속해서 4번의 변화까지는 참는다(계속 진행한다) patience = 4es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)# 모델 임시 저장 modelcheckpointmc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)# 콜백(callback) : 시스템이 어떤 상황이 되었을때 시스템에 의해 자동으로 호출되는 함수model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])history = model.fit(X_train, y_train, batch_size=128, epochs=2, callbacks=[es, mc], validation_data=(X_test, y_test))","link":"/2022/12/01/Study_folder/DL(Deep_Learning)/2022-12-03-callbacks/"},{"title":"Keras 손실(loss) 함수 종류","text":"손실 함수 손실 함수란?손실 함수는 값을 예측하려할 때 데이터에대한 예측값과 실제의 값을 비교하는 함수로 모델을 훈련시킬 때 오류를 최소화 시키기 위해 사용되는 함수이다.모델의 성능을 올리기 위해 loss 함수를 임의적으로 변형할 수 있다. 딥러닝에서 손실 함수는 여러가지 말로 표현됩니다. 비용 함수(cost function) = 손실 함수(loss function) = 오차 함수(error function) = 목적 함수(objective function) Mean_Squared_error 예측한 값과 실제 값 사이의 평균 제곱 오차를 정의한다. 공식이 매우 간단하며, 차가 커질수록 제곱 연산으로 인해서 값이 더욱 뚜렷해진다.그리고 제곱으로 인해서 오차가 양수이든 음수이든 누적 값을 증가시킨다. 123from sklearn.metrics import mean_squared_errorMSE = mean_squared_error(y, y_pred) RMSE(Root Mean Squared error) Mean_Squared_error에 Root를 씌운 값으로 왜곡을 줄여준다. 1234from sklearn.metrics import mean_squared_errorRMSE = mean_squared_error(y, y_pred)**0.5RMSE = mean_squared_error(y, y_pred, squared=False) RMSLE(Root mean squared Logarithmic error) 중간에 로그를 취해서 편차를 구함 1234from sklearn.metrics import mean_squared_log_errorMSLE = mean_squared_log_error(y, y_pred)RMSLE = mean_squared_log_error(y, y_pred)**0.5 Binary Crossentropy 이진 분류(0또는 1분류)를 할 경우 사용. sparse_categorical_crossentropy vs categorical_crossentropy 모두 다중 분류 손실 함수이다. categorical_crossentropy는 다중 분류 손실 함수로 one_hot_encoding 클래스이다 sparse_categorical_crossentropy는 int_type 클래스이다. 결론적으로, 원핫백터인 경우 categorical_crossentropyint인 경우에는 sparse_categorical_crossentropy를 사용하면 된다.","link":"/2022/12/20/Study_folder/DL(Deep_Learning)/2022-12-20-loss/"},{"title":"인코더(Encorder) - OneHotEncoder","text":"OneHotEncoder 사용하는 이유 : 일단 object type은 머신러닝 fit을 할 수 없고, 카테고리화를 하여 fitting을 정확하게 하기 위함이다. 방법에는 OneHotEncoder, get_dummies 등이 있다. 1OneHotEncoder().fit_transform(data3['column1','column2']) 1234567891011OneHotEncoder : 범주형 변수 -&gt; 이항 변수ex) 성적 column에 ['a','b','c','d'] 이렇게 있다면 OneHotEncoder 적용시 아래와같이 변경할 수 있다. 0 1 2 3 4(colums) --------- a 1 0 0 0 0 b 0 1 0 0 0 c 0 0 1 0 0 d 0 0 0 1 0 f 0 0 0 0 1 123456789101112# 예를 들어 data3을 OneHotEncoder로 돌리게 되면,# ex) 성별 : male(0), female(1)# 연령대 : 20대(0), 30대(1), 40대(1)# 성적 : a(0), b(1), c(2), d(3), f(4) =&gt;# 성별 데이터2, 연령대:3, 성적 :5data3 = np.array([ [0, 0, 0],#남자, 20대, a(성적) [0, 2, 4],#남자, 40대, f [1, 1, 1],#여자, 30대, b [1, 0, 3] #여자, 20대, c]) 1234567OneHotEncoder().fit_transform(data3).toarray()# 성별(1자리), 연령대(3), 성적(5) col &gt;[성별, 연 령 대 , 성 적 ~~~~~~~~]array([[1., 0., 1., 0., 0., 1., 0., 0., 0.], [1., 0., 0., 0., 1., 0., 0., 0., 1.], [0., 1., 0., 1., 0., 0., 1., 0., 0.], [0., 1., 1., 0., 0., 0., 0., 1., 0.]]) 인코딩된 값들을 확인하는 방법은 다음과 같습니다. 12345fitted_data3 = OneHotEncoder().fit(data3)fitted_data3.n_features_in_# 결과값은 &quot;3&quot;이 출력 (성별,연령대,성적)fitted_data3.categories_# 결과값은 [array([0, 1]), array([0, 1, 2]), array([0, 1, 3, 4])]","link":"/2022/11/21/Study_folder/ML(Machin_Learning)/2022-11-21-OneHotEncoder/"},{"title":"스케일링(Scaling)","text":"차원축소 (비지도 학습) 중 스케일링을 하는 이유차원 축소를 하는 이유 고차원 데이터(columns)의 의미적 특성 -&gt; 최대한 유지 -&gt; 저차원 축소 표현shape이 M x N 행렬에서(데이터프레임) PCA를 이용하여 N차원 -&gt; (N보다 작은)차원 차원의 저주를 피하기 위해 : outfitting을 피하기 위해 변수간 척도가 다름 -&gt; 비교가 불가능, 모수의 왜곡 -&gt; 변수의 표준화 작업이 필요(표준 정규분포로 표준화 등) 데이터 스케일링으로는 StandardScale, MinmaxScale, RobustScale, log1p 등이 있다. 스케일링 예시1.표준화(StandardScaling) 123456789101112131415# 표준화 방법은 아래와 같습니다.import numpy as npimport scipy.stats as ssfrom sklearn.preprocessing import StandardScalerdata = np.random.randint(30, size=(6,5))# (data - mean)/std 아래 3가지 방법 모두 동일한 결과값을 나타냄(data-np.mean(data, axis=0))/np.std(data,axis=0)== ss.zscore(data) ==StandardScaler().fit_transform(data) # 결과값array([[-1.2265705 , 0.67625223, 0.90236202, 1.28902742, -0.9166985 ], [-1.2265705 , -0.13525045, -1.44108561, -0.60505369, -0.9166985 ], [-0.19967427, -1.4877549 , -1.36027707, 1.28902742, -0.64168895], [ 1.51181945, 1.21725401, 0.82155348, -1.39425415, 1.74172714], [ 0.48492322, 0.81150267, 0.49831932, 0.02630668, -0.1833397 ], [ 0.65607259, -1.08200356, 0.57912786, -0.60505369, 0.9166985 ]]) 2.RobustScaler 표준화(이상치가 적다는 가정에서 수행), but 만약 이상치가 많다면? 이상치를 제거후에 표준화 평균 대신 덜 민감한 중위수, 표준편차 대신 IQR(data - mean)/std =&gt; (data - median)/IQR == RobustScaler 1234567891011# (data - median)/IQR == RobustScaler 이상치를 제거하고 mean대신 median값을 빼줌# 그래프 그려보면 표준화보다 이쁘게 나옴from sklearn.preprocessing import RobustScalerRobustScaler().fit_transform(data) # 결과값array([[-1.2265705 , 0.67625223, 0.90236202, 1.28902742, -0.9166985 ], [-1.2265705 , -0.13525045, -1.44108561, -0.60505369, -0.9166985 ], [-0.19967427, -1.4877549 , -1.36027707, 1.28902742, -0.64168895], [ 1.51181945, 1.21725401, 0.82155348, -1.39425415, 1.74172714], [ 0.48492322, 0.81150267, 0.49831932, 0.02630668, -0.1833397 ], [ 0.65607259, -1.08200356, 0.57912786, -0.60505369, 0.9166985 ]]) 3.MinMaxScaler 123456789from sklearn.preprocessing import MinMaxScaler, minmax_scaleMinMaxScaler().fit_transform(data)minmax_scale(data, axis=0) #위와 동일array([[0. , 0.8 , 1. , 1. , 0. ], [0. , 0.5 , 0. , 0.29411765, 0. ], [0.375 , 0. , 0.03448276, 1. , 0.10344828], [1. , 1. , 0.96551724, 0. , 1. ], [0.625 , 0.85 , 0.82758621, 0.52941176, 0.27586207], [0.6875 , 0.15 , 0.86206897, 0.29411765, 0.68965517]]) 4.log1p 그냥 log가 아닌 1p를 사용하는 이유는 0에 가까운 아주 작은 양수의 경우 (ex. 0.00001) 음의 무한대에 가까워지게 된다. 즉 -inf가 나오게되기에 1을 더한 log1p를 사용한다 123import numpy as npdf = np.log1p(df)np.log1p(data)","link":"/2022/11/21/Study_folder/ML(Machin_Learning)/2022-11-21-Scaling/"},{"title":"인코더(Encorder) - pd.get_dummies","text":"get_dummies OneHotEncoder와 동일하며 편한 방법으로 사용하면 됩니다. 1234567891011121314151617181920212223# 예시 데이터df = pd.DataFrame({'C1': np.random.randn(20),'C2': ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a','b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']})C1 C20 0.839674 a1 -0.514992 a2 -2.072183 a3 -1.664719 a4 0.881287 a5 -1.151537 a6 0.768122 a7 0.942399 a8 0.384411 a9 -1.072716 a10 0.216321 b11 -0.018060 b12 -0.297903 b13 1.430495 b14 -0.258497 b15 0.509483 b16 0.526239 b17 1.305845 b18 -0.900900 b19 -0.463382 b 123456789101112131415161718192021222324# C2열 더미화pd.get_dummies(df,columns=['C2'],prefix='word') C1 word_a word_b0 0.303355 1 01 0.320274 1 02 -1.192643 1 03 -0.582344 1 04 1.233197 1 05 0.053738 1 06 -0.761975 1 07 -0.702154 1 08 0.949892 1 09 1.346181 1 010 1.883862 0 111 -0.766519 0 112 -0.417308 0 113 -0.674398 0 114 0.589356 0 115 -1.489583 0 116 0.077735 0 117 0.479897 0 118 1.244302 0 119 -2.331532 0 1","link":"/2022/11/21/Study_folder/ML(Machin_Learning)/2022-11-21-get_dummies/"},{"title":"globals(), eval()","text":"globas() 함수를 사용하는 이유 for문을 돌리기 전에 함수를 정의하면 1~2개 정도는 그냥 만들면 되지만 여러개의 함수를 명명할 경우 생각보다 보기에 안좋고 반복 작업이 될 수 있기에 globals() 함수를 사용한다. 1234567# 예를 들어 item_1 ~ item5 까지 함수를 만든다고 가정해보자# globals()를 사용하지 않는다면, 아래와 같이 지저분하게 보인다.item_1 = []item_2 = []item_3 = []item_4 = []item_5 = [] 123# 아래와 같이 2줄이면, 여러개의 함수를 생성 가능하다.for i in range(1,6): globals()[f'items_{i}'] = [] eval() 함수를 사용하는 이유 eval함수는 ' ' &lt;-이러한 홑따음표로 묶인것에서 홑따음표를 제거해주는 함수이다. for문에서 규칙성이 있는 함수를 호출(추출)할 경우 사용됩니다. 이게 무슨말인지 이해가 잘 안갈 수 있어서 예시를 보여주겠습니다. 123456789item_1 = ['가','나','다']item_2 = ['라','마','바']# eval을 사용하지 않은 경우for i in range(1,3): print(f'item_{i}')#출력 결과item_1item_2 1234567# eval을 사용한 경우for i in range(1,3): print(eval(f'item_{i}'))#출력 결과['가', '나', '다']['라', '마', '바'] globals()와 eval()을 같이 사용도 가능합니다1234567891011# 아래와 같이 새로운 변수명(함수)에 입력도 가능합니다.test_1 = [1,2,3]test_2 = [4,5,6]for i in range(1,3): globals()[f'items_{i}'] = eval(f'test_{i}')print(items_1)print(items_2)# 출력 결과[1, 2, 3][4, 5, 6]","link":"/2022/11/23/Study_folder/Basic_study/2022-11-23-globals(),eval()/"},{"title":"Class(클래스, 클래스 상속)","text":"클래스(Class) 클래스 : 클래스는 데이터와 기능을 함께 묶는 방법 인스턴스 : 클래스로 정의된 객체를 프로그램 상에서 이용할 수 있게 만든 변수 인스턴스를 사용하기 위해 클래스를 먼저 만들어야 함 클래스의 함수 : 메소드 &lt; 클래스 내부의 함수 클래스의 명명은 맨 앞자는 대분자를 쓰는게 일반적입니다. 아래는 간단한 예제 입니다. 예시1 - 클래스 생성12345678910111213141516171819202122232425262728293031323334class Charicters : # 클래스의 생성자 # def __init__ 은 이와 유사한 형식을 따라야 함 def __init__(self, name, hobby): self.name = name #클래스의 맴버 self.hobby = hobby #클래스의 맴버 #클래스의 메소드 def xxx(self): print(f'이름 : {self.name}, 취미 : {self.hobby}') #setter 메소드 def set_hobby(self, hobby): self.hobby = hobby #클래스 계승을 위한 함수(밑의 예시용) def nice(self): print('nice to meet you')a = Characters('조인환', '클라이밍')a.xxx()# 출력 결과# 이름 : 조인환, 취미 : 클라이밍a.hobby# 출력 결과# 클라이밍a = Characters('조인환', '클라이밍')a.set_hobby('산책')a.xxx()# 출력 결과# 이름 : 조인환, 취미 : 산책 예시 2 - 클래스 상속 - 딥러닝 모델 생성(pytorch) 부모와 자식 관계가 존재 부모 클래스 : 기존의 클래스 자식 클래스 : 부모 클래스를 상속 받은 클래스 12345678910111213141516#클래스에서 클레스를 만드는것을 클래스 상속이라고 합니다.class Baby(Characters) : def hansome(self): #super()를 사용하면 부모 클래스의 함수를 사용 가능합니다. super().nice() def hobi_is(self): print(f'{self.name}의 취미는 {self.hobby}입니다')b = Baby('조인환','클라이밍')b.nice()#nice to meet youb.hobi_is()# 조인환의 취미는 클라이밍입니다 예시 3 - 클래스 상속 - 딥러닝 모델 생성(pytorch)123456789101112131415161718192021222324252627282930313233343536import torchimport torch.nn as nn#NeuralNet이라는 클래스를 만듬(nn.Module을 상속)class NeuralNet(nn.Module): def __init__(self): # torch.nn.Module의 NeuralNet을 사용한다는 의미 # (from nn import NeuralNet)를 수행 super(NeuralNet, self).__init__() #Conv2d를 2번, Linear를 2번 실행한다는 의미 self.conv1 = nn.Conv2d(1,8,2) self.conv2 = nn.Conv2d(8,10,3) self.fc1 = nn.Linear(10*5*5,60) self.fc2 = nn.Linear(60,10) def forward(self,x): x = F.max_pool2d(F.relu(self.conv1(x)), 2) x = F.max_pool2d(F.relu(self.conv2(x)), 2) #Linear는 flatten한 데이터만 사용 가능해서 데이터 편집 x = x.view(-1,self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = self.fc2(x) return x def num_flat_features(self, x): size = x.size()[1:] num_features = 1 for s in size: num_features *= s return num_featuresnet = NeuralNet()print(net)","link":"/2022/11/30/Study_folder/Basic_study/2022-11-30-Class/"},{"title":"pickle()-파일,리스트 저장","text":"pickle 파이썬에서 작업중이던 리스트, 딕셔너리 등을 저장해서 다른 위치에서 열고 싶을 경우 피클 파일을 이용하여 저장, 불러오기가 가능합니다. with open(파일 이름, 파일 모드) as f: # &lt;- f라는 이름의 파일로 파일 이름, 모드를 연다는 의미. 12345678import picklelist_save = [1, 2, 3, 4]# 피클 파일로 저장하는 겁니다. 파일 명은 &quot;list_ex.pkl&quot;로 저장# 편집 가능하게 'wb'with open(&quot;list_ex.pkl&quot;,&quot;wb&quot;) as f: pickle.dump(list_save, f) 12345678# 불러오는 방법입니다import picklewith open(&quot;list_save.pkl&quot;,&quot;rb&quot;) as f: list_load = pickle.load(f)print(list_load)# [1, 2, 3, 4] 파일 모드 설명(보통 wb,rb를 많이 사용)) |file_mode|기능|설명||’r’|읽기 전용|파일을 읽어오는 기능이며, 파일이 없으면 에러||’w’|쓰기 전용|파일이 있으면 내용을 덮어 씀||’a’|추가|파일이 없으면 파일을 생성||’b’|바이너리 모드|파일의 내용을 그대로 읽고, 값을 그대로 사용| joblib으로 저장, 불러오기1234567#joblib으로 dump하여 피클 형식으로 저장하기.import joblibjoblib.dump((x_train, x_test, y_train, y_test), 'review.pkl')# 불러오기x_train, x_test, y_train, y_test = joblib.load('review.pkl')","link":"/2022/11/30/Study_folder/Basic_study/2022-11-30-save-list(pickle)/"},{"title":"Portfolio(미니 프로젝트 1)","text":"미니 프로젝트 1 문제123456789101112131415161718192021222324252627282930313233343536373839404142431. tmdb_5000_movies 데이터 셋 분석1) 예산과 장르 관계?2) 키워드로 많이 사용된 단어는? 3) 장르와 키워드 관계는?4) 평균 평점과 장르 사이의 관계?5) 연도별로 많이 제작된 영화 장르는?6) 인기도와 예산 관계는?7) 영화 run time과 인기도 사이에 관계가 있을까?2. tmdb_5000_movies 데이터 기반 추천시스템 제작​3. dataset 데이터 분석 및 연관 규칙 생성1) dataset 파일 분석2) 연도별 많이 / 적게 팔린 아이템은?3) member id 에 따른 구매 연(월, 요일)도, 아이템 분석- 무슨 요일에 와서 구매를 많이 했을까?4) 연관 규칙 생성5) 연관 규칙에 따른 vip member에게 어떤 상품을 추천할까?-vip는 매출횟수가 가장 많은 상위 100명 ​4. 와인의 화학 조성을 사용하여 와인의 종류를 예측하기 위한 데이터이다. load_wine() 명령으로 로드하며 다음과 같이 구성되어 있다. 와인의 종류를 예측할 수 있는 모델을 생성하시오.from sklearn.datasets import load_wine 1,2번 문제1234import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as sns 123t_df = pd.read_csv('../Downloads/tmdb_5000_movies.csv')g_df = pd.read_csv('../Downloads/Groceries_dataset.csv')pd.options.display.max_columns=50 123t_df.drop(columns=['homepage','tagline','status'],inplace=True)t_df.dropna(inplace=True)t_df.reset_index(drop=True,inplace=True) 1234567# json 열을 name만 추출하여 전처리, keywords는 공백이 유의미한 단위이므로 따로 처리json_col = ['genres','production_companies','production_countries','spoken_languages']for i in json_col: t_df[i] = t_df[i].apply(lambda x : eval(x)) t_df[i] = t_df[i].apply(lambda x : [d['name'] for d in x]).apply(lambda x : ' '.join(x))t_df['keywords'] = t_df['keywords'].apply(lambda x : eval(x))t_df['keywords'] = t_df['keywords'].apply(lambda x : [d['name'] for d in x]).apply(lambda x : ','.join(x)) 123456# 1) 예산과 장르 관계?t_df['budget_cat'] = pd.qcut(t_df['budget'],5,duplicates='drop')t_df.pivot_table(index='budget_cat',columns='genres',aggfunc='count')# 웨스턴풍은 저예산이 많고, 어드벤쳐쪽은 고예산이 많다. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } budget ... vote_count genres Action Action Adventure Action Adventure Animation Comedy Family Action Adventure Animation Comedy Family Fantasy Romance Action Adventure Animation Comedy Family Fantasy Science Fiction Action Adventure Animation Comedy Science Fiction Action Adventure Animation Family Action Adventure Animation Family Fantasy Action Adventure Animation Fantasy Science Fiction Action Adventure Animation Science Fiction Thriller Action Adventure Comedy Action Adventure Comedy Crime Action Adventure Comedy Crime Drama Action Adventure Comedy Crime Mystery Thriller Action Adventure Comedy Crime Romance Thriller Action Adventure Comedy Crime Thriller Action Adventure Comedy Drama Family Music Romance Action Adventure Comedy Drama Foreign Action Adventure Comedy Drama Mystery Action Adventure Comedy Drama Science Fiction Thriller Action Adventure Comedy Family Action Adventure Comedy Family Fantasy Action Adventure Comedy Family Fantasy Science Fiction Action Adventure Comedy Family Science Fiction ... War War Action War Action Adventure Drama Thriller War Action Drama History Thriller War Adventure Drama Romance War Comedy Drama War Crime Drama Mystery Romance Thriller War Drama War Drama Action War Drama History War Drama History Action War Drama History Action Romance War Drama Romance War History Action Adventure Drama Romance War History Drama War Western Western Western Action Drama History Western Adventure Western Animation Adventure Comedy Family Western Comedy Western Drama Western Drama Adventure Thriller Western History Western History War budget_cat (-0.001, 7500000.0] 27 6 1 0 1 0 1 0 0 1 0 3 1 1 0 0 0 0 1 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 2 0 2 0 0 1 0 0 1 15 1 0 0 1 0 0 0 0 (7500000.0, 22000000.0] 0 3 0 0 0 0 0 0 0 0 0 4 2 0 0 0 0 1 0 0 1 0 0 1 0 ... 1 0 0 0 0 1 0 2 0 0 0 1 0 0 1 0 2 0 0 0 0 2 0 1 0 (22000000.0, 50000000.0] 0 7 0 0 0 1 0 0 0 0 1 5 0 0 0 0 0 0 0 0 0 2 0 0 1 ... 0 0 0 1 2 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 (50000000.0, 380000000.0] 0 5 7 2 0 0 0 1 1 0 0 8 0 0 1 1 3 0 0 1 0 1 1 1 0 ... 1 1 1 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 4 rows × 18800 columns 123456789101112# 2) 키워드로 많이 사용된 단어는? t_df.keywordsfrom collections import Countera = []for i in range(len(t_df['keywords'])): a.append(t_df['keywords'][i])a = ''.join(a)words = a.split(',')counter = Counter(words)print(counter.most_common(5))# ('independent film', 192), ('murder', 172), ('violence', 146), ('dystopia', 120), ('duringcreditsstinger', 116) [('independent film', 192), ('murder', 172), ('violence', 146), ('dystopia', 120), ('duringcreditsstinger', 116)] 12345678910111213141516171819202122232425262728293031# 3) 장르와 키워드 관계는?&quot;&quot;&quot;1) Tf(Term Frequency)하나의 문서(문장)에서 특정 단어가 등장하는 횟수2) Idf(Inverse Document Frequency)Df(Document Frequency)는 문서 빈도. 특정 단어가 몇 개의 문서(문장)에서 등장하는지를 수치화 한 것. 그것의 역수가 idf다.보통 그냥 역수를 취하기 보다는 아래처럼 수식화한다. 역수 개념을 사용하는 이유는, 적은 문서(문장)에 등장할수록 큰 숫자가 되게하고 반대로 많은 문서(문장)에 등장할수록 숫자를 작아지게 함으로써여러 문서(문장)에 의미 없이 사용되는 단어의 가중치를 줄이기 위해서다.&quot;&quot;&quot;from sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.metrics.pairwise import cosine_similaritytfidf_vector = TfidfVectorizer()#장르와 키워드를 백터화tfidf_matrix = tfidf_vector.fit_transform(t_df['genres']+ ' ' + t_df['keywords']).toarray()#벡터화되기 전 이름들을 추출tfidf_matrix_feature = tfidf_vector.get_feature_names_out()#데이터 프레임으로 만들기tfidf_matrix = pd.DataFrame(tfidf_matrix, columns=tfidf_matrix_feature, index = t_df.title)#코사인 유사도로 유사 관계 파악cosine_sim = cosine_similarity(tfidf_matrix)cosine_sim_df = pd.DataFrame(cosine_sim, index = t_df.title, columns = t_df.title)cosine_sim_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } title Avatar Pirates of the Caribbean: At World's End Spectre The Dark Knight Rises John Carter Spider-Man 3 Tangled Avengers: Age of Ultron Harry Potter and the Half-Blood Prince Batman v Superman: Dawn of Justice Superman Returns Quantum of Solace Pirates of the Caribbean: Dead Man's Chest The Lone Ranger Man of Steel The Chronicles of Narnia: Prince Caspian The Avengers Pirates of the Caribbean: On Stranger Tides Men in Black 3 The Hobbit: The Battle of the Five Armies The Amazing Spider-Man Robin Hood The Hobbit: The Desolation of Smaug The Golden Compass King Kong ... Rampage Slacker Dutch Kills Dry Spell Flywheel Backmask The Puffy Chair Stories of Our Lives Breaking Upwards All Superheroes Must Die Pink Flamingos Clean The Circle Tin Can Man Cure On The Downlow Sanctuary: Quite a Conundrum Bang Primer Cavite El Mariachi Newlyweds Signed, Sealed, Delivered Shanghai Calling My Date with Drew title Avatar 1.000000 0.033911 0.017435 0.004221 0.248501 0.035139 0.024894 0.059988 0.023661 0.022763 0.043666 0.022542 0.022596 0.010072 0.088634 0.043253 0.077686 0.102427 0.143133 0.092661 0.021569 0.029362 0.045073 0.022733 0.013829 ... 0.010297 0.008659 0.000000 0.061893 0.000000 0.000000 0.023269 0.000000 0.031020 0.074724 0.030946 0.000000 0.000000 0.0 0.000000 0.012150 0.000000 0.000000 0.048523 0.000000 0.006793 0.056478 0.026349 0.0 0.0 Pirates of the Caribbean: At World's End 0.033911 1.000000 0.021037 0.034132 0.012511 0.111809 0.011378 0.017092 0.044458 0.027466 0.029152 0.027200 0.474144 0.012153 0.029551 0.068232 0.038306 0.208616 0.008180 0.028966 0.037160 0.029612 0.030120 0.027430 0.142986 ... 0.012424 0.000000 0.000000 0.000000 0.015401 0.000000 0.000000 0.000000 0.000000 0.000000 0.018194 0.038199 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.008197 0.000000 0.021293 0.0 0.0 Spectre 0.017435 0.021037 1.000000 0.078061 0.070146 0.060568 0.030749 0.111236 0.017189 0.057658 0.103059 0.549934 0.022014 0.018065 0.062035 0.076458 0.062227 0.023734 0.012160 0.023387 0.021013 0.024623 0.018135 0.096466 0.024804 ... 0.045032 0.000000 0.097970 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.012025 0.000000 0.000000 0.0 0.070458 0.000000 0.000000 0.000000 0.000000 0.000000 0.123730 0.000000 0.000000 0.0 0.0 The Dark Knight Rises 0.004221 0.034132 0.078061 1.000000 0.004502 0.051288 0.024913 0.062652 0.000000 0.180473 0.109040 0.109994 0.005329 0.004373 0.150406 0.023430 0.059022 0.005746 0.065193 0.005662 0.082179 0.005961 0.000000 0.000000 0.012210 ... 0.061043 0.079586 0.186559 0.000000 0.007272 0.019680 0.020162 0.094137 0.000000 0.120491 0.013621 0.010274 0.027136 0.0 0.019153 0.004788 0.032963 0.010261 0.093245 0.023555 0.033142 0.000000 0.007541 0.0 0.0 John Carter 0.248501 0.012511 0.070146 0.004502 1.000000 0.012965 0.049956 0.088164 0.010223 0.034291 0.154023 0.024046 0.013092 0.032782 0.151277 0.060955 0.105641 0.073203 0.119286 0.063651 0.012497 0.031320 0.078151 0.057371 0.014752 ... 0.010983 0.009237 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.079708 0.000000 0.000000 0.000000 0.0 0.034295 0.012961 0.000000 0.000000 0.051759 0.000000 0.007246 0.000000 0.000000 0.0 0.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... El Mariachi 0.006793 0.008197 0.123730 0.033142 0.007246 0.008494 0.000000 0.009899 0.000000 0.008641 0.009172 0.053005 0.008578 0.007039 0.009297 0.000000 0.009326 0.009248 0.011084 0.009113 0.008188 0.009594 0.000000 0.000000 0.009665 ... 0.056640 0.000000 0.146807 0.000000 0.000000 0.031677 0.000000 0.000000 0.000000 0.032542 0.010962 0.000000 0.000000 0.0 0.019168 0.000000 0.053056 0.000000 0.009552 0.037913 1.000000 0.000000 0.000000 0.0 0.0 Newlyweds 0.056478 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.053283 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ... 0.000000 0.051092 0.000000 0.332759 0.000000 0.000000 0.411991 0.000000 0.549240 0.000000 0.036634 0.000000 0.000000 0.0 0.000000 0.000000 0.275376 0.000000 0.000000 0.000000 0.000000 1.000000 0.154088 0.0 0.0 Signed, Sealed, Delivered 0.026349 0.021293 0.000000 0.007541 0.000000 0.022065 0.000000 0.064265 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.060542 0.000000 0.008210 0.000000 0.000000 0.000000 0.000000 0.000000 0.005281 ... 0.009198 0.007873 0.033924 0.051274 0.006188 0.000000 0.080640 0.080105 0.084631 0.000000 0.005645 0.008742 0.023091 0.0 0.042630 0.004074 0.042432 0.008732 0.005635 0.000000 0.000000 0.154088 1.000000 0.0 0.0 Shanghai Calling 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ... 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.0 0.0 My Date with Drew 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.132800 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ... 0.000000 0.044517 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.0 1.0 4799 rows × 4799 columns 1234567891011121314151617181920#4) 평균 평점과 장르 사이의 관계?tfidf_vector = TfidfVectorizer()#장르를 백터화tfidf_matrix = tfidf_vector.fit_transform(t_df['genres']).toarray()#벡터화되기 전 이름들을 추출tfidf_matrix_feature = tfidf_vector.get_feature_names_out()#유사도 값이 아닌 이진화tfidf_matrix = pd.DataFrame(tfidf_matrix, columns=tfidf_matrix_feature)tfidf_matrix = tfidf_matrix.applymap(lambda x : 0 if x == 0 else 1)# 평점을 범주화 (평점 5가 가장 높음)t_df['cat_voteav'] = pd.qcut(t_df['vote_average'],5,labels=[1,2,3,4,5])tfidf_matrix['vote_average'] = t_df['cat_voteav']tfidf_matrix.groupby('vote_average').sum()# 호러*코미디가 평점이 낮고, 음악&amp;역사&amp;드라마가 평점이 높은 경향이 있다. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } action adventure animation comedy crime documentary drama family fantasy fiction foreign history horror movie music mystery romance science thriller tv war western vote_average 1 266 149 31 427 95 19 273 122 99 137 7 10 203 4 22 59 152 137 288 4 7 15 2 291 203 54 458 146 9 416 124 109 122 3 21 130 1 32 70 207 122 309 1 19 12 3 279 162 45 397 190 13 503 102 83 104 5 44 95 1 39 102 192 104 321 1 29 13 4 173 144 51 267 139 24 578 92 67 98 13 54 55 1 50 55 200 98 200 1 36 16 5 145 132 53 173 126 43 526 73 66 74 6 68 36 1 42 62 143 74 156 1 53 26 123456789# 5) 연도별로 많이 제작된 영화 장르는?# 평점을 범주화 (평점 5가 가장 높음)t_df['cat_year'] = pd.qcut(t_df['vote_average'],10,labels=range(1,11))tfidf_matrix['cat_year'] = t_df['cat_year']tfidf_matrix.groupby('cat_year').sum(numeric_only=True)#액션은 최근에 줄어드는 경향이 있음.#드라마는 늘어나는 경향이 있음. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } action adventure animation comedy crime documentary drama family fantasy fiction foreign history horror movie music mystery romance science thriller tv war western cat_year 1 135 69 12 203 45 14 136 61 42 76 2 2 114 2 12 29 69 76 145 2 3 10 2 131 80 19 224 50 5 137 61 57 61 5 8 89 2 10 30 83 61 143 2 4 5 3 127 79 24 205 59 5 156 56 44 59 1 4 56 0 17 27 83 59 136 0 3 5 4 164 124 30 253 87 4 260 68 65 63 2 17 74 1 15 43 124 63 173 1 16 7 5 117 68 10 154 85 3 187 37 26 43 4 13 44 1 18 43 81 43 136 1 13 4 6 162 94 35 243 105 10 316 65 57 61 1 31 51 0 21 59 111 61 185 0 16 9 7 95 72 28 130 59 7 231 41 33 52 4 27 32 0 24 27 87 52 95 0 17 8 8 78 72 23 137 80 17 347 51 34 46 9 27 23 1 26 28 113 46 105 1 19 8 9 74 59 18 88 59 17 235 24 26 31 2 31 19 0 23 32 68 31 85 0 21 10 10 71 73 35 85 67 26 291 49 40 43 4 37 17 1 19 30 75 43 71 1 32 16 12345# 6) 인기도와 예산 관계는?t_df.pivot_table(index = t_df['cat_voteav'], columns='budget_cat',aggfunc='count')['budget']#평점 5가 높은거임.#평점이 높다고 예산이 높은 영화이지는 않지만, 평점이 낮으면 저예산 영화일 확률은 높다. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } budget_cat (-0.001, 7500000.0] (7500000.0, 22000000.0] (22000000.0, 50000000.0] (50000000.0, 380000000.0] cat_voteav 1 508 174 177 138 2 352 212 250 234 3 345 217 249 214 4 375 202 188 157 5 341 180 139 147 123456# 7) 영화 run time과 인기도 사이에 관계가 있을까?t_df['cat_runtime'] = pd.qcut(t_df['runtime'],5,labels=range(1,6))t_df.pivot_table(index = t_df['cat_runtime'], columns='cat_voteav',aggfunc='count')['budget']# 확실히 런타임이 길면 길 수록 평점이 높은 경향이 있다. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cat_voteav 1 2 3 4 5 cat_runtime 1 389 229 156 111 87 2 278 241 202 158 87 3 195 248 240 169 123 4 90 217 238 245 167 5 45 113 189 239 343 1234567891011121314def genre_recommendations(df , mat, items): a = mat.loc[df].sort_values(ascending=False)[1:21] scale_score = pd.DataFrame(a) scale_score = scale_score.reset_index() for i in range(20): scale_score.loc[i,['score']] = (items[items['title'] == a.index[i]]['vote_average']).values #스케일링 scale_score['total_score'] = scale_score.iloc[:,1]+(scale_score.iloc[:,2]/0.8) return scale_score.sort_values(by='total_score',ascending=False)[:10]# 추천 시스템 예시 - Avatar와 유사한 영화 10개 추천genre_recommendations('Avatar',cosine_sim_df,t_df).reset_index(drop=True) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } title Avatar score total_score 0 Alien 0.377950 7.9 10.252950 1 Aliens 0.427672 7.7 10.052672 2 Star Trek Into Darkness 0.420840 7.4 9.670840 3 Gravity 0.417716 7.3 9.542716 4 Treasure Planet 0.320138 7.2 9.320138 5 Stargate: The Ark of Truth 0.315874 6.9 8.940874 6 Spaceballs 0.329561 6.7 8.704561 7 Starship Troopers 0.298873 6.7 8.673873 8 Silent Running 0.410423 6.3 8.285423 9 Space Dogs 0.391469 6.3 8.266469 3번 문제123456789101112131415# #3. dataset 데이터 분석 및 연관 규칙 생성# 1) dataset 파일 분석# 2) 연도별 많이 / 적게 팔린 아이템은?# 3) member id 에 따른 구매 연(월, 요일)도, 아이템 분석# - 무슨 요일에 와서 구매를 많이 했을까?# 4) 연관 규칙 생성# 5) 연관 규칙에 따른 vip member에게 어떤 상품을 추천할까?# -vip는 매출횟수가 가장 많은 상위 100명 12#Date열을 datetime으로 변경g_df['Date'] = pd.to_datetime(g_df['Date'],format='%d-%m-%Y' ) 12# na.nan데이터 없음g_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 38765 entries, 0 to 38764 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Member_number 38765 non-null int64 1 Date 38765 non-null datetime64[ns] 2 itemDescription 38765 non-null object dtypes: datetime64[ns](1), int64(1), object(1) memory usage: 908.7+ KB 1234# 2) 연도별 많이 / 적게 팔린 아이템은?g_df['year'] = g_df['Date'].dt.yeara = g_df.pivot_table(index='year',columns='itemDescription',aggfunc='count')['Date']a.max(axis=1) year 2014 1038.0 2015 1464.0 dtype: float64 123a#Whole milk가 연도별로 가장 높고,#bags, toilet cleaner 가장 적다 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } itemDescription Instant food products UHT-milk abrasive cleaner artif. sweetener baby cosmetics bags baking powder bathroom cleaner beef berries beverages bottled beer bottled water brandy brown bread butter butter milk cake bar candles candy canned beer canned fish canned fruit canned vegetables cat food ... sparkling wine specialty bar specialty cheese specialty chocolate specialty fat specialty vegetables spices spread cheese sugar sweet spreads syrup tea tidbits toilet cleaner tropical fruit turkey vinegar waffles whipped/sour cream whisky white bread white wine whole milk yogurt zwieback year 2014 37.0 160.0 12.0 13.0 1.0 1.0 67.0 11.0 177.0 128.0 109.0 319.0 504.0 17.0 323.0 273.0 126.0 54.0 34.0 136.0 266.0 70.0 8.0 49.0 109.0 ... 25.0 110.0 37.0 125.0 20.0 10.0 23.0 48.0 155.0 34.0 13.0 19.0 12.0 5.0 364.0 27.0 29.0 166.0 365.0 3.0 213.0 82.0 1038.0 640.0 24.0 2015 23.0 163.0 10.0 16.0 2.0 3.0 55.0 6.0 339.0 199.0 142.0 368.0 429.0 21.0 248.0 261.0 137.0 39.0 32.0 83.0 451.0 46.0 13.0 33.0 68.0 ... 21.0 100.0 35.0 115.0 9.0 1.0 17.0 52.0 110.0 35.0 8.0 8.0 10.0 NaN 668.0 53.0 22.0 114.0 297.0 5.0 149.0 94.0 1464.0 694.0 36.0 2 rows × 167 columns 12345#3g_df['weekday'] = g_df['Date'].dt.weekday#0이 월요일 6이 일요일g_df['month'] = g_df['Date'].dt.monthg_df.groupby('Member_number').sum(numeric_only=True) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year weekday month Member_number 1000 26192 43 76 1001 24175 35 51 1002 16116 46 36 1003 16114 24 30 1004 42296 27 150 ... ... ... ... 4996 20150 24 90 4997 12088 36 50 4998 4030 4 20 4999 32236 58 62 5000 14101 27 34 3898 rows × 3 columns 1g_df_dummies = pd.get_dummies(g_df,columns=['weekday','month']) 1234g_df_dummies.groupby('Member_number').sum(numeric_only=True).iloc[:,1:].sum()#0이 월요일 6이 일요일#요일은 상관 없이 구매량은 비슷하다.#월도 크게 차이가 나는 편은 아니다. weekday_0 5382 weekday_1 5558 weekday_2 5562 weekday_3 5620 weekday_4 5562 weekday_5 5551 weekday_6 5530 month_1 3324 month_2 2997 month_3 3133 month_4 3260 month_5 3408 month_6 3264 month_7 3300 month_8 3496 month_9 3059 month_10 3261 month_11 3254 month_12 3009 dtype: int64 1# 4,5 123456# 매출횟수 상위 100명에게 vip타이틀 부여b = g_df.groupby('Member_number')['itemDescription'].size().sort_values(ascending=False)[:100]g_df['vip']= g_df['Member_number'].apply(lambda x : 1 if x in b.index else 0)#vipvip = g_df[g_df['vip']==1] /var/folders/pr/27tft1vj6396wqnj02ngz5p80000gn/T/ipykernel_28378/1060107556.py:2: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`. b = g_df.groupby('Member_number')['itemDescription'].size().sort_values(ascending=False)[:100] 12345678910111213141516171819202122232425262728293031323334353637383940from mlxtend.preprocessing import TransactionEncoderfrom mlxtend.frequent_patterns import apriori, association_rules#vip들이 구매한 상품을 그룹화(장바구니화)vip['itemDescription'] = vip['itemDescription'].apply(lambda x : x+',')vip_item = vip.groupby('Member_number')['itemDescription'].sum().to_list()items = [0 for x in range(len(vip_item))]for i in range(len(vip_item)): items[i] = vip_item[i].split(',')# TransactionEncoder(아이템 더미화)te = TransactionEncoder()te_result = te.fit(items).transform(items)df = pd.DataFrame(te_result, columns=te.columns_ ,dtype='int')# aprioriapr_result = apriori(df, min_support=0.09, use_colnames=True)lift_based = association_rules(apr_result, metric='lift', min_threshold=3)# # #lift와 confidence를 소수점 1,2자리까지만 출력 -&gt; 정렬lift_based['lift'] = lift_based['lift'].apply(lambda x : f'{x:.1f}')lift_based['confidence'] = lift_based['confidence'].apply(lambda x : f'{x:.2f}')#frozenset -&gt; object로 변경 (잘못된 공백값을 제거하기 위해)lift_based['antecedents'] = lift_based['antecedents'].apply(lambda x: ', '.join(list(x))).str.replace(' ','')lift_based['consequents'] = lift_based['consequents'].apply(lambda x: ', '.join(list(x))).str.replace(' ','')# , 제거lift_based['antecedents'] = lift_based['antecedents'].apply(lambda x : x if x[0] != ',' else x[1:])lift_based['consequents'] = lift_based['consequents'].apply(lambda x : x if x[0] != ',' else x[1:])#lift와 confidence로 정렬lift_sort = lift_based.sort_values(by = ['lift','confidence'], ascending=False)# yogurt,tropicalfruit,rolls/buns &amp; wholemilk,othervegetables,cannedbeer# 이것들은 여기 상점 VIP의 연관성이 가장 높은 물품들이다. 이 들의 동선을 살짝 멀찍이 띄어두고 # 그 사이에 이벤트 상품들을 판매하면 다른 상품들의 매출도 늘 것 같다.lift_sort /var/folders/pr/27tft1vj6396wqnj02ngz5p80000gn/T/ipykernel_28378/1298349058.py:6: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy vip['itemDescription'] = vip['itemDescription'].apply(lambda x : x+',') /opt/homebrew/lib/python3.10/site-packages/mlxtend/frequent_patterns/fpcommon.py:111: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type warnings.warn( .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } antecedents consequents antecedent support consequent support support confidence lift leverage conviction 8 yogurt,tropicalfruit,rolls/buns othervegetables,cannedbeer,wholemilk 0.11 0.21 0.09 0.82 3.9 0.0669 4.345000 14 yogurt,tropicalfruit,rolls/buns othervegetables,cannedbeer,wholemilk 0.11 0.21 0.09 0.82 3.9 0.0669 4.345000 20 yogurt,tropicalfruit,rolls/buns othervegetables,cannedbeer,wholemilk 0.11 0.21 0.09 0.82 3.9 0.0669 4.345000 9 othervegetables,cannedbeer,wholemilk yogurt,tropicalfruit,rolls/buns 0.21 0.11 0.09 0.43 3.9 0.0669 1.557500 15 othervegetables,cannedbeer,wholemilk yogurt,tropicalfruit,rolls/buns 0.21 0.11 0.09 0.43 3.9 0.0669 1.557500 21 othervegetables,cannedbeer,wholemilk yogurt,tropicalfruit,rolls/buns 0.21 0.11 0.09 0.43 3.9 0.0669 1.557500 6 yogurt,rolls/buns,tropicalfruit,wholemilk othervegetables,cannedbeer 0.10 0.25 0.09 0.90 3.6 0.0650 7.500000 12 tropicalfruit,wholemilk,yogurt,rolls/buns othervegetables,cannedbeer 0.10 0.25 0.09 0.90 3.6 0.0650 7.500000 17 yogurt,rolls/buns,tropicalfruit,wholemilk othervegetables,cannedbeer 0.10 0.25 0.09 0.90 3.6 0.0650 7.500000 11 othervegetables,cannedbeer yogurt,rolls/buns,tropicalfruit,wholemilk 0.25 0.10 0.09 0.36 3.6 0.0650 1.406250 18 othervegetables,cannedbeer yogurt,rolls/buns,tropicalfruit,wholemilk 0.25 0.10 0.09 0.36 3.6 0.0650 1.406250 23 othervegetables,cannedbeer tropicalfruit,wholemilk,yogurt,rolls/buns 0.25 0.10 0.09 0.36 3.6 0.0650 1.406250 0 yogurt,tropicalfruit,rolls/buns othervegetables,cannedbeer 0.11 0.25 0.09 0.82 3.3 0.0625 4.125000 2 yogurt,tropicalfruit,rolls/buns othervegetables,cannedbeer 0.11 0.25 0.09 0.82 3.3 0.0625 4.125000 4 yogurt,tropicalfruit,rolls/buns othervegetables,cannedbeer 0.11 0.25 0.09 0.82 3.3 0.0625 4.125000 1 othervegetables,cannedbeer yogurt,tropicalfruit,rolls/buns 0.25 0.11 0.09 0.36 3.3 0.0625 1.390625 3 othervegetables,cannedbeer yogurt,tropicalfruit,rolls/buns 0.25 0.11 0.09 0.36 3.3 0.0625 1.390625 5 othervegetables,cannedbeer yogurt,tropicalfruit,rolls/buns 0.25 0.11 0.09 0.36 3.3 0.0625 1.390625 10 yogurt,cannedbeer,othervegetables rolls/buns,tropicalfruit,wholemilk 0.13 0.22 0.09 0.69 3.1 0.0614 2.535000 16 yogurt,cannedbeer,othervegetables rolls/buns,tropicalfruit,wholemilk 0.13 0.22 0.09 0.69 3.1 0.0614 2.535000 22 yogurt,cannedbeer,othervegetables rolls/buns,tropicalfruit,wholemilk 0.13 0.22 0.09 0.69 3.1 0.0614 2.535000 7 rolls/buns,tropicalfruit,wholemilk yogurt,cannedbeer,othervegetables 0.22 0.13 0.09 0.41 3.1 0.0614 1.472308 13 rolls/buns,tropicalfruit,wholemilk yogurt,cannedbeer,othervegetables 0.22 0.13 0.09 0.41 3.1 0.0614 1.472308 19 rolls/buns,tropicalfruit,wholemilk yogurt,cannedbeer,othervegetables 0.22 0.13 0.09 0.41 3.1 0.0614 1.472308 4번 문제1234567891011121314151617181920212223242526272829303132333435363738394041# 4. 와인의 화학 조성을 사용하여 와인의 종류를 예측하기 위한 데이터이다. load_wine() 명령으로 로드하며 다음과 같이 구성되어 있다. # 와인의 종류를 예측할 수 있는 모델을 생성하시오.from sklearn.datasets import load_winefrom sklearn.preprocessing import StandardScalerfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Densefrom tensorflow.keras import optimizersfrom sklearn.model_selection import train_test_splitwine = load_wine()w_df = wine.dataw_target = wine.target#표준화scaler = StandardScaler()w_df = scaler.fit_transform(w_df)xtrain , xtest, ytrain, ytest = train_test_split(w_df,w_target,test_size= 0.3)#테스트가 값이 3종류라 카테고리화하기ytrain = to_categorical(ytrain)ytest = to_categorical(ytest)model=Sequential()model.add(Dense(3, input_dim=13, activation='softmax'))model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])hist = model.fit(xtrain, ytrain, epochs=300, batch_size=1, validation_data=(xtest, ytest))epochs = range(1, len(hist.history['accuracy']) + 1)plt.plot(epochs, hist.history['loss'])plt.plot(epochs, hist.history['val_loss'])plt.title('model loss')plt.ylabel('loss')plt.xlabel('epoch')plt.legend(['train', 'val'], loc='upper left')plt.show()# accuracy: 1.0000","link":"/2022/11/27/Study_folder/Basic_study/2022-11-27-mini-project-first/"},{"title":"파이썬에서 ZipFile 열기","text":"from zipfile import ZipFile로 압축 해제하지 않고 사용하기1234567891011121314151617181920from zipfile import ZipFileimport pandas as pd# zip파일 열기(압축 해제하지않고 사용)z = ZipFile('sentiment labelled sentences.zip')# 어떤 리스트가 있는지 확인z.namelist()# 출력 결과# ['sentiment labelled sentences/amazon_cells_labelled.txt']# zip파일의 데이터를 불러 읽어옴.# 편집 작업 시 아래와 같이 `pd.read_csv` 같은 함수로 변경(옮겨)하여 사용data = z.open('sentiment labelled sentences/amazon_cells_labelled.txt')df = pd.read_csv(data, sep=&quot;\\t&quot;, header=None)# 자세한 정보 확인zip_info = my_zip.getinfo('sentiment labelled sentences/amazon_cells_labelled.txt') print(zip_info.filename) #파일 이름print(zip_info.file_size)#파일 사이즈print(zip_info.date_time)#작성일 zip파일을 경로에 실제로 압축 해제하기1234!unzip test.zip# -d옵션으로 경로 지정 가능(폴더 자동 생성)%unzip test.zip -d test_folder/ 1234567891011121314from zipfile import ZipFile#특정 파일만 압축 해제할 경우 사용(리스트 for문이라 시간이 꽤 걸림)directory = './' #현재 경로with ZipFile('TeamA_name2.zip') as f : x = [f.extract(file, directory) for file in f.namelist() if file.endswith('jpg')] #특정 확장자만 압축 해제하기(endwith('jpg'))#x는 아무 변수나 줘도 됩니다.# 전체 압축 해제를 원하는 경우(엄청 빠름)import zipfilezip_file=zipfile.ZipFile('TeamA_name2.zip')#파일 이름zip_file.extractall(path='/content/temtem')#압축 해제 경로, default(path=None) 외부 경로를 통해 zip파일을 다운 받고 압축 해제하기(케라스 데이터셋에 저장)123456789101112131415161718192021222324252627282930313233## Download the movielens data from website urlimport tensorflow.keras as kerasfrom zipfile import ZipFilefrom pathlib import Pathimport os#zip파일이 있는 urlzipped_url = ( 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip')#zip_filezipped_file = keras.utils.get_file( 'zipped_dataset.zip', zipped_url, extract=True)keras_path = Path(zipped_file).parents[0] #전의 경로 설정print('datasets path:', keras_path) #데이터들이 케라스 폴더에 저장됩니다.# datasets path: /root/.keras/datasetsprint(os.listdir(keras_path))#['ml-latest-small', 'zipped_dataset.zip']movielens_dir = keras_path / os.listdir(keras_path)[0] #keras_path의 파일 목록 중 첫번째 목록을 임의로 지정#런타임 1회만 사용 가능if not movielens_dir.exists(): with ZipFile(zipped_file, &quot;r&quot;) as zip: zip.extractall(path=keras_path) #압축 해제print(os.listdir(movielens_dir))#['tags.csv', 'movies.csv', 'README.txt', 'links.csv', 'ratings.csv']","link":"/2022/12/03/Study_folder/Basic_study/2022-12-03-ZipFile/"},{"title":"반복 가능한 객체 iter","text":"iterator for 반복문을 사용할 때 range를 사용했습니다. 이 for 반복문을 설명할 때 for i in range(100):은 0부터 99까지 연속된 숫자를 만들어 내는것처럼 보이지만, 사실은 숫자를 모두 만들어 내는 것이 아니라 0부터 99까지 값을 차례대로 꺼낼 수 있는 이터레이터를 하나만 만들어냅니다. 이후 반복할 때마다 이터레이터에서 숫자를 하나씩 꺼내서 반복합니다. 12345678910list_iter= [1, 2, 3]list_iter = [1, 2, 3].__iter__()list_iter.__next__()#출력값 : 1list_iter.__next__()#출력값 : 2list_iter.__next__()#출력값 : 3list_iter.__next__()#error Traceback (most recent call last) &lt; 다음 값이 존재하지 않기 때문에 호출 에러 아래와 같은 파이토치 용법에서도 똑같은 의미로 사용된 것입니다. 기본적으로 iter()는 iterator를 반환하는 iris_loader에서 iter() 메서드를 호출합니다. next() 그런 다음 해당 반복자에서 next() 메서드를 호출하여 첫 번째 반복을 가져옵니다. next()를 다시 실행하면 반복자의 두 번째 항목 등을 얻을 수 있습니다. 1images, labels = next(iter(train_loader))","link":"/2022/12/12/Study_folder/Basic_study/2022-12-12-iter/"},{"title":"파이썬에서 OS모듈로 경로&#x2F;폴더 생성","text":"현재 파일이 실행되고있는 경로를 파악하는 함수1234567891011import osos.getcwd()# 또는 쉘명령어로 파악 가능#윈도우에서 확인할 경우!cwd %cwd #맥에서는!pwd%pwd 폴더를 생성하는 함수123456789101112import osos.mkdir('folder_name')#만약 다중 폴더(폴더의 폴더)를 생성할 경우os.makedirs('./folder_name/folder_name/')#또는 쉘명령어로 생성 가능!mkdir folder_name%mkdir folder_name!mkdir folder_name/folder_name%mkdir folder_name/folder_name 응용하여 폴더가 없으면 폴더 생성하는 함수 생성12345import osPATH = './folder/folder/' if not os.path.exists(PATH): os.makedirs(PATH) 파일 실행 경로를 수정하기12345678import osPATH = './folder/folder/' os.chdir(PATH)#또는 쉘 명령어로!cd folder/folder/%cd folder/folder/ 경로에 파일 리스트 확인1234567891011import os#현재 경로os.listdir()#해당 경로PATH = './folder/folder/' os.listdir(PATH)#또는 쉘 명령어로!ls%ls folder/folder/ 파일/폴더 삭제123456789101112import os#파일 삭제os.remove('./folder/test_file.txt')#폴더 삭제(빈 폴더만 삭제 가능)os.rmdir('./folder/test_folder')#폴더(빈 폴더가 아닌 폴더) 삭제를 원할 시import shutilshutil.rmtree('./folder/test_folder')#복구가 안되니 신중하게 실행하세요 파일 존재 유무 확인123import osos.path.exists('./folder/test_file.txt')# True 경로 조작 및 파일 이름 변경1234567#경로 생성import osprint('출력 결과 :' + ' join(): ' + os.path.join('test_folder', 'test_file.txt'))# 출력 결과 : 'join(): test_folder/test_file.txt'os.rename(a,b) #a-&gt;b#이를 활용하여 폴더 내의 파일들의 이름을 변경할 수 있습니다. 파일 및 폴더 이동1234import shutil#디렉토리가 없으면 오류가 나며, 파일 이름 변경하여 사용하여도 기존의 파일은 없어지고 새로운 파일이 생성된 채 옮겨집니다(리눅스의 mv랑 유사)shutil.move('./test_folder/test.txt','./test_folder/test_folder2/test2.txt')","link":"/2022/12/12/Study_folder/Basic_study/2022-12-12-os_module/"},{"title":"Magic Command","text":"매직 명령어(magic command) Magic command는 IPython kernel에서 제공되는 명령어이다. 주피터 노트북, 랩, 그리고 코랩 환경에서 사용 가능합니다. Magic command는 명령프롬프트(터미널)에서 사용하는 명령어와 거의 유사합니다 %lgmagic 입력 시 사용 가능한 명령어를 확인 할 수 있습니다. Magic command는 %, %% 키워드를 사용하여 실행할 수 있습니다. %는 Line magic으로 부르며 한 줄 안에서 명령어 실행이 시작되고 종료된다. 1234%ls#위의 ls명령어와 별개로 아래의 셀 실행이 됨.print('hi')print(3+5) %%는 Cell magic으로 부르며 노트북의 Cell안에서 명령어 실행이 시작되고 종료된다. (셀 전체가 프롬프트 명령어를 계속 실행한다는 의미) 아래는 셀 전체가 명령프롬프트화 되어 확장자 .py 파일을 생성하는 예시 123456789%%writefile sample.pydef mul(a,b): return a*bdef main(): print(mul(3*6))if __name__ == '__main__': main() pwd, mkdir, cd, history, mv, write, load 등 사용 가능하며 기능은 리눅스나 윈도우와 동일합니다. 경로 설정 키워드1234. 현재 디렉토리.. 부모 디렉토리(전 폴더)/ 최상위 root~ 홈(HOME, 메인) 12# 이런식으로 사용 가능하다.(전의 전 폴더의 test_foler의 파일 리스트 확인)!ls ../../test_folder","link":"/2022/12/14/Study_folder/Basic_study/2022-12-14-magic-command/"},{"title":"glob.glob()","text":"폴더의 모든 목록 globs는 리눅스 운영체제의 명령어 인자로, 파이썬에서도 import glob하여 사용이 가능합니다. 파일을 검색하는 명령어로 잘 사용하면 파일 관리에 매우 유용합니다. 1234567891011121314151617181920212223242526#파일 목록folder : file1.txt, file2.txt, file101.txt, file102.txt, file_a.txt, file_b.txt, file1.jpg, file2.jpgfolder/sub_folder : subfile1.txt, subfile2.txtfolder/sub_folder/sub/sub2 : sub2.txtimport glob# '*'는 임의 길이의 모든 문자열을 의미한다.output = glob.glob('folder/*.txt')print(output)['folder/file1.txt', 'folder/file102.txt', 'folder/file2.txt', 'folder/file_a.txt', 'folder/file_b.txt']# '?'는 한자리의 문자를 의미한다. file_a.txt 같은 파일들은 검색하지 못함output = glob.glob('folder/file?.*')print(output)['folder/file1.bmp', 'folder/file1.txt', 'folder/file2.bmp', 'folder/file2.txt', 'folder/file_a.txt', 'folder/file_b.txt']# '**'은 항목을 전체 찾아줍니다.(디렉토리 이름까지 가능)output = glob.glob('folder/**xt')print(output)['folder/file1.txt', 'folder/file102.txt', 'folder/file2.txt', 'folder/file_a.txt', 'folder/file_b.txt', 'folder/file12.txt]# '[]'를 사용하여 정규표현식으로 사용도 가능합니다.output = glob.glob('folder/file[0-9]*.txt')print(output)['folder/file1.txt', 'folder/file102.txt', 'folder/file2.txt']","link":"/2022/12/15/Study_folder/Basic_study/2022-12-15-glob/"},{"title":"파이썬 리스트 합치기(더하기)","text":"리스트 합치기 (a+b)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 단순 더하기a = [1,2,3]b = [4,3,2]print(a + b)# [1, 2, 3, 4, 3, 2]#extend 사용하기 - 리스트 a 에 b를 연결 합니다. a 리스트가 변경됨.(단순 더하기)a = [4, 3, 2]b = [1, 2, 3]print(a.extend(b))# [1, 2, 3, 4, 3, 2]# '+' 연산이랑 값이 같음#append 사용하기 - 리스트 뒤에 값을 추가하기a = [1,2,3]b = [4,3,2]print(a.append(b))[1, 2, 3, [4, 3, 2]]# 2차원 리스트 -&gt; (2-1)차원 리스트 (차원을 1단계만 낮춰줍니다)a = [['a','b'],['c','d'],[1,2]]a = sum(a, [])print(a)# ['a', 'b', 'c', 'd', 1, 2]# 만약 3차원리스트 -&gt; 1차원리스트를 원할 경우a = [[['a','b'],['c','d'],[1,2]]]a = sum(a, [])a = sum(a, [])print(a)# ['a', 'b', 'c', 'd', 1, 2]#만약 부분적으로 2차원리스트의 값을 가지고 있다면import itertoolslist1 = ['1', '2', '3', ['4', '3', '2']]list2 = list(itertools.chain.from_iterable(list1))print(list2)#['1', '2', '3', '4', '3', '2']list1 = ['1', '2', '3', ['4', '3', '2']]list2 = list(itertools.chain(*list1))print(list2)#['1', '2', '3', '4', '3', '2']","link":"/2022/12/17/Study_folder/Basic_study/2022-12-17-list-sum/"},{"title":"Pillow(from PIL import Image)","text":"PIL 이미지 기본적 사용법123456789101112#열기 방법1img = Image.open('test_file.jpg') img.show()#열기 방법2from PIL import Imagewith Image.open(&quot;test_file.jpg&quot;) as im: im.show()#정보 확인(가로, 세로, 사이즈)img.width, img.height, img.size#사이즈 변경image.resize((w, h)) Thumbnail 파일 생성하기123456789101112131415from PIL import Imageimport glob, ossize = 128, 128#현재 폴더에서 jpg파일들을 썸네일로 만듬for infile in glob.glob(&quot;*.jpg&quot;): file, ext = os.path.splitext(infile) im = Image.open(infile) im.thumbnail(size) im.save(file + &quot;.thumbnail&quot;, &quot;JPEG&quot;)#이미지 열기 img = Image.open('test_file.thumbnail') img.show() 이미지 겹치기(사이즈가 같아야함)12345# 이미지 합성1(겹치기)Image.alpha_composite(img_1, img_2)# 이미지 합성2(희미하게 만들어서 겹치기 - 새로운 이미지 생성)Image.blend(img_1, img_2, .1) 이미지 증식1234567891011121314151617181920212223242526272829# 이미지 자르기 : 특정 영역을 오려낸 사본을 만든다. box는 (left, upper, right, lower)로 구성된 튜플이다.Image.crop(box=None)# ex) Image.crop((x, y, x+64, y+64)) #튜플형식으로 입력해야됨!# 이미지 붙여넣기# 다른 이미지를 이미지 내 특정 영역이나 위치에 붙여넣는다. mask를 써서 붙이는 이미지에 마스크를 적용할 수 있다. 마스크는 이미지 외에 투명도값을 줄 수 있다.image.paste(im, box=None, mask=None) # 이미지 회전하기# 주어진 각도만큼 이미지를 회전한다. 각도는 도(degree)로 주어진다.image.rorate(angle, resample=0, expand=0, center=None, translate=None) #이미지 회전image.transpose(Image.FL) #좌우 반전image.transpose(Image.FLIP_TOP_BOTTOM) #상하 반전image.transpose(Image.ROTATE_90) #90도 회전# 이미지 변형하기image.transform(size, method, data=None, resample=0, fill=1, fillcolor=None)# 이미지 필터(필터는 ImageFilter 모듈에 정의되어 있다.)from PIL import Image, ImageDraw, ImageFont, ImageFilterimg = Image.open('cat_img.jpg').resize((300,300))d = ImageDraw.Draw(img)d.text((40, 10), 'this is BLUR', fill=&quot;Red&quot;) #위치 좌표는 튜플 타입으로 입력img = img.filter(ImageFilter.BLUR)img.save('BLUR.jpg','jpeg')img.show() 이미지 필터 종류 ImageFilter 사진 BLUR CONTOUR DETAIL EDGE_ENHANCE EMBOSS FIND_EDGES SHARPEN SMOOTH 이미지 편집 시 필요한 함수들12345678910111213# 밴드 얻기from PIL import Imageim = Image.open(&quot;hopper.jpg&quot;)print(im.getbands()) # Returns ('R', 'G', 'B')# 바운딩 박스 얻기# 이미지에서 0이아닌 영역의 범위를 구함im = Image.open(&quot;hopper.jpg&quot;)print(im.getbbox())# Returns four coordinates in the format (left, upper, right, lower)# 채널 얻기Image.getchannel(channel) 이미지에 텍스트 삽입1234567from PIL import Image, ImageDraw, ImageFontim = Image.open('cat_img.jpg').resize((150,150))ft = ImageFont.truetype('Pantherdam Signature Italic.ttf', size=20) #폰트 사용시 파일 지정d = ImageDraw.Draw(im)d.text((10, 10), 'text any text', font=ft, fill=&quot;#ff3&quot;) #위치 좌표는 튜플 타입으로 입력im.show()","link":"/2022/12/18/Study_folder/Basic_study/2022-12-18-Pillow/"},{"title":"이스케이프 시퀀스(escape sequence) 프린트문에 색상 입히기","text":"파이썬 프린트문(print)에 색상 입히기 BRIGHT_BLACK = ‘\\033[90m’ BRIGHT_RED = ‘\\033[91m’ BRIGHT_GREEN = ‘\\033[92m’ BRIGHT_YELLOW = ‘\\033[93m’ BRIGHT_BLUE = ‘\\033[94m’ BRIGHT_MAGENTA = ‘\\033[95m’ BRIGHT_CYAN = ‘\\033[96m’ BRIGHT_WHITE = ‘\\033[97m’ BRIGHT_END = ‘\\033[0m’ Black = ‘\\033[30m’ Red = ‘\\033[31m’ Green = ‘\\033[32m’ Yellow = ‘\\033[33m’ Blue = ‘\\033[34m’ Magenta = ‘\\033[35m’ Cyan = ‘\\033[36m’ White = ‘\\033[37m’ 123456# 이런식으로 사용하면 됩니다.print('\\033[93m' + '안녕하세요') # 노란색의 '안녕하세요' 글자가 출력print('\\033[92m' + '안녕하세요') # 녹색의 '안녕하세요' 글자가 출력print('\\033[31m' + '안녕' + '\\033[95m' + '하세요') # 빨간색의 '안녕'과 '보라색'의 하세요 글자가 출력","link":"/2022/12/18/Study_folder/Basic_study/2022-12-19-escape-sequence/"},{"title":"포매팅(fomatting)","text":"소수점 자리수 나타내는 포매팅12345678910# fomat함수는 아래와 같이 3가지 방법으로 표현 할 수 있으며 결과는 동일합니다.# 실수는 다음과 같이 .(점) 앞에 정렬할 길이를 지정하고, 점 뒤에 소수점 이하 자릿수를 지정합니다.num = 12.1print(f'소수 첫번째 자리까지 표기: {num:.1f} , 두번째 자리까지: {num:.2f}')# 소수 첫번째 자리까지 표기: 12.1 , 두번째 자리까지: 12.10print(('소수 첫번째 자리까지 표기: {0:.1f} , 두번째 자리까지: {1:.2f}').format(num,num))# 소수 첫번째 자리까지 표기: 12.1 , 두번째 자리까지: 12.10print('소수 첫번째 자리까지 표기: %.1f , 두번째 자리까지: %.2f' %(num,num))# 소수 첫번째 자리까지 표기: 12.1 , 두번째 자리까지: 12.10 자리수를 고정한 채로 출력하는 포매팅1234567891011121314151617181920# 자리수를 고정한 채로 출력을 원하는 경우'%10d' % 222f'{222:10d}''{0:10d}'.format(int(222))' 222'# 자리수를 고정 + 왼쪽 정렬(문자열만 가능)num = '222'f'{num:10s}'# '222 ''%-10s' % '222'# '222 '# 왼쪽 정렬에 '&lt;' 기호, 오른쪽 정렬에 '&gt;'기호를 사용하여 출력도 가능합니다.num = '222'f'{num:&lt;10s}'num = '222'# '222 'f'{num:&gt;10s}'# ' 222' 자리수를 고정한 채 공백(보통 숫자 ‘0’으로) 채우기12345678910111213'{0:08.2f}'.format(22.33)# '00022.33'# 중요 !!! 왼쪽에 문자 채우고 0으로 나머지 칸 채우기(총8칸)num = '222'f'{num:0&lt;8s}''22200000'# 오른쪽에 문자 채우고 0으로 나머지 칸 채우기(총8칸)num = '222'f'{num:0&gt;8s}''00000222' 3자리 단위마다 ‘,’있는 숫자로 표현하기123'{0:,}'.format(1234567890)# '1,234,567,890'","link":"/2022/12/22/Study_folder/Basic_study/2022-12-22-fomatting/"},{"title":"파이썬 에러 생성 및 처리 방법(assert, raise, try)","text":"Assert 사용 방법 예외 처리(try, raise)함수와 달리 assert는 가정 설명문 입니다. 즉, 어떤 조건이 True면 pass되고, 아니면 Error(오류)를 생성합니다. 방법은 간단합니다. assert (가정법), '에러 메시지 작성'이렇게 작성하기면 됩니다. 12345678emd = 256heads =8head_dim = emd // headsassert (head_dim * heads == emd), 'embed size needs to be div by heads'print('hello')# 실행하면 (head_dim * heads == emd)이 True 때문에 아무 출력 결과가 없다.hello 1234567891011# 앞의 값이 False라면assert (head_dim * heads == emd + 1), 'embed size needs to be div by heads'print('hello') 3 head_dim = emd // heads 4 ----&gt; 5 assert (head_dim * heads == emd + 1), 'embed size needs to be div by heads' 6 AssertionError: embed size needs to be div by heads# 위와 같은 AssertionError가 생성된다. Raise 사용 방법 일부러 에러를 발생시켜야 되는 경우가 있는데, 이때 사용하는게 assert와 raise raise는 assert와 달리 실행만 되면 에러가 생성됩니다. 1234567891011# Exception으로 문구를 작성해도 되고 안해도 됩니다(옵션)raise Exception('hi')print('hello')# 결과-에러----&gt; 1 raise Exception('아무 에러 문구') 2 3 print('hello')Exception: 아무 에러 문구 12345678910# 보통 이런식으로 조건문과 같이 사용합니다b = 1a = [1,2,3]if b not in a: raise Exception('a에 없는 값이라서 에러 발생 시킬게요.')print('hello')# 출력 결과hello 123456789101112131415b = 5a = [1,2,3]if b not in a: raise Exception('a에 없는 값이라서 에러 발생 시킬게요.')print('hello')# 출력 결과 4 if b not in a:----&gt; 5 raise Exception('a에 없는 값이라서 에러 발생 시킬게요.') 6 7 print('hello')Exception: a에 없는 값이라서 에러 발생 시킬게요. Try, Except 사용 방법 try는 에러가 발생하더라도 그냥 작업을 진행하게 만들고 싶을 때 많이 사용됩니다. except뒤 에 발생되는 [에러 이름] 그 뒤에 처리를 어떻게 할지 사용 에러가 나와도 그냥 pass를 원하면 pass 입력 예시를 보면 바로 이해되실 겁니다. 123456789101112131415161718192021222324# 숫자 2개를 나눌때 나오는 에러 테스트입니다.try: a, b = map(int,input('숫자 2개를 띄어쓰기를 사용해서 입력해주세요 : ').split()) print(a / b)except ZeroDivisionError: # 숫자를 0으로 나눠서 에러가 발생했을 때 실행됨 print('숫자를 0으로 나눌 수 없습니다.')except ValueError as a: # input값을 제대로 입력하지 않으면 실행 print('잘못된 입력입니다', a) passprint('hi')# 결과숫자 2개를 띄어쓰기를 사용해서 입력해주세요 : 3 13.0hi# 결과(zerodivision)숫자 2개를 띄어쓰기를 사용해서 입력해주세요 : 3 0숫자를 0으로 나눌 수 없습니다.# 결과(valueeroor)숫자 2개를 띄어쓰기를 사용해서 입력해주세요 : 3abd잘못된 입력입니다 invalid literal for int() with base 10: '3abd'hi 만약 어떤 에러인지 모르는 경우 테스트를하여 에러를 확인한다. 123456789101112131415a, b = map(int,input('숫자 2개를 띄어쓰기를 사용해서 입력해주세요 : ').split())print(a / b)# '3 0'을 입력할 경우 'ZeroDivisionError'&lt;ipython-input-65-a03c397788a2&gt; in &lt;module&gt; 1 a, b = map(int,input('숫자 2개를 띄어쓰기를 사용해서 입력해주세요 : ').split())----&gt; 2 print(a / b)ZeroDivisionError: division by zero# '3abd'을 입력할 경우 'ValueError'----&gt; 1 a, b = map(int,input('숫자 2개를 띄어쓰기를 사용해서 입력해주세요 : ').split()) 2 print(a / b)ValueError: invalid literal for int() with base 10: '3abd'","link":"/2023/01/05/Study_folder/Basic_study/2023-01-05-assert/"},{"title":"Einsum (Einstein Summation)","text":"einsum 참고 : &lt;Aladdin youtube&gt; Einsum은 Einstein Summation Convention으로 연산을 하는 방법입니다. 연산을 통해 내적(Dot products), 외적(Outer porducts), 전치(Transpose), 행렬곱(Matmul) 등을 표현할 수 있으며, 형태(dim, shape)을 관리할 때 매우 유용하다. einsum은 numpy, torch, tensorflow에서 사용가능하다.ex) numpy.einsum(), torch.einsum(), tensorflow.einsum() 간단하게 아래처럼 사용할 수 있습니다.(차원 표현으로 ijk... 으로 많이 사용됩니다.) a,b 중 같은 차원이라면 동일한 알파벳으로 입력해주기. einsum의 통상적인 사용방법은 다음과 같습니다. torch인 a.shape==(2,3,4),b.shape(3,4,1)가 있다면,torch.einsum(‘ijk , jka -&gt; jki’ , [a,b])결과는 [3,4,2] 라는 식으로 나옵니다. 수학적으로 표현하자면 너무 복잡해지니 예시를 통해 간단한 사용 방법을 익혀봅시다. 예시123456789101112# MATRIX TRANSPOSEimport torcha = torch.arange(6).reshape(2, 3)print(a)torch.einsum('ij-&gt;ji', [a])tensor([[0, 1, 2], [3, 4, 5]])tensor([[0, 3], [1, 4], [2, 5]]) 12345# SUMa = torch.arange(6).reshape(2, 3)torch.einsum('ij-&gt;', [a])tensor(15) # 6! 123456789# COLUMN SUMa = torch.arange(6).reshape(2, 3)print(a)torch.einsum('ij-&gt;j', [a])tensor([[0, 1, 2], [3, 4, 5]])# 0+3 , 1+4, 2+5tensor([3, 5, 7]) 12345678910# ROW SUMa = torch.arange(6).reshape(2, 3)print(a)torch.einsum('ij-&gt;i', [a])tensor([[0, 1, 2], #0+1+2-&gt;3 [3, 4, 5]]) #3+4+5-&gt;12tensor([ 3, 12]) 123456789101112# MATRIX-VECTOR MULTIPLICATIONa = torch.arange(6).reshape(2, 3)b = torch.arange(3)torch.einsum('ik,k-&gt;i', [a, b])tensor([ 5, 14])# 행렬곱과 값이 동일np.matmul(a,b) tensor([ 5, 14]) 12345678# MATRIX-MATRIX MULTIPLICATIONa = torch.arange(6).reshape(2, 3)b = torch.arange(15).reshape(3, 5)torch.einsum('ik,kj-&gt;ij', [a, b])tensor([[ 25, 28, 31, 34, 37], [ 70, 82, 94, 106, 118]]) 1234567# DOT PRODUCT(vector)a = torch.arange(3)b = torch.arange(3,6) # [3, 4, 5]torch.einsum('i,i-&gt;', [a, b])tensor(14) 1234567# DOT PRODUCT(matrix)a = torch.arange(6).reshape(2, 3)b = torch.arange(6,12).reshape(2, 3)torch.einsum('ij,ij-&gt;', [a, b])tensor(145) 12345# HADAMARD PRODUCTa = torch.arange(6).reshape(2, 3)b = torch.arange(6,12).reshape(2, 3)torch.einsum('ij,ij-&gt;ij', [a, b]) 1234567891011121314# OUTER PRODUCTa = torch.arange(3)b = torch.arange(3,7) #[3, 4, 5, 6]c = torch.einsum('i,j-&gt;ij', [a, b])print(a.shape,b.shape,c.shape)ctorch.Size([3]) torch.Size([4]) torch.Size([3, 4])tensor([[ 0, 0, 0, 0], [ 3, 4, 5, 6], [ 6, 8, 10, 12]])","link":"/2023/01/05/Study_folder/Basic_study/2023-01-05-einsum/"},{"title":"pd.read_csv()","text":"pd.read_csv()에서 인코딩 옵션123456import pandas as pd# 인코딩은 3종류가 있습니다. 인코딩 오류가 나면 3개 중 하나를 입력해보시면 됩니다.df = pd.read_csv('/court_precedent.csv', encoding='euc-kr')df = pd.read_csv('/court_precedent.csv', encoding='utf-8')df = pd.read_csv('/court_precedent.csv', encoding='cp949') pd.read_csv()에서 많이 사용되는 옵션 예시12345678910# 맨 위의 값을 컬럼으로 안쓰고 행으로 사용할 경우, header = Nonepd.read_csv('/court_precedent.csv', header = None)# 맨 위의 이상한 행이 2개 이상일 경우 skiprows를 사용하면 된다.pd.read_csv('/court_precedent.csv', skiprows = 2)# 파일이 공백 2개나 3개 등으로 구분 되어있을 경우(정규 표현식 사용 가능)pd.read_csv('/court_precedent.csv', sep ='\\s+')# 파일이 탭(공백 4칸)으로 구분되어있을 경우pd.read_csv('/court_precedent.csv', sep ='/t')","link":"/2022/11/22/Study_folder/Pandas,%20Numpy/2022-11-22-file-encoding/"},{"title":"범주화(카테고리화) - pd.cut(), pd.qcut()","text":"cut, qcut은 수치형 데이터를 카테고리화 하는 함수입니다. pd.cut() pd.cut()은 구간(bin, 나누는 개수)을 특정 범위로 설정 가능합니다. 1234567891011121314151617181920df = pd.DataFrame({'Age' : range(1,100,7)})df['Age_category'] = pd.cut(df['Age'],bins=[0,10,70,100],labels=['young','medium','older']) df# 출력 결과 Age Age_category0 1 young1 8 young2 15 medium3 22 medium4 29 medium5 36 medium6 43 medium7 50 medium8 57 medium9 64 medium10 71 older11 78 older12 85 older13 92 older14 99 older pd.qcut() qcut은 cut과 달리 구간 설정을 정확히 나눌 경우 사용되어 임의의 계산이 필요 없다. 아래와 같은 경우 라벨이 young은 df[‘Age’].quantile(0.33)이하의 값이고 medium은 0.33 ~ 0.66이며, older는 df[‘Age’].quantile(0.66)의 값이 bin이다. 12345678910111213141516171819202122df = pd.DataFrame({'Age' : range(1,100,7)})df['Age_category'] = pd.qcut(df['Age'],3,labels=['young','medium','older']) df# 만약,duplicates오류가 나온다면 duplicates='drop'을 추가 옵션에 넣으면 됩니다.# 출력 결과 Age Age_category0 1 young1 8 young2 15 young3 22 young4 29 young5 36 medium6 43 medium7 50 medium8 57 medium9 64 medium10 71 older11 78 older12 85 older13 92 older14 99 older","link":"/2022/11/25/Study_folder/Pandas,%20Numpy/2022-11-25-cut()-qcut()/"},{"title":"데이터프레임 필터링 피봇테이블 - pd.pivot_table()","text":"pivot_table() 아래와 같은 데이터 프레임이 있을 때, ‘Age_category’를 엑셀 필터를 거는것 처럼 만들고 싶다면, 다양한 방법이 있지만 가장 보기 편한 것 중 하나가 피봇테이블이다. 엑셀이나 스프레드시트의 피봇테이블과 유사하며 방법도 그리 어렵지 않다. 아래는 간단한 예시입니다. 12345678910111213141516171819202122232425262728293031323334353637#index는 맨 왼쪽에 gropby할 열이름을 입력,#columns는 보고 싶은 열의 이름을 입력,#fill_value는 연산하고 싶은 열의 이름을 입력(int,float타입만 가능) - 입력하지 않으면 default값으로 수치화할 수 있는 열들이 들어감.#aggfunc 에는 연산하고 싶은 방식 선택 ex) 'count', 'sum', 'mean', np.mean 등이 있다.import pandas as pddf = pd.DataFrame({'Age' : range(1,101,10),'Number' :range(100,1001,100)})df['Age_category'] = pd.qcut(df['Age'],3,labels=['young','medium','older']) df Age Number Age_category0 1 100 young1 11 200 young2 21 300 young3 31 400 young4 41 500 medium5 51 600 medium6 61 700 medium7 71 800 older8 81 900 older9 91 1000 olderimport numpy as npdf.pivot_table(index='Age',columns='Age_category',aggfunc='mean')NumberAge_category young medium olderAge 1 100.0 NaN NaN11 200.0 NaN NaN21 300.0 NaN NaN31 400.0 NaN NaN41 NaN 500.0 NaN51 NaN 600.0 NaN61 NaN 700.0 NaN71 NaN NaN 800.081 NaN NaN 900.091 NaN NaN 1000.0 아래는 판다스 메뉴얼의 예시입니다 공식 메뉴얼을 보면, 확실히 이해 할 수 있을겁니다. 12345678910111213141516171819202122232425262728293031323334df = pd.DataFrame({&quot;A&quot;: [&quot;foo&quot;, &quot;foo&quot;, &quot;foo&quot;, &quot;foo&quot;, &quot;foo&quot;, &quot;bar&quot;, &quot;bar&quot;, &quot;bar&quot;, &quot;bar&quot;], &quot;B&quot;: [&quot;one&quot;, &quot;one&quot;, &quot;one&quot;, &quot;two&quot;, &quot;two&quot;, &quot;one&quot;, &quot;one&quot;, &quot;two&quot;, &quot;two&quot;], &quot;C&quot;: [&quot;small&quot;, &quot;large&quot;, &quot;large&quot;, &quot;small&quot;, &quot;small&quot;, &quot;large&quot;, &quot;small&quot;, &quot;small&quot;, &quot;large&quot;], &quot;D&quot;: [1, 2, 2, 3, 3, 4, 5, 6, 7], &quot;E&quot;: [2, 4, 5, 5, 6, 6, 8, 9, 9]})df A B C D E0 foo one small 1 21 foo one large 2 42 foo one large 2 53 foo two small 3 54 foo two small 3 65 bar one large 4 66 bar one small 5 87 bar two small 6 98 bar two large 7 9#pivot_tablepd.pivot_table(df, values=['D', 'E'], index=['A', 'C'], aggfunc={'D': np.mean, 'E': [min, max, np.mean]}) D E mean max mean minA C bar large 5.500000 9 7.500000 6small 5.500000 9 8.500000 8foo large 2.000000 5 4.500000 4small 2.333333 6 4.333333 2","link":"/2022/11/26/Study_folder/Pandas,%20Numpy/2022-11-26-pd.pivot_table()/"},{"title":"numpy의 차원과 axis의 이해","text":"차원과 axis의 이해(sum으로)12345678910111213141516171819202122232425262728293031323334353637383940np_array = np.arange(0, 4*2*4)print(np_array) #32차원 배열# array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,# 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])reshape = np_array.reshape(4,2,4)print(reshape) #x,y,z차원 == 4,2,4 차원# array([[[ 0, 1, 2, 3],# [ 4, 5, 6, 7]],# [[ 8, 9, 10, 11],# [12, 13, 14, 15]],# [[16, 17, 18, 19],# [20, 21, 22, 23]],# [[24, 25, 26, 27],# [28, 29, 30, 31]]])print(reshape.sum(axis=0)) #x,y,z 차원에서 y,z 차원으로 바뀜# array([[48, 52, 56, 60],# [64, 68, 72, 76]])print(reshape.sum(axis=1)) #x,y,z 차원에서 x,z 차원으로 바뀜# array([[ 4, 6, 8, 10],# [20, 22, 24, 26],# [36, 38, 40, 42],# [52, 54, 56, 58]])print(reshape.sum(axis=2)) #x,y,z 차원에서 x,y 차원으로 바뀜# array([[ 6, 22],# [ 38, 54],# [ 70, 86],# [102, 118]])print(reshape.sum(axis=(1,2))) #x,y,z 차원에서 z 차원으로 바뀜# array([ 28, 92, 156, 220])print(reshape.sum(axis=(0,2))) #x,y,z 차원에서 y차원으로 바뀜# array([216, 280])","link":"/2022/12/14/Study_folder/Pandas,%20Numpy/2022-12-15-numpy-axis/"},{"title":"np.transpose()","text":"numpy.T T 함수(메소드)는 shape의 차원을 뒤바꿔주는 함수이다 2차원의 배열에서는 T와 transpose의 결과가 같지만, 3차원이 넘는 고차원에서는 구분 할 필요가 있다. 특히 컬러이미지는 3차원 이상이기 때문에 transpose 함수를 자주 사용한다. 1234567891011121314a = np.random.randint(1,20,6).reshape(2,3)print(a)# [[18 14 5]# [ 1 3 18]]print(a.T)# [[18 1]# [14 3]# [ 5 18]]print(a.transpose())# [[18 1]# [14 3]# [ 5 18]] numpy.transpose()123456789101112131415161718192021222324252627282930a = np.random.randint(1,20,24).reshape(2,3,4)print(a)[[[ 4 7 12 14] [ 6 19 19 14] [ 4 6 2 12]] [[ 4 13 14 16] [ 7 1 19 19] [10 12 2 6]]]print(a.T)[[[ 4 4] [ 6 7] [ 4 10]] [[ 7 13] [19 1] [ 6 12]] [[12 14] [19 19] [ 2 2]] [[14 16] [14 19] [12 6]]]a.transpose(2,1,0)# 결과는 a.T와 같음 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647a.transpose(1,2,0) # 차원 reshape하는 느낌# 그러나 reshape과 차원빼고는 결과가 전혀 다름을 유의!array([[[ 4, 4], [ 7, 13], [12, 14], [14, 16]], [[ 6, 7], [19, 1], [19, 19], [14, 19]], [[ 4, 10], [ 6, 12], [ 2, 2], [12, 6]]])#transpose랑 전혀 다름!a.reshape(3,4,2)array([[[ 4, 7], [12, 14], [ 6, 19], [19, 14]], [[ 4, 6], [ 2, 12], [ 4, 13], [14, 16]], [[ 7, 1], [19, 19], [10, 12], [ 2, 6]]])a.shape# (2, 3, 4)a.T.shape# (4, 3, 2)a.transpose(2,1,0)# (4, 3, 2)# 2,3,4 -&gt; 4,3,2a.transpose(1,2,0) # (3, 4, 2)# 2,3,4 -&gt; 3,4,2","link":"/2022/12/24/Study_folder/Pandas,%20Numpy/2022-12-24-np-transpose/"},{"title":"자연어 처리 NLP(Natural Language Processing)","text":"NLP란 자연어 처리(Natural Language Processing)텍스트 전처리토큰화 : 코퍼스로부터 토큰 단위로 나누는 작업, nltk, konlpy를 사용토큰 : 처리하는 단위(단어, 문자, 문장, 문단). 일반적으로 단어 단위로 토큰화 작업을 실행영어와 달리 한국어는 조사를 분리해야 함 형태소 : 가장 작은 말의 단위(자립 형태소, 의존 형태소가 있음) 자립 형태소 : 조사, 어미와 상관없이 자립하여 사용 가능(자체가 단어가 됨) 의존 형태소 : 다른 형태소와 결합해서 사용(접사, 어미, 조사, 어간) 문장 : 길동이가 코딩을 합니다 토큰화(띄어쓰기) : [길동이가, 코딩을, 합니다] 자립 형태소 : 길동이, 코딩 의존 형태소 : ‘-가’, ‘을’, ‘-합’, ‘-니’, ‘-다’ 현재는 한국어 자연어처리는 의존 형태소를 버리고 있는데, 이를 살려야 보다 정확한 정보를 획득 가능","link":"/2022/11/22/Study_folder/NLP(Natural_Language_Processing)/2022-11-22-NLP/"},{"title":"BERT 파인튜닝(Fine Tunning) 설명","text":"Bert 구조 Fine Tunning(파인튜닝)을 하기 위해서는 기존의 학습되기 전의 데이터 타입으로 바꿔 줄 필요가 있습니다. 즉 파인튜닝을 위해서는 Bert방식의 데이터 정제가 필요합니다. Bert는 아래의 그림과 같이 3가지의 입력 임베딩(Token, Segment, Position 임베딩)의 합으로 구성되어 학습된 모델입니다. Token Embeddings Word Piece 임베딩 방식 사용 자주 등장하면서 가장 긴 길이의 sub-word를 하나의 단위로 생성 즉, 자주 등장하는 sub-word은 그 자체가 단위가 되고, 자주 등장하지 않는 단어(rare word)는 sub-word로 쪼개짐 기존 워드 임베딩 방법은 Out-of-vocabulary (OOV) 문제가 존재하며, 희귀 단어, 이름, 숫자나 단어장에 없는 단어에 대한 학습, 번역에 어려움이 있음 Word Piece 임베딩은 모든 언어에 적용 가능하며, sub-word 단위로 단어를 분절하므로 OOV 처리에 효과적이고 정확도 상승효과도 있음 Sentence Embeddings BERT는 두 개의 문장을 문장 구분자([SEP],스페셜 토큰)와 함께 결합 한국어는 보통 평균 20 subword로 구성되고 99%가 60 subword를 넘지 않기 때문에 입력 길이를 두 문장이 합쳐 128(max_len)으로 설정 해도 충분합니다 간혹 긴 문장이 있으므로 우선 입력 길이 128로 제한하고 학습한 후, 128보다 긴 입력들을 모아 마지막에 따로 추가 학습하는 방식을 사용 Position Embedding BERT는 Transformer 모델을 착용 그 중 Self-Attention 모델을 사용 Self-Attention은 입력의 위치에 대해 고려하지 못하므로 입력 토큰의 위치 정보가 필요(position embedding필요성) Position encoding은 단순하게 Token 순서대로 0, 1, 2, …와 같이 순서대로 인코딩 Fine Tunning의 2가지 대표적인 방법 BERT는 위에서 소개한 3가지의 입력 임베딩(Token, Segment, Position 임베딩)을 취합하여 하나의 임베딩 값으로 생성 임베딩의 합에 Layer Normalization과 Dropout을 적용하여 입력으로 사용 MLM(Masked Language Model) 입력 문장에서 임의로 Token을 마스킹(masking), 그 Token을 맞추는 방식인 MLM 학습 진행 문장의 빈칸 채우기 문제를 학습 생성 모델 계열은(예를 들어 GPT) 입력의 다음 단어를 예측 MLM은 문장 내 랜덤한 단어를 마스킹 하고 이를 예측 입력의 15% 단어를 [MASK] Token으로 바꿔주어 마스킹 이 때 80%는 [MASK]로 바꿔주지만, 나머지 10%는 다른 랜덤 단어로, 또 남은 10%는 바꾸지 않고 그대로 둠 이는 튜닝 시 올바른 예측을 돕도록 마스킹에 노이즈를 섞음 NSP(Next Sentence Prediction) NSP는 두 문장이 주어졌을 때 두 번째 문장이 첫 번째 문장의 바로 다음에 오는 문장인지 여부를 예측하는 방식 두 문장 간 관련이 고려되어야 하는 NLI와 QA의 파인튜닝을 위해 두 문장이 연관이 있는지를 맞추도록 학습 위에서 설명한 MLM과 동시에 NSP도 적용된 문장들 첫 번째 문장과 두 번째 문장은 [SEP]로 구분(스페셜 토큰) 두 문장이 실제로 연속하는지는 50% 비율로 참인 문장과, 50%의 랜덤하게 추출된 상관 없는 문장으로 구성 이 학습을 통해 문맥과 순서를 학습 가능 아래 그림은 NSP의 입력 예시 참고 자료[ebbn] : https://ebbnflow.tistory.com/151[NLP-kr] : https://github.com/NLP-kr/tensorflow-ml-nlp-tf2[이수한컴퓨터연구소] : https://www.youtube.com/watch?v=LEtLfx1dS7Q[위키독스] : https://wikidocs.net/156998","link":"/2022/12/10/Study_folder/NLP(Natural_Language_Processing)/2022-12-10-BERT_fine_intro/"},{"title":"KoGPT2 파인튜닝하여 챗봇 생성","text":"KoGPT2 파인 튜닝을 사용한 챗봇 만들기 전처리는 감성대화말뭉치(from AI허브)의 Q,A부분만 사용하여 테스트 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163#Thinkbig_KoGPT2_fine_tunningimport numpy as npimport pandas as pdimport torchfrom torch.utils.data import DataLoader, Datasetfrom transformers import PreTrainedTokenizerFast, GPT2LMHeadModelimport refrom tqdm import tqdm# 스페셜 토큰U_TKN = '&lt;usr&gt;' #Qusetion토큰S_TKN = '&lt;sys&gt;' #Answer토큰BOS = '&lt;/s&gt;'#문장의 시작 토큰 #시작과 끝을 구분하기 때문에 bos,eos토큰의 값을 동일 시 해도 상관없다.EOS = '&lt;/s&gt;'#문장의 끝 토큰MASK = '&lt;unused0&gt;'#마스크 토큰SENT = '&lt;unused1&gt;'#문장 토큰(Q와 A토큰 사이에 넣어서 구분)PAD = '&lt;pad&gt;' #패드 토큰# #hugging_face의 KoGPT2(이미 학습된 데이터)를 가져옴koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained('skt/kogpt2-base-v2', bos_token=BOS, eos_token=EOS, unk_token='&lt;unk&gt;', pad_token=PAD, mask_token=MASK)model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')# 파라미터, 크로스엔트로피로스, 옵티마이저(아담)epoch = 2Sneg = -1e18learning_rate = 3e-5criterion = torch.nn.CrossEntropyLoss(reduction='none')optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)# 전처리된 데이터 불러오기df = pd.read_csv('../../ChatbotData.csv')df.head()#데이터셋 클래스 상속class ChatbotDataset(Dataset): # 데이터셋의 전처리를 해주는 부분 def __init__(self, chats, max_len=64): self._data = chats self.max_len = max_len self.q_token = U_TKN self.a_token = S_TKN self.sent_token = SENT self.eos = EOS self.pad = PAD self.mask = MASK self.tokenizer = koGPT2_TOKENIZER def __len__(self): return len(self._data) #Q,A만 사용하여 파인튜닝을 위한 토큰화(인덱스(idx)에 해당하는 입출력 데이터 반환) def __getitem__(self, idx): turn = self._data.iloc[idx] q = turn['Q'] # 질문을 가져온다. q = re.sub(r'([?.!,])', r' ', q) # 특수기호 생략(이거 안하면 결과가 이상하게 나올 때가 많음) a = turn['A'] # 답변을 가져온다. a = re.sub(r'([?.!,])', r' ', a) q_toked = self.tokenizer.tokenize(self.q_token + q + self.sent_token) q_len = len(q_toked) a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos) a_len = len(a_toked) #질문의 길이가 최대길이(64)보다 크면 if q_len &gt; self.max_len: a_len = self.max_len - q_len #답변의 길이를 최대길이 - 질문길이 if a_len &lt;= 0: #질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면 q_toked = q_toked[-(int(self.max_len / 2)) :] #질문길이를 최대길이의 반으로 q_len = len(q_toked) a_len = self.max_len - q_len #답변의 길이를 최대길이 - 질문길이 a_toked = a_toked[:a_len] a_len = len(a_toked) #질문의 길이 + 답변의 길이가 최대길이보다 크면 if q_len + a_len &gt; self.max_len: a_len = self.max_len - q_len #답변의 길이를 최대길이 - 질문길이 if a_len &lt;= 0: #질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면 q_toked = q_toked[-(int(self.max_len / 2)) :] #질문길이를 최대길이의 반으로 q_len = len(q_toked) a_len = self.max_len - q_len #답변의 길이를 최대길이 - 질문길이 a_toked = a_toked[:a_len] a_len = len(a_toked) # 답변 labels = [mask, mask, ...., mask, ..., &lt;bos&gt;,..답변.. &lt;eos&gt;, &lt;pad&gt;....] labels = [self.mask] * q_len + a_toked[1:] # mask = 질문길이 0 + 답변길이 1 + 나머지 0 mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len) # 답변 labels을 index 로 만든다. labels_ids = self.tokenizer.convert_tokens_to_ids(labels) # 최대길이만큼 PADDING while len(labels_ids) &lt; self.max_len: labels_ids += [self.tokenizer.pad_token_id] # 질문 + 답변을 index 로 만든다. token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked) # 최대길이만큼 PADDING while len(token_ids) &lt; self.max_len: token_ids += [self.tokenizer.pad_token_id] #질문+답변, 마스크, 답변 return (token_ids, np.array(mask), labels_ids)# batches가 1이 아닌 경우 이런식으로 세팅하여 DataLoader의 collate_fn에 넣어준다.def collate_batch(batch): data = [item[0] for item in batch] mask = [item[1] for item in batch] label = [item[2] for item in batch] return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)df=df[['Q','A']]# df = df.iloc[:100,:] #테스트 시 데이터를 짧게 만들어서 구동여부 확인device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')print(f'GPU 사용 가능한가요 ? : {torch.cuda.is_available()}') train_set = ChatbotDataset(df, max_len=64) #윈도우 환경에서는 num_workers 는 무조건 0으로 지정train_dataloader = DataLoader(train_set, batch_size=32, num_workers=0, shuffle=True, collate_fn=collate_batch)model.to(device)model.train()# 가중치 같은거 없이 학습print ('학습 시작')for epoch in range(epoch): for batch_idx, samples in enumerate(tqdm(train_dataloader)): optimizer.zero_grad() #Pytorch에서는 gradients값들을 추후에 backward를 해줄때 계속 더해주기 때문 token_ids, mask, label = samples out = model(token_ids) out = out.logits #Returns a new tensor with the logit of the elements of input mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2) mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out)) loss = criterion(mask_out.transpose(2, 1), label) # 평균 loss 만들기 avg_loss[0] / avg_loss[1] &lt;- loss 정규화 avg_loss = loss.sum() / mask.sum() avg_loss.backward() # 학습 끝 optimizer.step()#경사하강법(gradient descent)print ('학습 종료')### 챗봇 실행 'quit' 입력 시 종료with torch.no_grad(): #requires_grad=False 상태가 되어 메모리 사용량 아껴줌 print('챗봇 작동 중입니다. 종료를 원하면 \\&quot;quit\\&quot;을 입력해주세요') print(' ') while True : q = input('나 &gt; ').strip() if q == 'quit': break a = '' while True: input_ids = torch.LongTensor(koGPT2_TOKENIZER.encode(U_TKN + q + SENT + S_TKN + a)).unsqueeze(dim=0) pred = model(input_ids) pred = pred.logits #마지막 dim의 최대값 인덱스 gen = koGPT2_TOKENIZER.convert_ids_to_tokens(torch.argmax(pred, dim=-1).squeeze().numpy().tolist())[-1] if gen == EOS: break a += gen.replace('▁', ' ') print('Chatbot &gt; {}'.format(a.strip()))","link":"/2022/12/10/Study_folder/NLP(Natural_Language_Processing)/2022-12-10-KoGPT2_chatbot/"},{"title":"SBERT(Sentence BERT)를 활용한 챗봇 생성","text":"SBERT SBERT는 기본적으로 BERT의 문장 임베딩의 성능을 우수하게 개선시킨 모델입니다. SBERT는 위에서 언급한 BERT의 문장 임베딩을 응용하여 BERT를 파인 튜닝합니다. 문장 쌍 분류 태스크로 파인 튜닝 SBERT를 학습하는 첫번째 방법은 문장 쌍 분류 태스크. 대표적으로는 NLI(Natural Language Inferencing) 문제를 푸는 것입니다. NLI는 두 개의 문장이 주어지면 수반(entailment) 관계인지, 모순(contradiction) 관계인지, 중립(neutral) 관계인지를 맞추는 문제입니다. 문장 쌍 회귀 태스크로 파인 튜닝 SBERT를 학습하는 두번째 방법은 문장 쌍으로 회귀 문제를 푸는 것으로 대표적으로 STS(Semantic Textual Similarity) 문제를 푸는 경우입니다. STS란 두 개의 문장으로부터 의미적 유사성을 구하는 문제를 말합니다. SBERT의 임베딩을 사용한 챗봇만들기 SBERT에서 임베딩 부분만 사용(STS) df의 임베딩 값과 질문의 임베딩값 사이의 유사도를 코사인 유사도로 구하여 답을 하는 구조로 구성되어 있음. streamlit을 사용하여(웹 애플리케이션) 챗봇 생성 전처리는 감성대화말뭉치(from AI허브)에서 Q,A부분만 가져와서 사용 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#Thinkbig_chatbot SBERT(sentence_transformers)import streamlit as stfrom streamlit_chat import message# streamlit_chat은 python 3.8버전 이상에서만 제대로 동작import pandas as pdfrom sentence_transformers import SentenceTransformerfrom sklearn.metrics.pairwise import cosine_similarityimport jsonimport osfrom typing import Literal, Optional, Unionimport streamlit.components.v1 as components@st.cache(allow_output_mutation=True)# cache : 모델을 여러번 부르지 않고 한번만 불러오는 역할# Streamlit의 캐시 주석으로 함수를 표시하면 함수가 호출될 때마다 다음 세 가지를 확인해야 한다고 Streamlit에 알린다.def cached_model(): model = SentenceTransformer('jhgan/ko-sroberta-multitask') return model# 모델은 미리 학습된 SentenceTransformer을 가져와서 사용# SentenceTransformer는 최신 문장, 텍스트 및 이미지 임베딩을 위한 python 프레임워크# 이 프레임워크를 사용하여 100개 이상의 언어에 대한 문장/텍스트 임베딩을 계산# 그런 다음 이러한 임베딩을 예를 들어 코사인 유사도와 비교하여 유사한 의미를 가진 문장을 찾는다.# wellness_dataset_final.csv : ai허브의 감성대화 데이터와 챗봇 데이터를 병합한 후 전처리한 데이터@st.cache(allow_output_mutation=True)def get_dataset(): df = pd.read_csv('wellness_dataset_final.csv') # 임베딩된 데이터셋 로드 df['embedding'] = df['embedding'].apply(json.loads) # 임베딩 return df# streamlit에 위에서 정의한 함수를 사용하여 모델과 데이터셋 로드model = cached_model()df = get_dataset()# 화면에 표시되는 순서대로 출력st.title('자연어처리 프로젝트') # 제목st.header('심리상담 챗봇') # 헤더# st.subheader(&quot;서브헤더&quot;)# st.text(&quot;텍스트&quot;)st.markdown(&quot;❤️chatbot_think_big&quot;) # 마크다운#st.subheader(&quot;&quot;)st.markdown(&quot;&quot;&quot; 🙂 자연어처리 1차 팀프로젝트를 위한 심리 상담 챗봇입니다. &quot;&quot;&quot; &quot;&quot;&quot; 💜 SentenceTransformer를 사용하여 문장을 임베딩하고 이를 코사인 유사도와 함께 비교하여 가장 유사한 답변을 채택합니다. &quot;&quot;&quot;)# 왼쪽 sidebar 부분st.sidebar.header(&quot;NLP PROJECT&quot;)st.sidebar.subheader(&quot;TEAM : Think_Big&quot;)st.sidebar.subheader(&quot;팀원&quot;)st.sidebar.text(&quot;조인환(팀장)&quot;)st.sidebar.text(&quot;김영진&quot;)st.sidebar.text(&quot;최예은&quot;)st.sidebar.text(&quot;백서윤&quot;)# session_state : 각 사용자 세션에 대해 재실행 간에 변수를 공유하고 상태를 저장하고 유지# 챗봇이 대화한 내용을 저장하는 generated session_state를 만든다if 'generated' not in st.session_state: st.session_state['generated'] = []# 유저가 대화한 내용을 저장하는 past session_state를 만든다if 'past' not in st.session_state: st.session_state['past'] = []## session_state를 사용하면 streamlit이 자동으로 재실행되도 초기화가 되지 않도록 한다.# form을 만들어서 유저의 입력박스와 전송 버튼을 만든다with st.form('form', clear_on_submit=True): # clear_on_submit=True : 전송 버튼을 누르면 텍스트 박스가 자동으로 지워진다. user_input = st.text_input('당신: ', '') submitted = st.form_submit_button('전송')# 유저의 input에 질문이 입력되면 인코딩하여 벡터화한다if submitted and user_input: embedding = model.encode(user_input) # 유저의 질문과 데이터의 질문들을 코사인 유사도로 비교한 후, # 가장 유사도가 높은 답변을 출력 df['distance'] = df['embedding'].map(lambda x: cosine_similarity([embedding], [x]).squeeze()) answer = df.loc[df['distance'].idxmax()] # 유저의 질문을 past에 저장 st.session_state.past.append(user_input) # 챗봇의 답변을 generated에 저장 st.session_state.generated.append(answer['A'])# 유저의 질문과 챗봇의 답변을 메세지창에 출력하도록 하는 구문for i in range(len(st.session_state['past'])): message(st.session_state['past'][i], is_user=True, key=str(i) + '_user') if len(st.session_state['generated']) &gt; i: message(st.session_state['generated'][i], key=str(i) + '_bot')","link":"/2022/12/10/Study_folder/NLP(Natural_Language_Processing)/2022-12-10-SBERT-chatbot/"},{"title":"자연어 전처리 인코딩, 패딩","text":"자연어 VOCABULARY 만들기 자연어 처리에서는 텍스트를 숫자로 바꾸는 여러가지 기법들이 있습니다. 그러한 기법들을 사용하기 위해 첫 단계로 각 단어를 고유한 정수에 맵핑(mapping)시키는 전처리 작업입니다. Vocab을 설명하기 위해서는 특정 단어가 많이 있어야(제거해야할 필요성이 있음) 보여 줄 수 있기에 여러 문장, 단어를 사용 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import nltkfrom nltk.tokenize import sent_tokenizefrom nltk.tokenize import word_tokenizefrom nltk.corpus import stopwordstext = 'This is a good place. I want to go climbing right now. I dont know where this place is'# 자연어 처리를 위한 패키지 다운받기(한국어는 다른 패키지 사용해야 합니다)nltk.download('punkt')nltk.download('stopwords')# 문장 나누기(텍스트 데이터 -&gt; 문장 단위로 토큰화)sentences = sent_tokenize(text)print(sentences)# 단어 토큰화(문장 단위 -&gt; 토큰(단어)단위 토큰화)word_tokenize(sentences[0])# 출력 결과# ['This is a good place.', 'I want to go climbing right now.', 'I dont know where this place is']## ['This', 'is', 'a', 'good', 'place', '.']# 단어 사전 만들기vocab = {}preprocessed_sentences = []stop_words = set(stopwords.words('english'))for sentence in sentences: tokenized_sentence = word_tokenize(sentence) result = [] for word in tokenized_sentence: word = word.lower() if word not in stop_words : # 단어 토큰화 된 결과에 대해서 불용어를 제거한다.(보통 따로 추가로 단어장 만들어서 추가로 제거) if len(word) &gt; 2: result.append(word) if word not in vocab: vocab[word] = 0 vocab[word] += 1 preprocessed_sentences.append(result) print(preprocessed_sentences)# [['good', 'place'], ['want', 'climbing', 'right'], ['dont', 'know', 'place']]print('VOCAB :',vocab)# VOCAB : {'good': 1, 'place': 2, 'want': 1, 'climbing': 1, 'right': 1, 'dont': 1, 'know': 1}# 이제 빈도수 별로 인코딩을 하는데 Counter 함수를 많이 이용합니다.from collections import Counterall_words_list = sum(preprocessed_sentences, []) # 2차원리스트 -&gt; 1차원리스트vocab = Counter(all_words_list)print(vocab)# Counter({'place': 2, 'good': 1, 'want': 1, 'climbing': 1, 'right': 1, 'dont': 1, 'know': 1}) 패딩처리 하기(Padding) 컴퓨터는 길이가 전부 동일한 문서들에 대해 하나의 행렬로 보고, 한꺼번에 묶어서 처리할 수 있습니다. 다시 말해 병렬 연산을 위해서 문장의 길이를 통일 시켜주는 작업이 필요합니다. 파이토치의 from torch.nn.utils.rnn import pad_sequence 또는 케라스의 from tensorflow.keras.preprocessing.sequence import pad_sequences를 이용하여 패딩하면 됩니다. 12345678910111213141516171819202122232425print(preprocessed_sentences)# [['good', 'place'], ['want', 'climbing', 'right'], ['dont', 'know', 'place']]from tensorflow.keras.preprocessing.sequence import pad_sequencesfrom tensorflow.keras.preprocessing.text import Tokenizer# 토크나이저tokenizer = Tokenizer()tokenizer.fit_on_texts(preprocessed_sentences)encoded = tokenizer.texts_to_sequences(preprocessed_sentences)print(encoded)[[2, 1], [3, 4, 5], [6, 7, 1]]padded = pad_sequences(encoded)print(padded) #기본 옵션으로 패딩# array([[2, 1, 0],# [3, 4, 5],# [6, 7, 1]], dtype=int32)padded = pad_sequences(encoded, padding='post') #padding의 기본 옵션이 'post'이고 뒤에 0을 붙인다는 의미입니다. padded# array([[2, 1, 0, 0, 0],# [3, 4, 5, 0, 0],# [6, 7, 1, 0, 0]], dtype=int32) 한국어 처리는 nltk가 아닌 보통 다른 패키지를 konly같은 한국어용 버전을 따로 다운받아서 사용함 1234567891011import konlpyfrom konlpy.tag import Mecab###################mecab = Mecab()sentence = '안녕하세요 테스트용 텍스트 입니다.'temp_X = mecab.morphs(sentence)temp_X# 출력 결과['안녕', '하', '세요', '테스트', '용', '텍스트', '입니다', '.']","link":"/2022/12/30/Study_folder/NLP(Natural_Language_Processing)/2022-12-30-int-encoding/"},{"title":"Seq to Seq &amp; Attention &amp; Transformer","text":"기계 번역 모델의 발전 참고 : &lt;위키 독스&gt; 참고 : &lt;Jay Almmar 자료&gt; GPT : Transformer의 디코더 아키텍쳐를 활용 BERT : Transformer의 인코더 아키텍쳐를 활용 Seq2Seq2(시퀀스 투 시퀀스) - `[인코더 - 컨텍스트백터 - 디코더]`로 이루어져 있고 - seq2seq는 인코더와 디코더 아키텍처의 내부는 사실 `두 개의 RNN 구조`로 이루어진 모델 - 시퀀스-투-시퀀스(Sequence-to-Sequence, seq2seq)는 챗봇과 기계 번역에서 많이 사용됩니다. - 대략적인 구조는 `입력 시퀀스`(질문)와 `출력 시퀀스`(대답)으로 이루어져 있습니다. 입력 문장의 모든 단어들을 순차적으로 입력받은 뒤에 마지막에 이 모든 단어 정보들을 압축해 하나의 벡터로 만듭니다. 이를 컨텍스트 벡터(context vector)라고 부릅니다.(이는 고정된 크기라서 한계점이 존재) 입력 문장의 정보가 하나의 컨텍스트 벡터로 모두 압축되면 벡터를 디코더로 전송합니다. 디코더는 번역된 단어를 순차적으로 출력합니다. 아래 동영상을 보시면 쉽게 이해될 것입니다(10초) seq2seq의 한계점 항상 고정된 크기의 벡터에(컨텍스트 벡터) 모든 정보를 저장하기 때문에 bottle neck현상이 발생하여 정보 손실이 발생한다. 입력의 길이가 길어지면 기울기 소실 문제가 발생한다.(context vector를 기준으로 Encoder, Decoder가 완전히 분리되어 있으므로 입출력의 연관 관계가 너무 떨어져 있어서 역전파 시 기울기 소실 발생) 어텐션 모델(Attention) seq2seq의 문제점을 해결하기 위한 모델이 Attention 모델이다. 어텐션은 중요한 부분에 더 attention(집중)하자는 요지로 만들어진 모델이며, 간단히 말해서 인코딩의 모든 은닉층의 정보를 디코더로 전달하는 것이다.(Encoder의 hidden state를 Decoder에서도 사용하는 방법) 단어에 가중치를 주는 레이어가 추가됨(seq2seq에 어텐션 레이어가 추가됨) 아래의 동영상(10초)은 seq2seq에 attention을 추가한 동영상입니다. 트랜스포머(transformer) 앞의 어텐션을 RNN의 보정 용도가 아닌 어텐션만으로 인코더와 디코더를 만든 모델이 트렌스포머!(이 모델이 등장하고 자연어 분야의 생태계가 변했습니다) 인코더와 디코더라는 단위가 여러 개로 구성되는 구조입니다. 트랜스포머를 제안한 논문에서는 인코더와 디코더의 개수를 각각 6개 사용하였습니다. 디코더는 seq2seq 구조처럼 시작 토큰 부터 종료 토큰 까지 연산을 진행합니다. 이는 RNN은 사용되지 않지만 여전히 인코더-디코더의 구조는 유지되고 있음을 보여줍니다. 하지만 트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니므로 단어의 위치 정보를 다른 방식으로 알려줄 필요가 있는데 단어의 위치 정보를 얻기 위해서 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용하는데, 이를 포지셔널 인코딩(positional encoding)이라고 합니다. 인코더는 총 레이어 개수 만큼의 층 연산을 순차적으로 한 후에 마지막 층의 인코더의 출력을 디코더에게 전달합니다. 인코더 연산이 끝났으면 디코더 연산이 시작되어 디코더 또한 만큼의 연산을 하는데, 이때마다 인코더가 보낸 출력을 각 디코더 층 연산에 사용합니다. 디코더도 인코더와 동일하게 임베딩 층과 포지셔널 인코딩을 거친 후의 문장 행렬이 입력됩니다. 트랜스포머의 디코더에서는 현재 시점의 예측에서 미래에 있는 단어들을 참고하지 못하도록 룩-어헤드 마스크(look-ahead mask)를 도입했습니다.","link":"/2022/12/31/Study_folder/NLP(Natural_Language_Processing)/2022-12-31-nlp-model-history/"},{"title":"SBERT(Sentence BERT) 개념","text":"SBERT(Sentence + BERT) Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks(2019)의 논문에서 처음 등장하였다.기존의 BERT는 학습시간이 너무 많이 걸리는 단점을 보완한 모델 Sentence-BERT 는 word embedding model이다. 워드임베딩이란 텍스트 분석을 위한 단어 표현에 사용되는 용어로, 일반적으로 벡터 공간에서 더 가까운 단어가 의미가 유사할 것으로 예상되도록 단어의 의미를 인코딩하는 실수 값 벡터의 형태입니다. SBERT의 데이터는 2가지 종류로 구분됩니다. STS(Semantic Textual Similarity) NLI(Natural Language Inference) STS 두 문장 사이의 문장간 similarity(의미적 유사성)의 정도를 평가하고 분류 보통 0(관련 없음)에서 5.0(거의 유사)사이의 값으로 분류 NLI(Natural Language Inference) 두 문장(premise, hypothesis)을 입력 받아 두 관계를 entailment(positive), contradiction(negative), neutral로 분류 a : 나는 학원에 간다(anchor)b : 나는 버스를 타고 학원에 간다(entailment)c : 나는 헬스장에서 운동을 한다(contradiction) 학습 :두 문장(s1, s2)이 초기화 된 siamese network(샴 네트워크)로 입력두 입력 문장에 대한 임베딩 벡터값(e1, e2)을 추출두 임베딩 벡터(e1, e2)에 대해 유사도를 계산만약 두 입력 문장이 같은 class라면 거리가 가까워지도록, 다른 class라면 거리가 멀어지도록 weight을 조절하며 학습 SBERT에서는 cost function으로 triplet loss을 사용 ==&gt; 앵커 문장과 동일(다른) 클래스 문장 거리 softmax를 사용하는 대신 -&gt; MNR loss(triplet loss와 유사) 사용 SBERT - finetuning하기 위한 개요 NLI dataset(문장과의 관계도), 문장과의 유사도(거리) 두 가지가 필요하다. triplet 구조로(엥커, Positive 문장, Negative 문장) NLI 데이터셋을 구성한다. 예를 들면 아래와 같은 구조이다.(a,b,c) a : 나는 학원에 간다(anchor) b : 나는 버스를 타고 학원에 간다(entailment) c : 나는 헬스장에서 운동을 한다(contradiction) pretrained BERT모델로 NLI 데이터를 문장 임베딩 벡터로 변환 MNR loss(MultipleNegativesRankingLoss) 함수를 이용하여 NLI 데이터를 파인튜닝","link":"/2023/01/04/Study_folder/NLP(Natural_Language_Processing)/2023-01-04-SBERT-nli/"},{"title":"Attention 파악하기","text":"참고 자료 : &lt;김진솔님 블로그&gt; 참고 영상 : &lt;Aladdin youtube&gt; 어텐션 모델 seq2seq의 문제점을 해결하기 위한 모델이 Attention 모델이다. 어텐션은 중요한 부분에 더 집중을 하자는 요지로 만들어진 모델이며, 간단히 말해서 인코딩의 모든 은닉층의 정보를 디코더로 전달하는 것이다. 단어에 가중치를 주는 레이어가 추가됨(seq2seq에 어텐션 레이어가 추가됨) 마스크(Mask)란? Masking 라는 의미는 가린다는 의미이다. 디코더(Decoder)에서의 Self-Attention Layer 는 반드시 자기 자신 보다 앞쪽에 포지션에 해당하는 토큰들의 어텐션 스코어만 볼수있다. 아웃풋들이 주어졌을 때 뒤에 나오는 단어들은 볼 수 없다. (transformer도 같음) Masking을 수학적으로 구현할 때는 Score 값을 -inf (마이너스 무한대) 값으로 표기함으로써 구현할 수 있습니다.(값을 구하고 이를 -inf 값으로 변경) MLM(Masked Language Model) MLM은 마스크가 무엇인지 명료하게 알려주는 모델입니다. 입력 문장에서 임의로 Token을 마스킹(masking), 그 Token을 맞추는 방식인 MLM 학습 진행 문장의 빈칸 채우기 문제를 학습 생성 모델 계열은(예를 들어 GPT) 입력의 다음 단어를 예측 MLM은 문장 내 랜덤한 단어를 마스킹 하고 이를 예측 입력의 15% 단어를 [MASK] Token으로 바꿔주어 마스킹 이 때 80%는 [MASK]로 바꿔주지만, 나머지 10%는 다른 랜덤 단어로, 또 남은 10%는 바꾸지 않고 그대로 둠 이는 튜닝 시 올바른 예측을 돕도록 마스킹에 노이즈를 섞음 Scaled Dot Product Attention(흔히 어텐션으로 알려진 Attention) 입력값은 Q(query), K(key), V(value) 그리고 attention mask로 구성 되어 있습니다. 입력값 중 K, V는 같은 값 이어야 합니다. Q까지 K, V와 동일한 경우는 self attention이라 합니다. Query : 찾고자 하는 대상 Key : 저장된 데이터를 찾고자 할 때 참조하는 값 Value : 저장되는 데이터 1234567# dictionary 구조를 생각하시면 됩니다{ 'key1' : 'value1', 'key2' : 'value2'}# query의 값으로는 'key1' 또는 'key2'가 될 수 있습니다.# 이때 query와 같은 key값을 선택할 지 또는 가장 유사한 key값을 선택할 지는 문제에 따라 달라지게 됩니다. 여기서 중요한 점은 Q와 Key값들이 얼마나 유사한지 계산하는 것입니다. 즉, (softmax를 적용하여 총 합이 1인) Key값들과 Value의 값을 곱한 후 모두 더하면 Attention value가 되는 원리입니다. Query는 Decoder의 은닉층(hidden state)가 됩니다. Attention에서는 Encoder의 hidden state를 Key와 Value로 사용합니다.(앞서 말한 self attention) 즉, Key와 Value는 같고 단어의 갯수 만큼 Key 값을 가집니다. 원-논문에서 Compare는 Fully Connected 방식의 연산을 이용하였고 Aggregate의 경우 모든 key-value에 대하여 벡터의 element-wise multiplication 연산을 한 후 element-wise sum을 하여 Attention Value를 생성합니다.수식은 아래와 같습니다. $$ \\text{Compare}(q, k_{j}) = q \\cdot k_{j} = q^{T}k_{j} $$$$ \\text{Aggregate}(c, V) = \\sum_{j} c_{j}v_{j} $$ 요약 아래 그림들로 요약을 하겠습니다. Encoder의 hidden state는 (Key, Value)로 사용됩니다. 아래의 그림에서 h는 Key와 Value로 사용됩니다. Decoder의 hidden state는 Query로 사용됩니다. Decoder에서 s라는 Query가 입력되고 그 Query와 모든 key 값인 h (아래 그림에서는 $$ h_{0}, h_{1}, h_{2} $$)와 Comparison 연산을 통하여 유사도를 구합니다. 값은 softmax를 거치기 때문에 확률 값처럼 총 합이 1이 됩니다. 그러면 Value에 해당하는 h와 유사도를 곱하고 결과들을 합하여 최종적으로 a 라는 Attenen value를 출력합니다. 수식으로 표현하면 다음과 같습니다. $$ c_{i} = \\text{softmax}(s_{i}^{T}h_{j}) $$$$ a_{i} = \\sum_{j}c_{i}h_{j} $$ 그리고 Decoder의 은닉층은 RNN(또는 LSTM)에서 연산하여 $$ s_{i} $$ → $$ s_{i+1} $$로 만듭니다. 그 후 $$ a_{i} $$와 $$ s_{i+1} $$을 합하고 ($$ v_{i} = [s_{i}; a_{i-1}] $$) 하여 $$ v_{i+1} $$을 만듭니다. 이 값을 FC layer와 Softmax를 거쳐서 최종 출력인 $$ y_{i} $$를 출력합니다. attention에서 Query, Key, Value를 사용하지 않고 다른 방법으로 사용도 가능합니다. 바다나우 어텐션 vs Dot_Product_Attention 컨텍스트 벡터(context vector) 바다나우: 컨텍스트 벡터를 구할 때 이전 시점의 은닉 상태를 사용한다.Dot_Product_Attention: 컨텍스트 벡터를 구할 때 현재 시점의 은닉 상태 st를 사용한다. 출력 바다나우: 현재 시점의 은닉 상태로부터 출력이 나온다.Dot_Product_Attention: 현재 시점의 은닉 상태는 RNN의 은닉 상태 역할만 하고, 새로운 벡터를 사용한다. 계산 속도 바다나우: 디코더의 은닉 상태를 구할 때 컨텍스트 벡터가 사용되므로 RNN의 재귀 연산이 수행될 때 컨텍스트 벡터가 구해질 때까지 기다려야 한다. 계산이 느림Dot_Product_Attention: 계산이 빠름 인코더의 은닉 상태 사용 바다나우: 인코더의 모든 은닉 상태의 벡터를 본다.Dot_Product_Attention: 특정 하이퍼파라미터 D에 대해 (2D+1)개의 부분집합 벡터만 본다. 셀프 어텐션이란? 같은 문장 내의 두 token 사이의 Attention을 계산하는 방식은, Self-Attention이라고 부른다. Q, K, V 형태가 동일 반면, 서로 다른 두 문장에 각각 존재하는 두 token 사이의 Attention을 계산하는 것을 Cross-Attention이라고 부른다.","link":"/2023/01/05/Study_folder/NLP(Natural_Language_Processing)/2023-01-05-attention/"},{"title":"Attention 코드로 구현하기","text":"스케일드 닷-프로덕트 어텐션 참조 : &lt;위키독스&gt; 닷-프로덕트 어텐션(dot-product attention)에서 스케일링하는 것을 추가하면 스케일드 닷-프로덕트 어텐션(Scaled dot-product Attention)이라고 합니다 scaled_dot_product_attention을 tensorflow로 구현, 살펴보겠습니다. 12345678910111213141516171819202122232425def scaled_dot_product_attention(query, key, value, mask): # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads) # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads) # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads) # padding_mask : (batch_size, 1, 1, key의 문장 길이) # Q와 K의 곱. 어텐션 스코어 행렬. matmul_qk = tf.matmul(query, key, transpose_b=True) # 스케일링 depth = tf.cast(tf.shape(key)[-1], tf.float32) logits = matmul_qk / tf.math.sqrt(depth) # 마스킹, 매우 작은 값이므로 소프트맥스 함수에 의해 0이 된다. if mask is not None: logits += (mask * -1e9) # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행(axis=-1) # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이) attention_weights = tf.nn.softmax(logits, axis=-1) # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads) output = tf.matmul(attention_weights, value) return output, attention_weights 테스트 temp_q의 값 [0, 10, 0]은 Key에 해당하는 temp_k의 두번째 값 [0, 10, 0]과 일치하게 설정 1234567891011121314import tensorflow as tfimport numpy as np# 임의의 Query, Key, Value인 Q, K, V 행렬 생성np.set_printoptions(suppress=True) #옵션 넣어줘야 보기 편함(소수점 반올림)temp_k = tf.constant([[10,0,0], [0,10,0], [0,0,10], [0,0,10]], dtype=tf.float32) # (4, 3)temp_v = tf.constant([[1,0], [10,0], [100,5], [1000,6]], dtype=tf.float32) # (4, 2)temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32) # (1, 3) #transpose_b 어텐션 분포는 [0, 1, 0, 0]의 값을 가지며 Value의 두번째 값인 [10, 0]이 출력되는 것을 확인할 수 있습니다. 12345678# 함수 실행temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)print(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)# tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)print(temp_out) # 어텐션 값# tf.Tensor([[10. 0.]], shape=(1, 2), dtype=float32)","link":"/2023/01/08/Study_folder/NLP(Natural_Language_Processing)/2023-01-08-attention-imp/"},{"title":"OpenCV 기초","text":"OpenCV 이미지 확인123456789import cv2# imread로 이미지 읽어오기img = cv2.imread('cat_img.jpg')# imshow('이미지 표시 이름',이미지파일)입력cv2.imshow('output',img)waitKey(0) #(0을 넣으면 == 계속 딜레이 - 대기하라는 의미) imshow의 앞 ‘output’이름으로 img 파일이 출력됩니다. 사진, 영상을 끄려면 키보드 q를 누르시면 됩니다. 내장 카메라(웹캠) 실행12345678910111213141516import cv2# 웹캠cap = cv2.VideoCapture(0) #노트북은 0번이 기본 내장 웹캠cap.set(3,640) #id_nubmer, widthcap.set(4,480) #id_number, heightcap.set(10,100) #id_number,bright# 웹캠은 프레임 단위로 계속 출력되기 때문에 일반적으로 while문으로 실행한다.# 키보드 'q'버튼을 누르면 실행이 종료됩니다.# success는 Ture,False이고, img는 프레임 단위의 이미지로 저장되는 형태이다.while True: success, img = cap.read() cv2.imshow('video_mp4', img) if cv2.waitKey(1) &amp; 0xFF == ord('q'): break 실행 결과 이미지 저장1234import cv2loaded_img = cv2.imread('cat_img.jpg')cv2.imwrite('folder/folder1/img.jpg',loaded_img)","link":"/2023/01/11/Study_folder/OpneCV/2023-01-11-basic-cv2/"},{"title":"OpenCV에 관한 정보","text":"OpenCV openCV를 주피터 환경에서 실행 시 동영상이나 사진의 (x)버튼을 누르면 주피터가 먹통이 될 때가 많다. 키보드 ‘q’를 눌러서 끄는걸 권장한다. 맥북 주피터 환경에서는 (x)버튼이 없어서 아래 명령어를 입력해주면 꺼지기는 한다.(터미널을 종료해도 꺼짐) 다만, 파이참이나 VScode를 활용해 .py파일을 인터프리터로 실행하기를 권장한다. 12345cv2.destroyAllWindows()cv2.waitKey(1)cv2.waitKey(1)cv2.waitKey(1)cv2.waitKey(1) 인터프리터로 실행 시 경로 설정하기 인터프리터 환경(터미널환경)에서 실행 시 주피터의 실행 경로와 다를 수 있기 때문에 경로를 수정해줘야 오류가 안나온다. VScode에서 경로를 확인, 수정하는 방법은 다음과 같다. 먼저, 주피터 노트북에서도 아래의 명령어를 입력하고 실행을 한다. 그 다음 py 파일에에도 같은 입력을 하고 저장 &amp; 실행을 한다. 123456import sysprint(sys.executable)# 출력 결과# '/opt/anaconda3/bin/python' VScode의 경우는 명령 팔레트를 열고 (Shift + command + P) Python: Select Interpreter을 검색 &amp; 누른다. 제 경우는 /opt/anaconda3/bin/python 앞의 출력 결과(주피터)에 동일한 결과를 선택한다. 실행해서 잘 되는지 확인한다.","link":"/2023/01/11/Study_folder/OpneCV/2023-01-11-basic-tip-cv2/"},{"title":"OpenCV 이미지 변환","text":"Grayscale, GaussianBlur1234567891011121314import cv2img = cv2.imread('cat_img.jpg')# 컬러 -&gt; 흑백으로 변환imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)# ksize == kernel size이므로 홀수만 사용 가능, sigmaX는 블러 정도라고 생각하면 편하며,# 보통 -3~3사이의 값을 사용imgBlur = cv2.GaussianBlur(imgGray, ksize=(7,7),sigmaX=3)cv2.imshow('output',imgGray)cv2.imshow('output2',imgBlur)cv2.waitKey(0) 결과 imgGray 결과 imgBlur Canny, Dialation1234567891011121314151617import cv2img = cv2.imread('cat_img.jpg')imgCanny = cv2.Canny(img,100,100)kernel = np.ones((5,5), np.uint8)# iterations에 높은 수를 넣으면 윤곽선이 더 커집니다.imgDialation = cv2.dilate(imgCanny, kernel=kernel,iterations=1)imgEroded = cv2.erode(imgDialation, kernel=kernel, iterations=1)cv2.imshow('output3',imgCanny)cv2.imshow('output4',imgDialation)cv2.imshow('output5',imgEroded)cv2.waitKey(0) 결과 imgCanny 결과 imgDialation 결과 imgEroded","link":"/2023/01/11/Study_folder/OpneCV/2023-01-11-img_change/"},{"title":"OpenCV shape 변환","text":"OpenCV와 PIL의 이미지 형태 결과를 보시면 알겠지만 width, height이 반대입니다. 1234567891011121314import cv2from PIL import Imageimg = cv2.imread('Resources/cards.jpg')h, w, c = img.shape #가로 세로 채널(색)print(img.shape)#(500, 477, 3)pil_img = Image.open('Resources/cards.jpg')w, h = pil_img #세로 가로print(a.size)#(477, 500) openCV 사진 좌표 형태 openCV에서 좌표는 아래 사진처럼 표현 됩니다. x축은 오른쪽으로 가지만 y축은 아래로 가는게 중요합니다. Resize12345678910111213141516import cv2img = cv2.imread('lena.png')print(img.shape)#(512,512,3)imgResize = cv2.resize(img,(200,150))print(imgResize.shape)#(200,150,3)cv2.imshow('origin',img)cv2.imshow('resize',imgResize)cv2.waitKey(0) Crop1234567891011121314151617181920212223import cv2img = cv2.imread('lena.png')print(img.shape)#(512,512,3)h, w, c = img.shapeimgResize = cv2.resize(img,(500,400))print(imgResize.shape)#(500,400,3)# cv에서 crop은 [width(세로), height(가로), 색상 채널]입니다imgCropped = img[0:200,200:400]cv2.imshow('resize',imgResize)cv2.imshow('cropped',imgCropped)cv2.waitKey(0) Drawing 영역 만들기 123456789101112import cv2#0값은 black으로 표현됨img = np.zeros((512,512,3), np.uint8)# 파란색 영역을 그림img[200:300, 100:500] = 255,0,0 #BGR(Blue, Green, Red)cv2.imshow('image1', img)cv2.waitKey(0) line, rectangle 그리기 1234567img = np.zeros((512,512,3), np.uint8)#start_point, end_point, color, thicknesscv2.line(img,(0,0),(300,300),color=(0,0,255),thickness=3)cv2.imshow('image1', img)cv2.waitKey(0) 1234cv2.rectangle(img,(50,50),(500,200),(0,255,0),3)cv2.imshow('image1', img)cv2.waitKey(0) 1234cv2.circle(img,(200,200), 30, (255,20,50),thickness=3)cv2.imshow('image1', img)cv2.waitKey(0) 12345cv2.putText(img, 'THIS IS Text', (50,300),fontFace=cv2.FONT_HERSHEY_COMPLEX,fontScale=1,color=(255,0,0),thickness=5)cv2.imshow('image1', img)cv2.waitKey(0)","link":"/2023/01/11/Study_folder/OpneCV/2023-01-11-shape/"},{"title":"OpenCV 코랩에서 웹캠 사용하기","text":"Javascript를 이용하여 코랩에서 웹캠을 켜는 방법입니다 filename은 photo.jpg로 저장 되는데 변경해서 사용하시면 됩니다. js로 함수를 설정합니다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263from IPython.display import display, Javascriptfrom google.colab.output import eval_jsfrom base64 import b64decodedef take_photo(filename='photo.jpg', quality=0.8): #javascript 작성 시작 js = Javascript(''' async function takePhoto(quality) { //div(공간) 생성 const div = document.createElement('div'); //button 생성 const capture = document.createElement('button'); capture.textContent = 'Capture'; div.appendChild(capture); //video 생성 const video = document.createElement('video'); //비디오 모양 네모네모 video.style.display = 'block'; //카메라(웹캠) 불러오기 const stream = await navigator.mediaDevices.getUserMedia({video: true}); //div 밑에 child 공간 추가 document.body.appendChild(div); //공간에 video 넣기 div.appendChild(video); //video와 웹캠 연결 video.srcObject = stream; //await -&gt; 비동기식 처리 (thread와 관련)(async와 세트) await video.play(); // Resize the output to fit the video element. google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true); // Wait for Capture to be clicked. await new Promise((resolve) =&gt; capture.onclick = resolve); //canvas 생성 const canvas = document.createElement('canvas'); //크기 맞추기 canvas.width = video.videoWidth; canvas.height = video.videoHeight; //이미지 그리기 canvas.getContext('2d').drawImage(video, 0, 0); //비디오 끄기 stream.getVideoTracks()[0].stop(); //div 삭제 div.remove(); // 파일 주소 반환 return canvas.toDataURL('image/jpeg', quality); } ''') display(js) data = eval_js('takePhoto({})'.format(quality)) #웹 브라우저에서 데이터를 저장할때 base64로 저장 binary = b64decode(data.split(',')[1]) with open(filename, 'wb') as f: f.write(binary) return filename 실행 코드입니다. 1234567891011from IPython.display import Imagetry: filename = take_photo() print('Saved to {}'.format(filename)) # Show the image which was just taken. display(Image(filename))except Exception as err: # Errors will be thrown if the user does not have a webcam or if they do not # grant the page permission to access it. print(str(err)) 실행 결과 입니다.(capture button을 누르면 실행 종료)","link":"/2023/01/15/Study_folder/OpneCV/2023-01-15-webcam-in-colab/"},{"title":"OpenCV 윤곽선 검출","text":"contour123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import cv2import numpy as np# 이미지 여러 장 출력 함수def stackImages(scale,imgArray): rows = len(imgArray) cols = len(imgArray[0]) rowsAvailable = isinstance(imgArray[0], list) width = imgArray[0][0].shape[1] height = imgArray[0][0].shape[0] if rowsAvailable: for x in range ( 0, rows): for y in range(0, cols): if imgArray[x][y].shape[:2] == imgArray[0][0].shape [:2]: imgArray[x][y] = cv2.resize(imgArray[x][y], (0, 0), None, scale, scale) else: imgArray[x][y] = cv2.resize(imgArray[x][y], (imgArray[0][0].shape[1], imgArray[0][0].shape[0]), None, scale, scale) if len(imgArray[x][y].shape) == 2: imgArray[x][y]= cv2.cvtColor( imgArray[x][y], cv2.COLOR_GRAY2BGR) imageBlank = np.zeros((height, width, 3), np.uint8) hor = [imageBlank]*rows hor_con = [imageBlank]*rows for x in range(0, rows): hor[x] = np.hstack(imgArray[x]) ver = np.vstack(hor) else: for x in range(0, rows): if imgArray[x].shape[:2] == imgArray[0].shape[:2]: imgArray[x] = cv2.resize(imgArray[x], (0, 0), None, scale, scale) else: imgArray[x] = cv2.resize(imgArray[x], (imgArray[0].shape[1], imgArray[0].shape[0]), None,scale, scale) if len(imgArray[x].shape) == 2: imgArray[x] = cv2.cvtColor(imgArray[x], cv2.COLOR_GRAY2BGR) hor= np.hstack(imgArray) ver = hor return ver#윤곽선 검출 함수def getContour(img): contours, hierarchy = cv2.findContours(img,mode = cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE) for cnt in contours: area = cv2.contourArea(cnt) # imgContour이미지에 contour를 파란색으로 drawing한다는 의미 if area &gt; 500 : cv2.drawContours(imgContour, cnt, -1, (255,0,0),3) img = cv2.imread('Resources/shapes.png')empty = np.zeros_like(img)imgContour = img.copy()imgGray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)imgBlur = cv2.GaussianBlur(imgGray, (7,7),1)imgCanny = cv2.Canny(imgBlur,50,50)# 보통 contour는 img-&gt;gray-&gt;blur-&gt;canny-&gt;contour를 이용하여 검출한다.getContour(imgCanny)stackimg = stackImages(0.5,[[img,imgGray, imgBlur], [imgCanny,imgContour,empty]])cv2.imshow('img', stackimg)cv2.waitKey(0) 보통 contour는 img-&gt;gray-&gt;blur-&gt;canny-&gt;contour를 이용하여 검출한다. 5번째 이미지(imgContour)가 윤곽선이 검출된 이미지 contour를 통하여 bounding box 검출하는 방법입니다123456789101112131415161718192021222324252627282930def getContour(img): contours, hierarchy = cv2.findContours(img,mode = cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE) for cnt in contours: area = cv2.contourArea(cnt) # imgContour이미지에 contour를 파란색으로 drawing한다는 의미 if area &gt; 500 : cv2.drawContours(imgContour, cnt, -1, (255,0,0),3) param = cv2.arcLength(cnt, True) approx = cv2.approxPolyDP(cnt, 0.02 * param, True) #근접한 포인트(점)이 있는지 확인 #print(len(approx)) #3이면 삼각형, 4는 사격형, 그 이상이면 원 obj_corner = len(approx) # object corner를 검출했으면 그걸로 바운딩 박스를 만듭니다. x,y,w,h = cv2.boundingRect(approx) if obj_corner == 3 : object_type = 'triangle' elif obj_corner ==4 : aspRatio = w/float(h) if aspRatio &gt; 0.95 and aspRatio &lt; 1.05: object_type = 'sqare' else: object_type = 'rectangle' else : object_type = 'circle' #rectangle을 만들고, 텍스트를 붙여서 출력합니다. cv2.rectangle(imgContour, (x,y), (x+w, y+h), (0,255,0),2) cv2.putText(imgContour, object_type, (x+(w//2)-7, y +(h//2)-10),cv2.FONT_HERSHEY_SIMPLEX,0.6,(0,0,0),2) 윤곽선 검출 및 바운딩 박스 생성 이미지(5번째 이미지)","link":"/2023/01/18/Study_folder/OpneCV/2023-01-18-contour/"},{"title":"OpenCV 얼굴, 눈 등 특정 객체 검출","text":"정면 얼굴 검출 haarcascade file 사용 haarcascade_frontalface_default.xml 을 사용하여 검출하는 방법입니다. 이 파일은 다른 사람들이 이미 얼굴을 검출하는 학습을 해둔 파일이며 이를 이용하면 편하게 얼굴을 인식할 수 있습니다. &lt;opencv-data-haarcascades&gt; 12345678910111213import cv2faceCascade = cv2.CascadeClassifier('Resources/haarcascade_frontalface_default.xml')img = cv2.imread('Resources/lena.png')imgGray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)faces = faceCascade.detectMultiScale(imgGray,1.1,4)for (x,y,w,h) in faces: cv2.rectangle(img, (x,y), (x+w, y+h), (255,0,0), 2)cv2.imshow('result',img)cv2.waitKey(0) opencv_face_detector.pbtxt 파일 사용 haarcascade와 방법은 유사합니다 12345678910111213141516171819202122232425262728293031323334353637383940414243import cv2def faceBox(faceNet, frame): frameWidth = frame.shape[1] frameHeight = frame.shape[0] blob = cv2.dnn.blobFromImage(frame, 1.0, (227,227), [104,117,123], swapRB=False) faceNet.setInput(blob) detection = faceNet.forward() bboxs = [] for i in range(detection.shape[2]): confidence = detection[0,0,i,2] if confidence &gt; 0.7 : x1 = int(detection[0,0,i,3] * frameWidth) y1 = int(detection[0,0,i,4] * frameHeight) x2 = int(detection[0,0,i,5] * frameWidth) y2 = int(detection[0,0,i,6] * frameHeight) bboxs.append([x1,y1,x2,y2]) cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0),1) return frame, bboxsfaceProto = &quot;opencv_face_detector.pbtxt&quot;faceModel = &quot;opencv_face_detector_uint8.pb&quot;faceNet = cv2.dnn.readNet(faceModel, faceProto)video = cv2.VideoCapture(0)padding = 20while True: ret, frame = video.read() frame, bboxs = faceBox(faceNet, frame) detect = faceBox(faceNet, frame) cv2.imshow('age_gender',frame) k = cv2.waitKey(1) if k == ord('q'): breakvideo.release()cv2.destroyAllWindows()","link":"/2023/01/18/Study_folder/OpneCV/2023-01-18-haarscascade/"},{"title":"OpenCV 여러장의 이미지를 한장으로 출력하는 방법","text":"OpenCV에서 여러 장의 이미지 한 윈도우로 출력하기 기본적으로 np.hstack, np,vstack 을 이용해 이미지를 복붙하는 원리입니다. 예시 코드를 보겠습니다. 12345678910111213import cv2import numpy as npimglena = cv2.imread('Resources/lena.png')print(imglena.shape)# (512, 512, 3)imgHor = np.hstack((imglena,imglena))print(imgHor.shape)# (512, 1024, 3) 이미지 numpy를 더 붙인 형태입니다.cv2.imshow('hori', imgHor)cv2.waitKey(0) 123456789101112131415161718192021222324252627282930313233343536373839404142434445import cv2import numpy as npdef stackImages(scale,imgArray): rows = len(imgArray) cols = len(imgArray[0]) rowsAvailable = isinstance(imgArray[0], list) width = imgArray[0][0].shape[1] height = imgArray[0][0].shape[0] if rowsAvailable: for x in range ( 0, rows): for y in range(0, cols): if imgArray[x][y].shape[:2] == imgArray[0][0].shape [:2]: imgArray[x][y] = cv2.resize(imgArray[x][y], (0, 0), None, scale, scale) else: imgArray[x][y] = cv2.resize(imgArray[x][y], (imgArray[0][0].shape[1], imgArray[0][0].shape[0]), None, scale, scale) if len(imgArray[x][y].shape) == 2: imgArray[x][y]= cv2.cvtColor( imgArray[x][y], cv2.COLOR_GRAY2BGR) imageBlank = np.zeros((height, width, 3), np.uint8) hor = [imageBlank]*rows hor_con = [imageBlank]*rows for x in range(0, rows): hor[x] = np.hstack(imgArray[x]) ver = np.vstack(hor) else: for x in range(0, rows): if imgArray[x].shape[:2] == imgArray[0].shape[:2]: imgArray[x] = cv2.resize(imgArray[x], (0, 0), None, scale, scale) else: imgArray[x] = cv2.resize(imgArray[x], (imgArray[0].shape[1], imgArray[0].shape[0]), None,scale, scale) if len(imgArray[x].shape) == 2: imgArray[x] = cv2.cvtColor(imgArray[x], cv2.COLOR_GRAY2BGR) hor= np.hstack(imgArray) ver = hor return verimg = cv2.imread(Resources/shapes.png)imgGray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)imglena = cv2.imread('Resources/lena.png')stackimg = stackImages(0.5,[[img,imgGray], [imglena,imglena]])cv2.imshow('img', stackimg)cv2.waitKey(0) 결과","link":"/2023/01/17/Study_folder/OpneCV/2023-01-18-opencv/"},{"title":"OpenCV로 아는 얼굴인지 확인하기","text":"예제 코드 123456789101112131415161718192021222324252627282930import cv2import numpy as npimport face_recognitionimgElon = face_recognition.load_image_file('elon1.png')imgTest = face_recognition.load_image_file('surprised_man.jpg')imgTest = cv2.cvtColor(imgTest,cv2.COLOR_BGR2RGB)imgElon = cv2.cvtColor(imgElon,cv2.COLOR_BGR2RGB)faceLoc = face_recognition.face_locations(imgElon)[0]encodeElon = face_recognition.face_encodings(imgElon)[0]cv2.rectangle(imgElon,(faceLoc[1],faceLoc[2]),(faceLoc[3],faceLoc[0]),(255,0,255),2)print(faceLoc) # (118, 304, 304, 118) top, right, bottom, leftfaceLocTest = face_recognition.face_locations(imgTest)[0]encodeTest = face_recognition.face_encodings(imgTest)[0]cv2.rectangle(imgTest,(faceLocTest[1],faceLocTest[2]),(faceLocTest[3],faceLocTest[0]),(255,0,255),2)results = face_recognition.compare_faces([encodeElon], encodeTest)faceDis = face_recognition.face_distance([encodeElon], encodeTest)print(results, faceDis)cv2.putText(imgTest, f'{results} {round(faceDis[0],2)}', (50,50), cv2.FONT_HERSHEY_COMPLEX,1, (0,0,255),1)cv2.imshow('imtest',imgTest)cv2.imshow('imelon',imgElon)cv2.waitKey(0) results가 True면 동일 인물, False면 다른 인물로 판단","link":"/2023/01/19/Study_folder/OpneCV/2023-01-19-face-recognizion/"},{"title":"Tensorflow 딥러닝 CNN","text":"Fasion_mnist 데이터 딥러닝으로 분류하기12345678910111213141516171819202122232425262728293031323334353637383940import tensorflow as tffrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Densefrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStoppingfrom tensorflow.keras.datasets import mnistfrom tensorflow.keras.utils import to_categorical## tensorflow 딥러닝#데이터fashion_mnist = tf.keras.datasets.fashion_mnist(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()# 차원 변환 후, 테스트셋과 학습셋으로 나누어 줍니다.train_images = train_images.reshape(train_images.shape[0], 784).astype('float32') / 255test_images = test_images.reshape(test_images.shape[0], 784).astype('float32') / 255train_labels = to_categorical(train_labels, 10)test_labels = to_categorical(test_labels, 10)#딥러닝 모델 정의model = Sequential()# 인풋 784 아웃풋 1024 (히든층)model.add(Dense(1024,input_dim = 784, activation= 'relu')) # 인풋 1024(생략), 아웃풋 512 (히든층)model.add(Dense(512, activation= 'relu')) # 인풋 512(생략), 아웃풋 10 (출력층)model.add(Dense(10, activation = 'softmax')) #input_dim생략 가능model.summary()model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])#콜백옵션modelpath=&quot;./MODEL_DIR/MNIST_MLP.hdf5&quot;checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)#학습history = model.fit(train_images, train_labels, validation_split=0.25, epochs=30, batch_size=200, verbose=0, callbacks=[early_stopping_callback,checkpointer])#평가print(&quot;\\n Test Accuracy: %.4f&quot; % (model.evaluate(test_images, test_labels)[1])) #0.8889 Fasion_mnist 데이터 CNN으로 분류하기123456789101112131415161718192021222324252627282930313233343536373839404142import tensorflow as tffrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStoppingfrom tensorflow.keras.datasets import mnistfrom tensorflow.keras.utils import to_categoricalfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D##tensorflow CNN#데이터fashion_mnist = tf.keras.datasets.fashion_mnist(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()# 차원 변환 후, 테스트셋과 학습셋으로 나누어 줍니다. + RGB차원 추가train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32') / 255test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32') / 255train_labels = to_categorical(train_labels, 10)test_labels = to_categorical(test_labels, 10)#모델 정의model = Sequential()model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(28, 28, 1), activation='relu'))model.add(Conv2D(64, (3, 3), activation='relu'))model.add(MaxPooling2D(pool_size=(2,2)))model.add(Dropout(0.25))model.add(Flatten())model.add(Dense(128, activation='relu'))model.add(Dropout(0.5))model.add(Dense(10, activation='softmax'))#컴파일 및 콜백옵션model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])modelpath=&quot;./MODEL_DIR/MNIST_CNN.hdf5&quot;checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)#학습history = model.fit(train_images, train_labels, validation_split=0.25, epochs=30, batch_size=200, verbose=0, callbacks=[early_stopping_callback,checkpointer])#평가print(&quot;\\n Test Accuracy: %.4f&quot; % (model.evaluate(test_images, test_labels)[1]))# Test Accuracy: 0.9237","link":"/2022/12/12/Study_folder/TensorFlow/2022-12-12-deep_CNN/"},{"title":"Tensorflow DATA Augmentation(이미지 증식)","text":"텐서플로우로 이미지 증식하기(데이터 뻥튀기)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120from zipfile import ZipFilefrom keras.models import Sequentialfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2Dfrom keras.preprocessing.image import ImageDataGenerator### 이미지 증식 전 분류하기#zip 해제하기directory = './'with ZipFile('TeamA_name2.zip') as f : x = [f.extract(file, directory) for file in f.namelist() if file.endswith('jpg')] #jpg파일만 압축 해제# 이미지 가져오기train_datagen = ImageDataGenerator(rescale=1./255)train_generator = train_datagen.flow_from_directory(directory='./TeamA_name2/train', target_size = (24,24), batch_size = 3, class_mode = 'categorical') # &quot;binary&quot; : 1D numpy array of binary labels . # &quot;categorical&quot; : 2D numpy array of one-hot encoded labels. Supports multi-label output. # &quot;sparse&quot; : 1D numpy array of integer labels.test_datagen = ImageDataGenerator(rescale =1./255)test_generator = test_datagen.flow_from_directory(directory='./TeamA_name2/test', target_size = (24,24), batch_size = 3, class_mode = 'categorical')#모델 세팅model = Sequential()model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(24,24,3)))model.add(Conv2D(64, (3, 3), activation='relu'))model.add(MaxPooling2D(pool_size=(2, 2)))model.add(Flatten())model.add(Dense(128, activation='relu'))model.add(Dense(4, activation='softmax'))model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics =['accuracy'])model.fit( train_generator, steps_per_epoch=15, epochs=30, validation_data=test_generator, validation_steps=5)scores = model.evaluate(test_generator, steps=5)print(&quot;%s: %.2f%%&quot; %(model.metrics_names[1], scores[1]*100))# 100% ########## 이미지 증식하여 시도#이미지 증식train_datagen = ImageDataGenerator( rescale=1./255, # rescale, fill_mode를 제외한 변수는 설정한 n값에 대해 -n ~ +n의 랜덤값을 가집니다 rotation_range=10, # 회전 범위 -10~10 width_shift_range=0.2, # 가로 방향으로 이미지 이동 height_shift_range=0.2, # 세로 방향으로 이미지 이동 shear_range=0.7, # 찌그러짐(이미지 밀림) zoom_range=[0.9, 2.2], # 확대 범위 horizontal_flip=True, # 가로 반전 vertical_flip=True, # 세로 반전 brightness_range=(0.1, 0.9), # 이미지 밝기 fill_mode='nearest') # 이미지를 회전, 이동하거나 축소할 때 생기는 공간을 채우는 방식# fill_mode 에는 아래 4종류가 있으며 nearest를 가장 많이 사용# 'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)# 'nearest': aaaaaaaa|abcd|dddddddd# 'reflect': abcddcba|abcd|dcbaabcd# 'wrap': abcdabcd|abcd|abcdabcd#이미지 불러오기train2_generator = train_datagen.flow_from_directory( './TeamA_name2/train', target_size=(24, 24), batch_size=3, class_mode='categorical') #2배로 증식되었으나 메시지로는 안나옴test2_datagen = ImageDataGenerator(rescale=1./255.)test2_generator=test2_datagen.flow_from_directory('./TeamA_name2/test', target_size=(24,24), batch_size=3, class_mode='categorical')#모델은 기존의 모델과 동일model = Sequential()model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(24,24,3)))model.add(Conv2D(64, (3, 3), activation='relu'))model.add(MaxPooling2D(pool_size=(2, 2)))model.add(Flatten())model.add(Dense(128, activation='relu'))model.add(Dense(4, activation='softmax'))model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics =['accuracy'])#학습(기존과 동일)model.fit( train2_generator, steps_per_epoch=15, epochs=30, validation_data=test2_generator, validation_steps=5)scores = model.evaluate(test2_generator, steps=5)print(&quot;%s: %.2f%%&quot; %(model.metrics_names[1], scores[1]*100))# 66%나옴. 이미지 증식도 알맞게 세팅을 해야할 것 같다.### 따로 이미지 1장으로 평가하고 싶을 경우from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img# 파일 불러오기def load_image(filename): img = load_img(filename, target_size=(24, 24)) img = img_to_array(img) img = img.reshape(1, 24, 24, 3) img = img.astype('float32') return img# 평가 예측 함수def run_example(): img = load_image('file_name.png') #파일 이름 입력 result=model.predict(img) ans = [k for k, v in test_generator.class_indices.items() if v == np.argmax(result[0])][0] print(f'{max(result[0])*100}% 확률로 &quot;{ans}&quot;가 작성한 signature 입니다.')run_example()","link":"/2022/12/15/Study_folder/TensorFlow/2022-12-15-img-generator/"},{"title":"DCGAN(DC 생산적 적대 신경망)","text":"Deep Convolutional GAN(Generative Adversarial Nets) GAN 모델은 ‘생성자 모델 G’(Generator)와 ‘판독자 모델 D’(Discriminator)을 만들고 D와 G의 경찰과 도둑 게임하는 형식으로 이미지 생성하는 방식이다. 그렇기 때문에 2가지 모델을 생성해야 한다. G모델이 이미지를 생성하고 그 이미지의 판독 결과가 0.5(진짜도 가짜도 아닌)될 때까지 학습시켜 이미지 생성 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103from tensorflow.keras.datasets import mnistfrom tensorflow.keras.layers import Input, Dense, Reshape,Flatten, Dropoutfrom tensorflow.keras.layers import BatchNormalization, Activation, LeakyReLU, UpSampling2D, Conv2Dfrom tensorflow.keras.models import Sequential, Modelimport numpy as npimport matplotlib.pyplot as plt##### 모델 G(생성자) #####generator = Sequential()generator.add(Dense(128*7*7 ,input_dim = 100 , activation = LeakyReLU(0.2))) generator.add(BatchNormalization())# 128은 임의의 수를 입력# 7*7은 나중에 *2 and *2를 해서 28을 맞추게 될 기초 값# 28*28로 (mnist 이미지가 28*28이기 때문) # LeakyReLU 뒤의 값은 음수일때의 기울기 값이며 보통 0.1을 입력#upsampling시 이미지 크기가 2배가 됨generator.add(Reshape((7,7,128)))# Conv2D가 받을 수있는 구조로 변경generator.add(UpSampling2D()) # 14,14,128generator.add(Conv2D(64, kernel_size =5, padding='same'))generator.add(BatchNormalization())generator.add(Activation(LeakyReLU(0.2)))generator.add(UpSampling2D())# 28,28,64generator.add(Conv2D(1, kernel_size = 5, padding='same', activation='tanh')) #28,28,1##### 모델 D(판독자) #####discriminator = Sequential()discriminator.add(Conv2D(64, kernel_size=5, strides=2, input_shape=(28,28,1), padding=&quot;same&quot;))discriminator.add(Activation(LeakyReLU(0.2)))discriminator.add(Dropout(0.3))discriminator.add(Conv2D(128, kernel_size=5, strides=2, padding=&quot;same&quot;))discriminator.add(Activation(LeakyReLU(0.2)))discriminator.add(Dropout(0.3))discriminator.add(Flatten())discriminator.add(Dense(1, activation='sigmoid'))discriminator.compile(loss='binary_crossentropy', optimizer='adam')discriminator.trainable = False #판별자, 학습 가능 여부 = False #가장 중요한 문구!######G모델의 input_dim이 100이기에 그에 맞춰야함g_input = Input(shape=(100,))dis_output = discriminator(generator(g_input))gan = Model(g_input, dis_output)gan.compile(loss='binary_crossentropy', optimizer = 'adam')##### 트레인 함수 생성, train_on_batch로 학습 ###### gan_train(2001,32,200)을 사용할 예정def gan_train(epoch, batch_size, saving_interval): (X_train, _), (_, _) = mnist.load_data() #gan에서는 타겟,vailidation이 필요없어서 X_train만 저장 X_train = X_train.reshape(X_train.shape[0],28,28,1).astype('float32') # print(X_train.shape[0]) X_train = (X_train - 127.5) / 127.5 #-1 ~ +1로 스케일링 # real = np.ones((batch_size,1)) #실제 이미지는 모두 참이므로 1로 초기화 fake = np.zeros((batch_size,1)) #32개(batch_size)의 가짜 이미지는 0으로 초기화 for i in range(epoch): #실제 이미지를 판별자에 입력하는 부분 idx = np.random.randint(0,X_train.shape[0], batch_size) imgs = X_train[idx] d_loss_real = discriminator.train_on_batch(imgs, real)#train_on_batch(실제 이미지, 레이블(==1)) #가짜 이미지를 판별자에 입력하는 부분 noise = np.random.normal(0,1, (batch_size, 100)) gen_imgs = generator.predict(noise) d_loss_fake = discriminator.train_on_batch(gen_imgs, fake) #판별자와 생성자의 오차를 계산 d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) #판별자의 오차 g_loss = gan.train_on_batch(noise, real) #생성자의 오차, 진짜라고 속여야되서 real을 줌(생성자에서 레이블을 1(ones)으로 설정해서 판별자로 전달) print(f'epoch: {i}, d_loss:{d_loss:.4f}, g_loss: {g_loss:.4f}') if i % saving_interval == 0: #r, c = 5, 5 noise = np.random.normal(0, 1, (25, 100)) gen_imgs = generator.predict(noise) # Rescale images 0 - 1 gen_imgs = 0.5 * gen_imgs + 0.5 fig, axs = plt.subplots(5, 5) count = 0 for j in range(5): for k in range(5): axs[j, k].imshow(gen_imgs[count, :, :, 0], cmap='gray') axs[j, k].axis('off') count += 1 fig.savefig(f'./gan_mnist_{i}.png')gan_train(2001,32,200) 기존의 이미지가 아닌 가짜의 이미지가 생성됩니다.(숫자지만 숫자 아닌듯한 느낌?)","link":"/2022/12/20/Study_folder/TensorFlow/2022-12-20-DCGAN/"},{"title":"AE(Auto_Encoder)","text":"AE의 원리 히든층을 입력층보다 작은 값을 입력하고, 압축(인코더)과 복원(디코더)을 통해 이미지를 생성 encoder와 decoder로 구성 이 과정에서 만들어야 할 것 (데이터 정제 -&gt; 인코더-&gt; 디코더 -&gt; 컴파일, 학습 -&gt; 테스트) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from tensorflow.keras.datasets import mnistfrom tensorflow.keras.models import Sequential, Modelfrom tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshapeimport tensorflow as tfimport numpy as npimport matplotlib.pyplot as plt(X_train, _), (X_test, _) = mnist.load_data() #mnist데이터를 가지고 작업(x_train으로 학습, x_test로 평가)# 데이터 x,28,28,1 차원으로 변경 후 nomalization 해줌(/255)X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255X_test = X_test.reshape(X_test.shape[0],28,28,1).astype('float32') / 255# 인코더 생성(출력 차원은 임의로 바꿔도 되며, 압축(차원을 줄임) -&gt; 복원(차원을 늘림)이 포인트입니다.)autoencoder = Sequential()autoencoder.add(Conv2D(16, kernel_size = 3, input_shape = X_train.shape[1:], activation='relu', padding = 'same'))# ,28,28,16autoencoder.add(MaxPooling2D(2, padding='same'))# ,14,14,16autoencoder.add(Conv2D(8, kernel_size =3, activation = 'relu', padding = 'same'))# ,14,14,8autoencoder.add(MaxPooling2D(2, padding='same'))# , 7,7,8autoencoder.add(Conv2D(8, kernel_size = 3, strides =2, padding= 'same', activation = 'relu'))# , 4,4,8 (strides가 2이기때문)# 인코더에 바로 디코더를 추가 생성함!# 디코더 생성autoencoder.add(UpSampling2D())# , 8,8,8autoencoder.add(Conv2D(8, kernel_size=3, padding='same', activation='relu'))# , 8,8,8autoencoder.add(UpSampling2D())# ,16,16,8autoencoder.add(Conv2D(16, kernel_size=3, activation='relu'))#no padding option# ,14,14,16autoencoder.add(UpSampling2D())# ,28,28,16autoencoder.add(Conv2D(1, kernel_size = 3, padding='same', activation='sigmoid'))# ,28,28,1# 컴파일 및 학습을 하는 부분입니다.with tf.device(&quot;/GPU:0&quot;): #GPU가 있을 경우 0번 GPU를 사용하여 학습 autoencoder.compile(optimizer='adam', loss='binary_crossentropy') autoencoder.fit(X_train, X_train, epochs=20, batch_size=128, validation_data=(X_test, X_test))# 원본 이미지와 AE로 생성된 이미지 비교random_test = np.random.randint(X_test.shape[0], size=5)ae_imgs = autoencoder.predict(X_test)plt.figure(figsize=(7,2)) #사이즈 설정for i, image_idx in enumerate(random_test): ax = plt.subplot(2,7, i+1) plt.imshow(X_test[image_idx].reshape(28,28)) ax = plt.subplot(2,7, 7+i+1) plt.imshow(ae_imgs[image_idx].reshape(28,28)) 첫번째 줄이 원본 이미지아래줄이 AE로 생성된 이미지","link":"/2022/12/21/Study_folder/TensorFlow/2022-12-21-Auto-encoder(AE)/"},{"title":"tf.cast()","text":"tf.cast(x, dtype, name=None) The operation casts x (in case of Tensor) or x.values (in case of SparseTensor or IndexedSlices) to dtype.해석하자면 x값을 새로운 형태의 dtype으로 캐스팅한다는 의미입니다.{:.prompt-info} 부동 소수점형에서 정수형으로 바꾼 경우 소수점을 버린다. Boolean으로 참조한 경우 True이면 1, False이면 0을 출력한다. 예시를 보면 이해가 될겁니다. 123456789101112x = tf.constant([1.8, 2.2, 3.3], dtype=tf.float32)print(x)# tf.Tensor([1.8 2.2 3.3], shape=(3,), dtype=float32)tf.cast(x, tf.int32)# 출력 결과를 보시면 반올림, 내림이 아닌 소수점을 버립니다.# &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt;tf.cast(x&gt;2, tf.float32)# &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 1., 1.], dtype=float32)&gt;","link":"/2023/01/08/Study_folder/TensorFlow/2023-01-08-cast/"},{"title":"Image Classification fine tuning w ResNetV2","text":"Image Classification fine tuning w ResNetV21234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import os import kerasimport numpy as np import pandas as pdfrom glob import globimport tensorflow as tffrom tensorflow.keras.utils import load_img, img_to_arrayfrom keras.preprocessing.image import ImageDataGeneratorfrom keras.models import Sequential, load_modelfrom keras.layers import GlobalAvgPool2D, Dense, Dropoutfrom keras.callbacks import EarlyStopping, ModelCheckpointfrom tensorflow.keras.applications import ResNet50V2# Data Visualizationimport plotly.express as pximport matplotlib.pyplot as pltfrom zipfile import ZipFilez = ZipFile('/content/drive/MyDrive/lesson_data/animal_data.zip')z.extractall()# Class Names(폴더 이름이 클래스 네임(종류))root_path = './Animal Classification/Animal Classification/Training Data/'test_path = './Animal Classification/Animal Classification/Testing Data/'valid_path = './Animal Classification/Animal Classification/Validation Data/'class_names = sorted(os.listdir(root_path))n_classes = len(class_names)# 이미지 증식(이미지들은 rescale 1/255.를 해줘야함train_gen = ImageDataGenerator(rescale=1/255., rotation_range=10, horizontal_flip=True)valid_gen = ImageDataGenerator(rescale=1/255.)test_gen = ImageDataGenerator(rescale=1/255)# Load Datatrain_ds = train_gen.flow_from_directory(root_path, class_mode='binary', target_size=(256,256), shuffle=True, batch_size=32) valid_ds = valid_gen.flow_from_directory(valid_path, class_mode='binary', target_size=(256,256), shuffle=True, batch_size=32) test_ds = test_gen.flow_from_directory(test_path, class_mode='binary', target_size=(256,256), shuffle=True, batch_size=32)# 0번 GPU를 사용하여 학습with tf.device(&quot;/GPU:0&quot;): ## Pre-Trained Model base_model = ResNet50V2(input_shape=(256,256,3), include_top=False) # include_top은 마지막 레이어 클래스를 예측하는 Dense Layer를 사용하지 않는다는 의미입니다. # 레이어와 이미 학습된 가중치만을 사용한다면 include_top을 False로 해야합니다. base_model.trainable = False # 기존 레이어의 가중치의 학습 안함 # Model Architecture name = 'ResNet50V2'#저장할 체크포인트 파일 이름 #ResNet50V2가 이미 깊게 쌓여진 모델이라 다른 레이어를 많이 만들 필요는 없음 model = Sequential([ base_model, GlobalAvgPool2D(), Dense(256, activation='relu', kernel_initializer='he_normal'), #가중치 초기화(kenel_initailizer에는 3종류가 있는데 여기서는 HE 초기화를 사용) Dropout(0.2), Dense(n_classes, activation='softmax') ], name=name) # Callbacks cbs = [EarlyStopping(patience=3, restore_best_weights=True), ModelCheckpoint(name + &quot;.h5&quot;, save_best_only=True)] # Model opt = tf.keras.optimizers.Adam(learning_rate=2e-3) model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) # Model Training history = model.fit(train_ds, validation_data=valid_ds, callbacks=cbs, epochs=15) 12345678910111213141516171819data = pd.DataFrame(history.history)data# loss accuracy val_loss val_accuracy# 0 0.224692 0.922000 0.142165 0.9544# 1 0.164413 0.943733 0.137304 0.9576# 2 0.126533 0.957733 0.126196 0.9596# 3 0.101600 0.964933 0.119228 0.9636# 4 0.090214 0.970933 0.113578 0.9668# 5 0.078054 0.974133 0.110474 0.9672# 6 0.071814 0.977467 0.105573 0.9668# 7 0.065274 0.981067 0.104021 0.9680# 8 0.065265 0.979867 0.101604 0.9684# 9 0.055805 0.982000 0.098761 0.9704# 10 0.047383 0.986267 0.098213 0.9692# 11 0.048346 0.983867 0.097358 0.9700# 12 0.040546 0.987733 0.097479 0.9700# 13 0.043776 0.987467 0.096769 0.9696# 14 0.034877 0.990000 0.095712 0.9704 12345678910111213141516171819202122232425262728293031323334353637383940model = load_model('./ResNet50V2.h5')model.summary()model.evaluate(test_ds)#0.9559#이미지 경로 불러오고, 이미지 보여주는 함수입니다.def load_image(path): image = tf.cast(tf.image.resize(img_to_array(load_img(path))/255., (256,256)), tf.float32) return imagedef show_image(image, title=None): plt.imshow(image) plt.axis('off') plt.title(title)#경로 설정path = './Animal Classification/Animal Classification/Interesting Images/'interesting_images = [glob(path + name + &quot;/*&quot;) for name in class_names]# 예측 결과(시각화)for name in class_names: plt.figure(figsize=(25, 8)) cat_interesting = interesting_images[class_names.index(name)] for i, i_path in enumerate(cat_interesting): name = i_path.split(&quot;/&quot;)[-1].split(&quot;.&quot;)[0] image = load_image(i_path) plt.subplot(1,len(cat_interesting),i+1) # Model Prediction org_class = name.title() preds = model.predict(image[np.newaxis,...])[0] pred_class = class_names[np.argmax(preds)] confidence_score = np.round(preds[np.argmax(preds)],2) # Configure Title title = f&quot;Pred : {pred_class}\\nConfidence : {confidence_score:.2}&quot; show_image(image, title=title) plt.show()","link":"/2022/12/21/Study_folder/TensorFlow/2022-12-21-img-resnetv2/"},{"title":"텐서플로 VGG16모델을 이용한 파인 튜닝","text":"1234567891011# 이미지넷 대회 : 120만개 이미지 -&gt; 분류기 -&gt; 1000클래스 분류# VGG16이라는 pretrained model을 사용해서 -&gt; 전이학습# VGG16 불러오기 + 우리의 네트워크를 연결 -&gt; fine-tuning# 학습 유형이 2가지 존재.# 첫 번째 유형은 우리가 가지고 있는 데이터로 추가 학습(기존 모델은 그대로 사용) - 대부분 첫 번째 방법 사용# 두 번째 유형은 전체 네트워크를 학습(시간이 엄청나게 소요) - 거의 안함# 전체적인 과정은 다음과 같습니다# 데이터 정제(VGG에서 학습한 사이즈대로) -&gt; VGG호출 -&gt; VGG 포함된 모델 생성 -&gt; 학습 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758from tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2Dfrom tensorflow.keras.preprocessing.image import ImageDataGeneratorfrom tensorflow.keras.callbacks import EarlyStoppingfrom tensorflow.keras import optimizers, Input, models, layers, optimizers, metricsfrom tensorflow.keras.applications import VGG16, EfficientNetB7import numpy as npimport matplotlib.pyplot as plt#train,test 데이터셋 정제train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1)train_generator = train_datagen.flow_from_directory('./train',target_size =(150,150),batch_size=5, class_mode='binary')test_datagen = ImageDataGenerator(rescale=1/255.)test_generator = test_datagen.flow_from_directory('./test',target_size =(150,150),batch_size=5, class_mode='binary')# VGG16이라는 이미지넷에서 사용된 pre_trained 모델 호출#include_top을 True로 할 경우 softmax 1000으로 클래스 분류 그대로 사용한다는 의미 (대부분 추가 학습의 경우 False 사용)transfer_model = VGG16(include_top= False, input_shape=(150,150,3), weights='imagenet')#새롭게 학습하지 않고 이어서 학습하기에 False로 설정transfer_model.trainable=False # transfer_model.summary()#새로운 모델을 설정finetune_model = Sequential()finetune_model.add(transfer_model) #파인튜닝이기때문에 이미 학습된 모델을 추가finetune_model.add(Flatten()) #1차원으로 통일finetune_model.add(Dense(64)) #(None, 64) finetune_model.add(Activation('relu'))finetune_model.add(Dropout(0.5))finetune_model.add(Dense(1)) # (None, 1)finetune_model.add(Activation('sigmoid'))# finetune_model.summary() #컴파일 및 콜백옵션finetune_model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=0.0002), metrics=['accuracy'])early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5)#학습history = finetune_model.fit(train_generator, epochs=20, validation_steps=10, validation_data=test_generator,callbacks=[early_stopping_callback])## epochs당 변화를 시각화 하기y_vloss = history.history['val_loss']y_loss = history.history['loss']x_len = np.arange(len(y_loss))plt.plot(x_len, y_vloss,marker='.', c='red', label='testsetloss')plt.plot(x_len, y_loss,marker='.', c='blue', label='trainsetloss')plt.legend(loc='upper right')plt.grid()plt.xlabel('epoch')plt.ylabel('loss')plt.show() 만약 파인튜닝이 아니고 그냥 학습 없이 pre_trained_data로 평가를 하고 싶은 경우12345678910111213141516171819from tensorflow.keras.preprocessing import imagefrom keras.applications import vgg16#평가할 사진을 불러옴img = image.load_img('flower.jpg', target_size=(224,224))# 평가할 사진을 (x,224,224,3) 차원으로 바꿔줌input_img = np.expand_dims(img, axis = 0) print(np.shape(img)) #(224,224,3)print(np.shape(input_img)) # (1,224,224,3)#VGG16모델을 불러옴model_1 = vgg16.VGG16() # 평가(예측)pred = model_1.predict(input_img)np.argmax(pred) #daisytop_ten = vgg16.decode_predictions(pred, top=10)top_ten # 예측값 상위 10개를 보여줌","link":"/2022/12/22/Study_folder/TensorFlow/2022-12-22-img-finetun/"},{"title":"Pytorch Dataset 클래스(상속) 파악하기!!","text":"BERT_Dataset 데이터를 정제하였다면, 각 데이터가 KoBERT 모델의 입력으로 들어갈 수 있는 형태가 되도록 토큰화, 정수 인코딩, 패딩 등을 해주어야 한다. 아래는 그를 수행할 클래스이다. Dataset을 상속 받는 클래스의 구성 사용자 정의 Dataset 클래스는 반드시 3개 함수를 구현해야 합니다: 1.__init__,2.__len__,3.__getitem__ __init__ 함수는 Dataset 객체가 생성(instantiate)될 때 한 번만 실행됩니다. __len__ 함수는 데이터셋의 샘플 개수를 반환합니다.len()함수를 사용 시 반환값이라고 생각하시면 됩니다. __getitem__ 함수는 클래스의 인덱스에 접근할 때 자동으로 호출되는 메서드(함수)이다.쉽게 표현하면 슬라이싱을 구현하려면 필요한 것은 __getitem__라는 메소드! 1234567891011121314151617181920212223242526272829303132333435363738from torch.utils.data import Datasetimport gluonnlp as nlpclass BERT_Dataset(Dataset): # __init__ 함수는 Dataset 객체가 생성(instantiate)될 때 한 번만 실행됩니다. # 여기서는 dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair)를 불러옵니다. # 함수를 호출하게되면 nlp.data.BERTSentenceTransform을 하고, sentences와 labels의 리스트를 만들게 됩니다. def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair): transform = nlp.data.BERTSentenceTransform( bert_tokenizer, max_seq_length = max_len, pad = pad, pair = pair) self.sentences = [transform([i[sent_idx]]) for i in dataset] self.labels = [np.int32(i[label_idx]) for i in dataset] # 인덱스를 기반으로 문장과 라벨을 반환합니다. return에 슬라이싱 &quot;[]&quot;이 필요함. def __getitem__(self, i): return (self.sentences[i] + (self.labels[i], )) # __len__ 함수는 데이터셋의 레이블 개수를 반환합니다. def __len__(self): return (len(self.labels))#######################################################################사용하기tokenizer = get_tokenizer()token = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)# 이런식으로 만든 함수를 사용합니다.data_train = BERTDataset(dataset = train_set_data, sent_idx = 0, label_idx = 1, bert_tokenizer = token, max_len = 64, pad = True, pair = False)# 아니면 아래와 같은 형식으로 '=' 기호를 빼고 사용하셔도 결과는 같습니다.data_train = BERTDataset(train_set_data, 0, 1, token, max_len, True, False) 코드 분해하여 원리 파악하기1234567891011121314151617181920212223242526272829303132from kobert.utils import get_tokenizerimport gluonnlp as nlptokenizer = get_tokenizer()bertmodel, vocab = get_pytorch_kobert_model()token = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)data_train = BERTDataset(dataset = train_set_data, sent_idx = 0, label_idx = 1, bert_tokenizer = token, max_len = 64, pad = True, pair = False)# data를 리스트[0]에는 문장을 넣고, 리스트[1]에는 감정(labels)으로 넣어서 리스트로 만들어줌.train_set_data = [[i, str(j)] for i, j in zip(train_set['data'], train_set['label'])]print(train_set_data[0])# 출력 결과 =&gt; ['큰아들이 결혼하는데 집을 사달라고 해서 화가 나.', '4']print(data_train[0])# 출력 결과 =&gt;(array([ 2, 4688, 6797, 5940, 950, 7795, 4384, 7088, 2573, 5794, 5439, 5007, 5112, 5330, 1370, 517, 54, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), array(18, dtype=int32), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), 4)# array의 첫 번째는 패딩된 시퀀스, 두 번째는 길이와 타입에 대한 내용, 세 번재는 어텐션 마스크 시퀀스이다. 아래는 from kobert.pytorch_kobert import get_pytorch_kobert_model의 get_kobert_model함수 형식입니다. bertmodel, vocab = get_pytorch_kobert_model()을 실행 시 에러가 나온다면 !pip install sentencepiece==0.1.91 !pip install transformers==4.8.2버전을 맞춰주면 해결이 될 수도 있습니다. 123456789def get_kobert_model(model_path, vocab_file, ctx=&quot;cpu&quot;): bertmodel = BertModel.from_pretrained(model_path) device = torch.device(ctx) bertmodel.to(device) bertmodel.eval() vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(vocab_file, padding_token='[PAD]') return bertmodel, vocab_b_obj KoBERT 함수 출처&lt;KoBERT&gt;","link":"/2022/12/02/Study_folder/Pytorch/2022-12-02-pytorch-Dataset(class)/"},{"title":"XAI(eXplainable AI) - 설명하는 AI","text":"설명 가능한 AI(eXplainable AI) - XAI XAI는 인공지능의 행위와 도출한 결과를 사람이 이해할 수 있는 형태로 이를 설명하는 방법론과 분야를 일컫는다. 흔히 인공지능 기술은 복잡한 일련의 과정(딥러닝)을 통해 결론을 도출하나, 그 과정을 설명할 수 없는 블랙 박스로 여겨진다. XAI는 이를 해소 시킬 수 있는 개념으로 인공지능의 신뢰성을 높이는 역할하고 있습니다. (1) CAM1 - flatten 작업 직전 단계에서 이때까지 만들어진 중간 결과들(feature map)을 수집 2 - 중간 결과들에 대한 평균값을 구함 3 - 평균값과 최종 예측값 사이에서 한번 더 학습 -&gt; 어떤 중간값이 최종 결정에 영향을 크게 줬는지 확인 12345678910111213141516!pip install tf-explainimport zipfilezipfile.ZipFile('img.zip').extractall()from tensorflow.keras.preprocessing.image import load_img, img_to_arrayfrom tensorflow.keras.applications import VGG16from tf_explain.core.grad_cam import GradCAMfrom tf_explain.core.occlusion_sensitivity import OcclusionSensitivityimport globimport matplotlib.pyplot as pltimport matplotlib.image as mpimg 원본 사진 파일 확인 12345678910111213print(glob.glob('*_0.jpg'))# ['yawl_0.jpg', 'squirrel_monkey_0.jpg', 'persian_cat_0.jpg', # 'maltese_0.jpg', 'grand_piano_0.jpg']images_originals = []for img_name in glob.glob(&quot;*_0.jpg&quot;): images_originals.append(mpimg.imread(img_name))plt.figure(figsize = (20,20))for i, img in enumerate(images_originals): plt.subplot(5,5,i+1) plt.imshow(img) 이제 VGG16에서 이미지 분류된 결과를 통해 원본 사진을 왜 카테고리(input_list)로 분류하였는지를 확인하겠습니다. 12345678910111213141516171819model = VGG16(weights=&quot;imagenet&quot;, include_top=True)input_list = [&quot;maltese&quot;, &quot;persian_cat&quot;, &quot;squirrel_monkey&quot;, &quot;grand_piano&quot;, &quot;yawl&quot;]imagenet_index = [&quot;153&quot;, &quot;283&quot;, &quot;382&quot;, &quot;579&quot;, &quot;914&quot;]#gradient CAM 알고리즘으로 XAI 생성explainer = GradCAM()for li, i in zip(input_list, imagenet_index): print(i) img = (load_img(f'{li}_0.jpg', target_size=(224,224))) img = img_to_array(img) data = ([img], None) # print(data) # print('--'*50) grid = explainer.explain(data, model, int(i))# 설명하는 ai 생성 explainer.save(grid, '.', f'./{li}_cam.jpg') #_cam.jpg파일이란 이름으로 저장 저장된 사진을 확인해봅시다. 12345678910111213#gradient CAM 알고리즘이 적용된 이미지를 저장할 리스트 정의images_cams = []plt.figure(figsize=(20,20))for img in glob.glob(&quot;*_cam.jpg&quot;): images_cams.append(mpimg.imread(img))# 출력for i, img in enumerate(images_cams): plt.subplot(5,5,i+1) plt.imshow(img) (2) 이미지를 일부를 가려서, 가려진 일부가 이미지 분류하는데 있어서 어느 정도 영향을 줬는지 계산하는 방식작성 중","link":"/2023/01/10/Study_folder/TensorFlow/2023-01-10-XAI/"},{"title":"Pytorch nn.Module 클래스(상속) 파악하기!!","text":"Pytorch에서 클래스로 모듈 불러오기 파이토치에서 대부분 모델을 생성할 때 클래스(Class)를 사용하고 있습니다. 아래의 예시에서는 nn.Module클래스의 BERTClassifier를 상속 받는 모델입니다. __init__()에서 모델의 구조와 동작을 정의하는 생성자를 정의합니다. 이는 파이썬에서 객체가 갖는 속성값을 초기화하는 역할로, 객체가 생성될 때 자동으호 호출됩니다. super() 함수를 부르면 여기서 만든 클래스는 nn.Module 클래스의 속성들을 가지고 초기화 됩니다. foward() 함수는 모델이 학습데이터를 입력받아서 forward 연산을 진행시키는 함수입니다. 이 forward() 함수는 model 객체를 데이터와 함께 호출하면 자동으로 실행이됩니다. nn.Module클래스에서 그렇게 만들어졌기에 그렇습니다. 123456789101112131415# 간단한 예시from torch import nnimport torch.nn.functional as Fclass MultiLinear(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(3, 1) # input_dim=3, output_dim=1. def forward(self, x): return self.linear(x)######model = MultiLinear()optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) nb_epochs = 200# ... KoBERT 분류기 클래스로 생성 어텐션 마스크(attention mask)는 BERT가 어텐션 연산을 할 때, 불필요하게 패딩 토큰에 대해서 어텐션을 하지 않도록 실제 단어와 패딩 토큰을 구분할 수 있도록 알려주는 입력입니다. 0과 1으로 이루어져있는데, 숫자 0은 해당 토큰은 패딩 토큰이므로 마스킹을 한다는 의미이고 숫자 1은 해당 토큰은 실제 단어이므로 마스킹을 하지 않는다라는 의미입니다. 12345678910111213141516171819202122232425262728293031323334353637383940414243from torch import nn#KoBERT 학습모델 생성class KoBERTClassifier(nn.Module): ## nn.Module 클래스를 상속 def __init__(self, bert, #bert모델 hidden_size = 768, #히든사이즈 num_classes=6, #클래스 수 dr_rate=None): #dropout super(BERTClassifier, self).__init__() #super함수 생성 시 self가 뒤에 와야한다. self.bert = bert self.dr_rate = dr_rate #inputsize, outputsize의 Linearmodel 만드는 함수 self.classifier = nn.Linear(hidden_size , num_classes) #dropout옵션을 사용한다면 아래 함수 생성 if dr_rate: self.dropout = nn.Dropout(p=dr_rate) #어텐션마스크 생성을 위한 함수 def gen_attention_mask(self, token_ids, valid_length): attention_mask = torch.zeros_like(token_ids) for i, v in enumerate(valid_length): attention_mask[i][:v] = 1 return attention_mask.float() def forward(self, token_ids, valid_length, segment_ids): #어텐션마스크 생성 attention_mask = self.gen_attention_mask(token_ids, valid_length) _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device)) #dropout옵션을 사용한다면 아래 함수 생성 if self.dr_rate: out = self.dropout(pooler) return self.classifier(out)######## 사용 예시 ########bertmodel, vocab = get_pytorch_kobert_model()model = KoBERTClassifier(bertmodel, dr_rate=0.5).to(device) 코드 분해하여 원리 파악하기1234567torch.nn.Linear(in_features,out_features,bias = True, device = None,dtype = None)train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)# train_dataloader의 리스트는# [[token_ids], [valid_length], [segment_ids], [label]] 의 순서로 이루어져 있습니다.# for i ,(token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader): 이런식으로 사용.","link":"/2022/12/03/Study_folder/Pytorch/2022-12-03-nn.Module(class)/"},{"title":"Pytorch Dataloader","text":"Dataloader 사용하는 방법 모든 Dataset 으로부터 DataLoader 를 생성할 수 있습니다. PyTorch의 DataLoader 는 배치 관리를 담당합니다. DataLoader란 Dataset을 batch기반의 딥러닝모델 학습을 위해서 미니배치 형태로 만들어서 우리가 실제로 학습할 때 이용할 수 있게 형태를 만들어주는 기능을 합니다. DataLoader를 통해 Dataset의 전체 데이터가 batch size로 slice되어 공급됩니다. dataset을 input으로 넣어주면 여러 옵션(데이터 묶기, 섞기, 알아서 병렬처리)을 통해 batch를 만들어줍니다. DataLoader는 iterator 형식으로 데이터에 접근 하도록 하며 batch_size나 shuffle 유무를 설정할 수 있다. 일반적인 사용 방법123456789101112from torch.utils.data import Dataloaderdataloader1 = Dataloader( dataset, batch_size = 1, shuffle = True,)dataloader2 = DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None) batch_size는 일반적으로 2의 배수를 사용합니다.(컴퓨터의 연산때문에 2의 배수로 해야 속도가 빠름) shuffle 은 Epoch 마다 데이터셋을 섞어, 데이터가 학습되는 순서를 바꾸는 기능을 말합니다. num_worker는 동시에 처리하는 프로세서의 수입니다. PC(특히 윈도우)에서는 default=0로 설정해야 오류가 나지 않습니다. collate_fn 함수는 DataLoader 로부터 생성된 샘플 배치로 동작합니다. collate_fn 의 입력은 DataLoader 에 배치 크기(batch size)가 있는 배치(batch) 데이터이며, collate_fn 은 이를 미리 선언된 데이터 처리 파이프라인에 따라 처리합니다.(아래의 예시) 12345678910# batches가 1이 아닌 경우 이런식으로 세팅하여 DataLoader의 collate_fn에 넣어준다.# 사용자 정의 collate_fn() 함수는 가변 길이 배치를 채우는 데 자주 사용됩니다.def collate_batch(batch): data = [item[0] for item in batch] mask = [item[1] for item in batch] label = [item[2] for item in batch] return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)train_dataloader = DataLoader(train_set, batch_size=32, num_workers=0, shuffle=True, collate_fn=collate_batch)","link":"/2022/12/10/Study_folder/Pytorch/2022-12-10-Dataloader/"},{"title":"맥북(Mac OS) 사용자의 Pytorch GPU를 사용법","text":"GPU사용 일단 아래의 사진을 보면 아시겠지만, 현재 파이토치는 맥OS에 CUDA가 지원되지 않습니다. mps로 GPU를 사용해야 합니다. 그래서 아나콘다 가상환경을 만들어 준 다음(필수는 아님) 123456#생성conda create --name name_of_conda_env#실행conda activate name_of_conda_env#만약 가상환경으로 돌아가고 싶다면conda deactivate 먼저, pytorch 홈페이지 로 들어가서 https://pytorch.org/ conda ...으로 입력된 칸의 명령어를 복사하여 터미널에서 실행해 설치합니다. 확인 및 사용 방법12345678910import torch#여기서 'cuda'가 아닌 'mps'를 사용합니다device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')print (f'PyTorch version:{torch.__version__}') # 1.12.1 이상print(f'MPS 장치를 지원하도록 build 되었는지: {torch.backends.mps.is_built()}') # True 여야 합니다.print(f'MPS 장치가 사용 가능한지: {torch.backends.mps.is_available()}') # True 여야 합니다.!python -c 'import platform;print(platform.platform())'model.to(device) 또는 12mps_device = torch.device('mps')model.to(mps_device)","link":"/2022/12/10/Study_folder/Pytorch/2022-12-10-pytorch-GPU/"},{"title":"파이토치로 nn모듈의 CNN사용하기","text":"파이토치로 CNN 모델 생성(fasion_mnist data)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115from torch.utils.data import Dataset, DataLoaderimport torchvision.transforms as transformsfrom torchvision import datasetsimport torch.nn as nnimport torch.nn.functional as Fimport torchimport torch.optim as optim#ToTensor() - 데이터를 tensor로 바꿔준다.#Normalize(mean, std, inplace=False) - 정규화한다.# Fasion-mnist 데이터 불러오기transform = transforms.Compose([transforms.ToTensor()])trainset = datasets.FashionMNIST(root='/content', train=True, download=True, transform = transform)testset = datasets.FashionMNIST(root='/content', train=False, download=True, transform = transform)#데이터로더로 데이터 정제train_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)test_loader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)#반복문에서 사용할 수 있도록 iter로 만들기images, labels = next(iter(train_loader))images.shape, labels.shape #(torch.Size([64, 1, 28, 28]), torch.Size([64]))#nn.Module 클래스 상속(CNN 모델 만들기)class FashionCNN(nn.Module): def __init__(self): super(FashionCNN, self).__init__() # ImgIn shape=(1, 28, 28, 1) # Conv -&gt; (1, 28, 28, 32) # Pool -&gt; (1, 14, 14, 32) self.layer1 = nn.Sequential( nn.Conv2d(1,32,3, padding=1), #padding을 하여 이미지 크기 손실이 없음 nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2,2) ) # ImgIn shape=(1, 14, 14, 1) # Conv -&gt; (1, 12, 12, 64) # Pool -&gt; (1, 6, 6, 64) self.layer2 = nn.Sequential( nn.Conv2d(32,64,3), #padding옵션이 없어서 이미지 크기가 -2되었음(kenel size=3이기때문에) nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2) ) self.fc1 = nn.Linear(64*6*6, 600) #layer2출력이 64, 이미지 사이즈가 6*6이기 때문에 1차원으로 표현하면 64*6*6 self.drop = nn.Dropout2d(0.25) self.fc2 = nn.Linear(600, 120) self.fc3 = nn.Linear(120, 10) def forward(self, x): out = self.layer1(x) out = self.layer2(out) out = out.view(out.size(0), -1) #64*6*6(1차원으로 변경) out = self.fc1(out) out = self.drop(out) out = self.fc2(out) out = self.fc3(out) return outmodel = FashionCNN()#손실함수, 옵티마이저 정의criterion = nn.CrossEntropyLoss()optimizer = optim.SGD(model.parameters(), lr=0.001,momentum=0.9)#학습 (20epochs)for epoch in range(20): running_loss = 0.0 for i, data in enumerate(train_loader, 0): inputs , labels = data optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() #item() == tensor에 저장된 값만 가져옴(텐서에서 값만 가져옴) if i % 100 == 99: print('Epoch : {}, Iter : {}, Loss : {}'.format(epoch+1,i+1, running_loss/2000)) runngin_loss = 0.0#평가correct = 0total = 0with torch.no_grad(): for data in test_loader: images, labels = data outputs = model(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item()print(100 * correct / total)#90.69#저장PATH = './fashion_mnist.pth'torch.save(model.state_dict(), PATH)#불러오기model = FashionCNN()model.load_state_dict(torch.load(PATH))","link":"/2022/12/12/Study_folder/Pytorch/2022-12-12-nn(CNN)/"},{"title":"파이토치 이미지 증식 방법 차이","text":"torchvision의 transforms와 albumentation의 차이점 보통 torchvision의 transforms에서는 Normalize를 사용할 때 데이터를 min_max scale(0~1)로 만들어주어야 하기 때문에 ToTensor를 먼저 사용한 후에 Normalize를 적용하는데 albumentation의 Normalize는 scaling과 normalize를 동시에 처리한다는 차이점이 있다. albumentations에서는 ToTensor 대신 ToTensorV2를 사용하는데 ToTesorV2는 ToTensor와 마찬가지로 tensor형변환, channel dimension을 첫번째 차원으로 가져오는 역할은 하지만 min_max scaling은 하지 않는다. (추가로 자료형 문제 때문에 albumentations에서 ToTensorV2를 Normalize 보다 앞에서 사용하면 에러가 발생한다.)","link":"/2022/12/15/Study_folder/Pytorch/2022-12-15-dif-tran-alb/"},{"title":"파이토치 함수","text":"ones(),zeros()123456789101112131415161718import torchtorch.zeros(4,4)# tensor([[0., 0., 0., 0.],# [0., 0., 0., 0.],# [0., 0., 0., 0.],# [0., 0., 0., 0.]])tensor = torch.ones(4, 4)# tensor([[1., 1., 1., 1.],# [1., 1., 1., 1.],# [1., 1., 1., 1.],# [1., 1., 1., 1.]])#슬라이싱도 가능tensor[:,1] = 0print(tensor)tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]]) cat() 판다스의 concat()함수와 마찬가지로 리스트로 만들어줘야한다. 123456789tensor = torch.ones(4, 4)tensor[:,1] = 0con_tensor = torch.cat([tensor, tensor, tensor], dim=1)print(con_tensor)# tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],# [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],# [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],# [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]]) argmax() 최대값의 index를 찾아주는 함수 12345678910111213141516171819202122import torch# dim 옵션이 없을 경우(default)a = torch.randn(3,3)argmax = torch.argmax(a)print(a)print(argmax)# tensor([[-0.0626, -1.4816, -0.6745],# [-0.8330, 0.2181, 0.5294],# [ 0.3596, -0.5217, 0.0292]])# tensor(5)#dim option을 줄 경우a = torch.randn(3, 3)argmax_dim_0 = torch.argmax(a, dim = 0)print(a)print(argmax_dim_0)# tensor([[ 2.1830, 0.1679, -0.7654],# [-0.4077, 0.1342, -0.3766],# [-0.9798, 1.2932, -0.4124]])# tensor([0, 2, 1]) max() 최대값을 출력해주는 함수 12345678a = torch.randn(3,3)justmax = torch.max(a)print(a)print(argmax)# tensor([[ 2.1830, 0.1679, -0.7654],# [-0.4077, 0.1342, -0.3766],# [-0.9798, 1.2932, -0.4124]])# tensor(2.1830) item() item()은 값만을 가져오고 싶을 경우 사용한다.(scalars에서만 사용 가능) 12345678tensor = torch.ones(4, 4)agg = tensor.sum()agg_item = agg.item()print(agg,agg_item, type(agg_item))# tensor(16.) # 16.0 # &lt;class 'float'&gt; eval() eval()은 보통 evaluation 과정 전에 사용되는 함수이다. eval() 함수는 evaluation 과정에서 사용하지 않아야 하는 layer들을 알아서 off 시키도록 하는 함수이다. evaluation/validation 과정에선 보통 model.eval()과 torch.no_grad()를 함께 사용한다고 한다. 12345678910model.eval()score = 0for i, (images, labels) in enumerate(val_loader): images = images.to(device) labels = labels.to(device) g_labels = model(images) score += int(torch.max(g_labels, 1)[1][0] == labels[0]) numpy.clip() -CV에서 자주 사용됨1234567numpy.clip(array, min, max) array 내의 값들에 대해서 min 값 보다 작은 값들을 min값으로 바꿔주고 max 값 보다 큰 값들을 max값으로 바꿔주는 함수.","link":"/2022/12/19/Study_folder/Pytorch/2022-12-19-methods-of-torch/"},{"title":"파이토치 기본 정보들","text":"파이토치 기본 함수들 torch.autograd 자동 미분을 위한 함수들이 포함되어져 있습니다. torch.nn 신경망을 구축하기 위한 다양한 데이터 구조나 레이어 등이 정의되어져 있습니다.RNN, LSTM과 같은 레이어, ReLU와 같은 활성화 함수, MSELoss와 같은 손실 함수들이 있습니다. torch.optim 확률적 경사 하강법(Stochastic Gradient Descent, SGD)를 중심으로 한 파라미터 최적화 알고리즘이 구현되어져 있습니다. torch.utils.data Dataset, Dataloader등의 함수가 내장되어 있습니다. torch.onnx ONNX(Open Neural Network Exchange)의 포맷으로 모델을 익스포트(export)할 때 사용합니다. ONNX는 텐서플로같은 다른 딥러닝 프레임워크간의 교류에 필요한 함수입니다. 파이토치 정보들 optimizer.zero_grad()가 필요한 이유 파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시키는 특징이 있어서 값을 초기화 시켜줄 필요가 있음 backward()는 기울기를 계산하는 함수","link":"/2022/12/30/Study_folder/Pytorch/2022-12-30-torch-basic/"},{"title":"파이토치 차원 변경(reshape,view,permute) 비교","text":"3가지 함수의 공통점 reshape, view, permute는 모두 텐서의 형태(차원을 바꾸는) 기능이다. view와 reshape view로 반환된 tensor는 원본 tensor와 기반이 되는 data를 공유한다. 만약 반환된 tensor의 값이 변경된다면, viewed되는 tensor에서 해당하는 값이 변경된다. view와 reshape은 연산 자체는 같고 기존 tensor가 변경되냐 되지 않는지의 차이가 있다.(contiguous) view는 붙어있는 차원 떼어낼 때 사용하면 용이하다. [A*2, B, C] -&gt; [A, 2, B, C] view는 contiguous한 함수에서만 사용 가능하다. reshape은 contiguous 하지 않는 함수에서도 작동한다. permute(input, dims) permute의 parameters에는 input=tensor, dims = index값을 입력한다. numpy의 transpose는 두 개의 차원을 맞교환할 수 있다. 그러나 permute()는 모든 차원들을 변경할 수 있다. 12345678910111213141516171819x = torch.randn(2, 3, 5)print(x)# tensor([[[ 0.5676, 0.4593, -0.2802, -1.9395, -1.2933],# [-0.6459, -1.0244, -0.3164, 1.5379, 0.9390],# [-0.2366, -1.4711, -0.6014, -0.4424, 0.1915]],# [[-1.3794, -0.4103, 0.1862, 0.2172, 0.3282],# [-1.9206, 1.1295, -0.4130, -0.5630, 0.9670],# [-1.0632, 0.7054, 0.4358, -0.4522, -0.3983]]])print(x.size())# torch.Size([2, 3, 5])torch.permute(x, (2, 0, 1)).size() #== x.permute(2, 0, 1)# torch.Size([5, 2, 3])# transpose는 두 개의 차원을 맞교환x.tranpose(0, 3) # 0과 3의 차원을 변경(교환) contiguous란 사전적 의미는 ‘접촉하는’ 이다. contiguous 란 matrix 의 눈에 보이는 형태(shape) 과 실제 matrix 의 각 데이터가 저장된 위치가 같은지의 여부이다. unsqueeze, squeeze batch가 1일 때 batch차원도 없애버릴 수 있기때문에 이 경우를 유의해서 사용해야한다. 1차원을 생성 및 제거를 할 경우 unsqueeze, squeeze함수가 편하다. Summary view : tensor 에 저장된 데이터의 물리적 위치 순서와 index 순서가 일치할 때 (contiguous) shape을 재구성한다. 이 때문에 항상 contiguous 하다는 성질이 보유된다. reshape : 순서가 일치하지 않아도 shape을 재구성한 이후에 강제로 일치시킨다. 이 때문에 항상 contiguous 하다는 성질이 보유된다. transpose : tensor 의 index 순서는 같다는 보장이 없으므로 항상 contiguous 하지 않다. permute : contiguous 하지 않다. transpose와 permute는 transpose.contiguous()이런식으로 사용 가능하다. einsum : 여러 연산을 통해 차원 관리를 쉽게 할 수 있다.","link":"/2023/01/02/Study_folder/Pytorch/2023-01-02-dim-shape/"},{"title":"Squeeze, Unsqueeze","text":"squeeze squeeze와 unsqueeze는 1인 차원을 제거, 생성할때 매우 유용한 함수이다. squeeze는 차원이 1인 차원을 제거해준다.(default값) 차원을 설정해주면 그 차원만 제거한다.(1인 차원만 제거되니 참고) 123456789101112131415import torchx = torch.rand(3, 20, 1, 1)print(x.shape)# torch.Size([3, 20, 1, 1])x = x.squeeze()print(x.shape)# torch.Size([3, 20])x = torch.squeeze(x, 1)print(x.shape)# torch.Size([3, 20]) unsqueeze unsqueeze는 차원이 1인 차원을 생성해준다. 12345678910111213141516171819202122232425262728x = torch.rand(3, 20, 30, 40)print(x.shape)# torch.Size([3, 20, 30, 40])x = x.unsqueeze(dim=1)print(x.shape)# torch.Size([3, 1, 20, 30, 40])x = torch.rand(3, 20, 30, 40)print(x.shape)# torch.Size([3, 20, 30, 40])x = torch.unsqueeze(x,1)print(x.shape)# torch.Size([3, 1, 20, 30, 40])x = torch.rand(3, 20, 30, 40)print(x.shape)# torch.Size([3, 20, 30, 40])x = x.unsqueeze(dim = 0)print(x.shape)# torch.Size([1, 3, 20, 30, 40])","link":"/2023/01/06/Study_folder/Pytorch/2023-01-06-squeeze/"},{"title":"nn.Embedding","text":"nn.Embedding 출처 : &lt;위키독스&gt; 임베딩 층(embedding layer)을 만들어 훈련 데이터로부터 처음부터 임베딩 벡터를 학습하는 방법을 nn.Embedding을 이용하여 구현합니다. 주요 파라미터는 2개입니다. num_embeddings : 임베딩을 할 단어들의 개수. (단어 집합의 크기) embedding_dim : 임베딩 할 벡터의 차원입니다. (사용자 정의) 1234567891011121314import torch.nn as nntrain_data = 'we can do lots of things like climbing do'# 중복을 제거한 단어들의 집합인 단어 집합 생성.(num_embeddings 인자)word_set = set(train_data.split())# 단어 집합의 각 단어에 고유한 정수 맵핑.vocab = {tkn: i+2 for i, tkn in enumerate(word_set)}vocab['&lt;unk&gt;'] = 0vocab['&lt;pad&gt;'] = 1print(vocab)# {'things': 2, 'lots': 3, 'can': 4, 'like': 5, 'do': 6, 'climbing': 7, 'of': 8, 'we': 9, '&lt;unk&gt;': 0, '&lt;pad&gt;': 1} 1234567891011121314151617181920212223# 단어 집합의 크기의 행을 가지는 임베딩 테이블 생성import torch.nn as nnembedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=3)embedding_layer# Embedding(10, 3, padding_idx=1)```````pythonprint(embedding_layer.weight)Parameter containing:tensor([[ 0.2915, 0.8197, 0.1080], [ 0.4103, 1.2429, -0.7658], [ 0.4185, -0.0410, 2.1945], [-0.9706, -0.6196, -1.3778], [-1.8044, -0.8070, -1.0277], [ 0.7752, -0.1011, 1.5459], [ 0.2195, 1.2008, 0.1253], [ 0.6568, 1.3255, 0.5347], [-1.2790, 0.3015, -0.2819], [-0.0371, -0.0291, -0.2894]], requires_grad=True)","link":"/2023/01/07/Study_folder/Pytorch/2023-01-07-nn.Embedding/"},{"title":"torch.tril","text":"torch.tril torch.tril(input, diagonal=0, *, out=None) 행렬의 아래쪽 삼각형 부분 (2 차원 텐서) 또는 행렬의 배치 input 을 반환합니다 .[행렬의 오른쪽 부분을(0으로 만듬)] attention 구조의 mask를 만들 때 많이 사용되는 함수입니다. 무슨 말인지 이해가 잘 안가실겁니다. 예제 출력 코드를 보면 바로 이해가 갈겁니다. 12345678910111213141516171819a = torch.ones((5, 5))torch.tril(a)tensor([[1., 0., 0., 0., 0.], [1., 1., 0., 0., 0.], [1., 1., 1., 0., 0.], [1., 1., 1., 1., 0.], [1., 1., 1., 1., 1.]])a = torch.ones((5, 5))torch.tril(a, diagonal=1)tensor([[1., 1., 0., 0., 0.], [1., 1., 1., 0., 0.], [1., 1., 1., 1., 0.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]])","link":"/2023/01/07/Study_folder/Pytorch/2023-01-07-tril/"},{"title":"torch를 boolean값으로 변경","text":"operation 참조 예시를 통해 알아보겠습니다. 12345678910import torchx=torch.tensor([1,2,3,4])print(x&gt;2)# tensor([False, False, True, True])print((x&gt;2).type(torch.float32))# tensor([0., 0., 1., 1.]) gt() 함수를 활용12345678910import torchx=torch.tensor([1,2,3,4])print(x.gt(2))# tensor([False, False, True, True])print(x.gt(2).to(torch.int32))# tensor([0, 0, 1, 1], dtype=torch.int32)","link":"/2023/01/08/Study_folder/Pytorch/2023-01-08-boolean/"},{"title":"파이토치에서 model 정보(summary) 확인","text":"파이토치에서 만들어진 모델 정보 확인하기 keras에서는 model.summary()의 내장 함수를 이용하면 간단하게 모델 정보를 확인 가능합니다. 파이토치에서도 여러 가지 방법을 통해 정보를 확인할 수 있습니다. 1. print를 하면 summary가 출력된다1print(model) 2. torchinfo 통해 summary를 출력한다1234!pip install torchinfofrom torchinfo import summarysummary(model) 3. torchsummary를 통해 summary출력 (input_size를 알아야만 출력 가능)1234!pip install torchsummaryfrom torchsummary import summarysummary(model, input_size = (1,28,28), batch_size= 6)","link":"/2023/01/09/Study_folder/Pytorch/2023-01-09-model-info/"}],"tags":[{"name":"Blog","slug":"Blog","link":"/tags/Blog/"},{"name":"Colab","slug":"Colab","link":"/tags/Colab/"},{"name":"Kaggle","slug":"Kaggle","link":"/tags/Kaggle/"},{"name":"Errors,Logs","slug":"Errors-Logs","link":"/tags/Errors-Logs/"},{"name":"Review,Dacon","slug":"Review-Dacon","link":"/tags/Review-Dacon/"},{"name":"Samples","slug":"Samples","link":"/tags/Samples/"},{"name":"Project","slug":"Project","link":"/tags/Project/"},{"name":"Project,Portfolio","slug":"Project-Portfolio","link":"/tags/Project-Portfolio/"},{"name":"Command,Shell,Terminal","slug":"Command-Shell-Terminal","link":"/tags/Command-Shell-Terminal/"},{"name":"Command,Jupyternotebook","slug":"Command-Jupyternotebook","link":"/tags/Command-Jupyternotebook/"},{"name":"Command,Terminal,Shell","slug":"Command-Terminal-Shell","link":"/tags/Command-Terminal-Shell/"},{"name":"Shell,Study","slug":"Shell-Study","link":"/tags/Shell-Study/"},{"name":"Mac","slug":"Mac","link":"/tags/Mac/"},{"name":"Study","slug":"Study","link":"/tags/Study/"},{"name":"Markdown","slug":"Markdown","link":"/tags/Markdown/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"Command","slug":"Command","link":"/tags/Command/"},{"name":"Errors","slug":"Errors","link":"/tags/Errors/"},{"name":"Logs","slug":"Logs","link":"/tags/Logs/"},{"name":"Icarus","slug":"Icarus","link":"/tags/Icarus/"},{"name":"Study,Pytorch","slug":"Study-Pytorch","link":"/tags/Study-Pytorch/"},{"name":"Study,Review","slug":"Study-Review","link":"/tags/Study-Review/"},{"name":"Review,Study","slug":"Review-Study","link":"/tags/Review-Study/"},{"name":"Deeplearning,Study","slug":"Deeplearning-Study","link":"/tags/Deeplearning-Study/"},{"name":"Deeplearning,Study,Tensorflow","slug":"Deeplearning-Study-Tensorflow","link":"/tags/Deeplearning-Study-Tensorflow/"},{"name":"sklean,Study","slug":"sklean-Study","link":"/tags/sklean-Study/"},{"name":"Study,Preprocessing","slug":"Study-Preprocessing","link":"/tags/Study-Preprocessing/"},{"name":"pandas,Study","slug":"pandas-Study","link":"/tags/pandas-Study/"},{"name":"Study,Python","slug":"Study-Python","link":"/tags/Study-Python/"},{"name":"NLP,Study","slug":"NLP-Study","link":"/tags/NLP-Study/"},{"name":"OpenCV","slug":"OpenCV","link":"/tags/OpenCV/"},{"name":"Study,OpenCV","slug":"Study-OpenCV","link":"/tags/Study-OpenCV/"},{"name":"Webcam","slug":"Webcam","link":"/tags/Webcam/"},{"name":"Deeplearning,Study,Pytorch","slug":"Deeplearning-Study-Pytorch","link":"/tags/Deeplearning-Study-Pytorch/"},{"name":"Pytorch,Study","slug":"Pytorch-Study","link":"/tags/Pytorch-Study/"}],"categories":[{"name":"Blog","slug":"Blog","link":"/categories/Blog/"},{"name":"Colab","slug":"Colab","link":"/categories/Colab/"},{"name":"Errors","slug":"Errors","link":"/categories/Errors/"},{"name":"Review","slug":"Review","link":"/categories/Review/"},{"name":"Samples","slug":"Samples","link":"/categories/Samples/"},{"name":"Project","slug":"Project","link":"/categories/Project/"},{"name":"Portfolio","slug":"Portfolio","link":"/categories/Portfolio/"},{"name":"Command","slug":"Command","link":"/categories/Command/"},{"name":"Study","slug":"Study","link":"/categories/Study/"},{"name":"Mac","slug":"Mac","link":"/categories/Mac/"},{"name":"Git","slug":"Git","link":"/categories/Git/"},{"name":"OpenCV","slug":"OpenCV","link":"/categories/OpenCV/"}],"pages":[{"title":"About me","text":"Hi there, I’m Inhwan Cho 👋📚 my STACKS📚 &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}]}