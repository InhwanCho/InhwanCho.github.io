<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Inhwan&#039;s Digital Space</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Inhwan&#039;s Digital Space"><meta name="msapplication-TileImage" content="/img/favicon-32x32.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Inhwan&#039;s Digital Space"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="I&amp;#39;m Learning ML&amp;#x2F;DL"><meta property="og:type" content="blog"><meta property="og:title" content="Inhwan&#039;s Digital Space"><meta property="og:url" content="http://inhwancho.github.io/"><meta property="og:site_name" content="Inhwan&#039;s Digital Space"><meta property="og:description" content="I&amp;#39;m Learning ML&amp;#x2F;DL"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="http://inhwancho.github.io/img/og_image.png"><meta property="article:author" content="InhwanCho"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://inhwancho.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://InhwanCho.github.io"},"headline":"Inhwan's Digital Space","image":["http://inhwancho.github.io/img/og_image.png"],"author":{"@type":"Person","name":"InhwanCho"},"publisher":{"@type":"Organization","name":"Inhwan's Digital Space","logo":{"@type":"ImageObject","url":"http://inhwancho.github.io/img/yo_img.jpg"}},"description":"I&#39;m Learning ML&#x2F;DL"}</script><link rel="icon" href="/img/favicon-32x32.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-dark.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-8RGKYVDD5B" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-8RGKYVDD5B');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/rss2.xml" title="Inhwan's Digital Space" type="application/rss+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/yo_img.jpg" alt="Inhwan&#039;s Digital Space" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="external nofollow noopener noreferrer" title="Download on GitHub" href="https://github.com/InhwanCho"><i class="fab fa-github"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2022-12-02T15:00:00.000Z" title="2022. 12. 3. 오전 12:00:00">2022-12-03</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2022-12-13T15:00:00.000Z" title="2022. 12. 14. 오전 12:00:00">2022-12-14</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">4분안에 읽기 (약 554 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/12/03/Study_folder/Basic_study/2022-12-03-ZipFile/">파이썬에서 ZipFile 열기</a></h1><div class="content"><h2 id="from-zipfile-import-ZipFile로-압축-해제하지-않고-사용하기"><a href="#from-zipfile-import-ZipFile로-압축-해제하지-않고-사용하기" class="headerlink" title="from zipfile import ZipFile로 압축 해제하지 않고 사용하기"></a>from zipfile import ZipFile로 압축 해제하지 않고 사용하기</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> zipfile <span class="keyword">import</span> ZipFile</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># zip파일 열기(압축 해제하지않고 사용)</span></span><br><span class="line">z = ZipFile(<span class="string">&#x27;sentiment labelled sentences.zip&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 어떤 리스트가 있는지 확인</span></span><br><span class="line">z.namelist()</span><br><span class="line"><span class="comment"># 출력 결과</span></span><br><span class="line"><span class="comment"># [&#x27;sentiment labelled sentences/amazon_cells_labelled.txt&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># zip파일의 데이터를 불러 읽어옴.</span></span><br><span class="line"><span class="comment"># 편집 작업 시 아래와 같이 `pd.read_csv` 같은 함수로 변경(옮겨)하여 사용</span></span><br><span class="line">data = z.<span class="built_in">open</span>(<span class="string">&#x27;sentiment labelled sentences/amazon_cells_labelled.txt&#x27;</span>)</span><br><span class="line">df = pd.read_csv(data, sep=<span class="string">&quot;\t&quot;</span>, header=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 자세한 정보 확인</span></span><br><span class="line">zip_info = my_zip.getinfo(<span class="string">&#x27;sentiment labelled sentences/amazon_cells_labelled.txt&#x27;</span>)  </span><br><span class="line"><span class="built_in">print</span>(zip_info.filename) <span class="comment">#파일 이름</span></span><br><span class="line"><span class="built_in">print</span>(zip_info.file_size)<span class="comment">#파일 사이즈</span></span><br><span class="line"><span class="built_in">print</span>(zip_info.date_time)<span class="comment">#작성일</span></span><br></pre></td></tr></table></figure>

<h2 id="zip파일을-경로에-실제로-압축-해제하기"><a href="#zip파일을-경로에-실제로-압축-해제하기" class="headerlink" title="zip파일을 경로에 실제로 압축 해제하기"></a>zip파일을 경로에 실제로 압축 해제하기</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">!unzip test.<span class="built_in">zip</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -d옵션으로 경로 지정 가능(폴더 자동 생성)</span></span><br><span class="line">%unzip test.<span class="built_in">zip</span> -d test_folder/</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> zipfile <span class="keyword">import</span> ZipFile</span><br><span class="line"></span><br><span class="line"><span class="comment">#특정 파일만 압축 해제할 경우 사용(리스트 for문이라 시간이 꽤 걸림)</span></span><br><span class="line">directory = <span class="string">&#x27;./&#x27;</span> <span class="comment">#현재 경로</span></span><br><span class="line"><span class="keyword">with</span> ZipFile(<span class="string">&#x27;TeamA_name2.zip&#x27;</span>) <span class="keyword">as</span> f :</span><br><span class="line">  x = [f.extract(file, directory) <span class="keyword">for</span> file <span class="keyword">in</span> f.namelist() <span class="keyword">if</span> file.endswith(<span class="string">&#x27;jpg&#x27;</span>)] <span class="comment">#특정 확장자만 압축 해제하기(endwith(&#x27;jpg&#x27;))</span></span><br><span class="line"><span class="comment">#x는 아무 변수나 줘도 됩니다.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 전체 압축 해제를 원하는 경우(엄청 빠름)</span></span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line">zip_file=zipfile.ZipFile(<span class="string">&#x27;TeamA_name2.zip&#x27;</span>)<span class="comment">#파일 이름</span></span><br><span class="line">zip_file.extractall(path=<span class="string">&#x27;/content/temtem&#x27;</span>)<span class="comment">#압축 해제 경로, default(path=None)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="외부-경로를-통해-zip파일을-다운-받고-압축-해제하기-케라스-데이터셋에-저장"><a href="#외부-경로를-통해-zip파일을-다운-받고-압축-해제하기-케라스-데이터셋에-저장" class="headerlink" title="외부 경로를 통해 zip파일을 다운 받고 압축 해제하기(케라스 데이터셋에 저장)"></a>외부 경로를 통해 zip파일을 다운 받고 압축 해제하기(케라스 데이터셋에 저장)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Download the movielens data from website url</span></span><br><span class="line"><span class="keyword">import</span> tensorflow.keras <span class="keyword">as</span> keras</span><br><span class="line"><span class="keyword">from</span> zipfile <span class="keyword">import</span> ZipFile</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment">#zip파일이 있는 url</span></span><br><span class="line">zipped_url = (</span><br><span class="line">    <span class="string">&#x27;http://files.grouplens.org/datasets/movielens/ml-latest-small.zip&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#zip_file</span></span><br><span class="line">zipped_file = keras.utils.get_file(</span><br><span class="line">    <span class="string">&#x27;zipped_dataset.zip&#x27;</span>, zipped_url, extract=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">keras_path = Path(zipped_file).parents[<span class="number">0</span>] <span class="comment">#전의 경로 설정</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;datasets path:&#x27;</span>, keras_path)       <span class="comment">#데이터들이 케라스 폴더에 저장됩니다.</span></span><br><span class="line"><span class="comment"># datasets path: /root/.keras/datasets</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(os.listdir(keras_path))</span><br><span class="line"><span class="comment">#[&#x27;ml-latest-small&#x27;, &#x27;zipped_dataset.zip&#x27;]</span></span><br><span class="line"></span><br><span class="line">movielens_dir = keras_path / os.listdir(keras_path)[<span class="number">0</span>] <span class="comment">#keras_path의 파일 목록 중 첫번째 목록을 임의로 지정</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#런타임 1회만 사용 가능</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> movielens_dir.exists(): </span><br><span class="line">    <span class="keyword">with</span> ZipFile(zipped_file, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> <span class="built_in">zip</span>:</span><br><span class="line">        <span class="built_in">zip</span>.extractall(path=keras_path) <span class="comment">#압축 해제</span></span><br><span class="line"><span class="built_in">print</span>(os.listdir(movielens_dir))</span><br><span class="line"><span class="comment">#[&#x27;tags.csv&#x27;, &#x27;movies.csv&#x27;, &#x27;README.txt&#x27;, &#x27;links.csv&#x27;, &#x27;ratings.csv&#x27;]</span></span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2022-12-02T15:00:00.000Z" title="2022. 12. 3. 오전 12:00:00">2022-12-03</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2022-12-03T15:00:00.000Z" title="2022. 12. 4. 오전 12:00:00">2022-12-04</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">5분안에 읽기 (약 785 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/12/03/Study_folder/Pytorch/2022-12-03-nn.Module(class)/">Pytorch nn.Module 클래스(상속) 파악하기!!</a></h1><div class="content"><h2 id="Pytorch에서-클래스로-모듈-불러오기"><a href="#Pytorch에서-클래스로-모듈-불러오기" class="headerlink" title="Pytorch에서 클래스로 모듈 불러오기"></a>Pytorch에서 클래스로 모듈 불러오기</h2><ul>
<li>파이토치에서 대부분 모델을 생성할 때 클래스(Class)를 사용하고 있습니다.<ul>
<li>아래의 예시에서는 <code>nn.Module</code>클래스의 <code>BERTClassifier</code>를 상속 받는 모델입니다.</li>
<li><code>__init__()</code>에서 모델의 구조와 동작을 정의하는 생성자를 정의합니다. 이는 파이썬에서 객체가 갖는 속성값을 초기화하는 역할로, 객체가 생성될 때 자동으호 호출됩니다. <code>super()</code> 함수를 부르면 여기서 만든 클래스는 nn.Module 클래스의 속성들을 가지고 초기화 됩니다. <code>foward()</code> 함수는 모델이 학습데이터를 입력받아서 forward 연산을 진행시키는 함수입니다. 이 <code>forward()</code> 함수는 model 객체를 데이터와 함께 호출하면 자동으로 실행이됩니다. <code>nn.Module</code>클래스에서 그렇게 만들어졌기에 그렇습니다.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 간단한 예시</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="number">3</span>, <span class="number">1</span>) <span class="comment"># input_dim=3, output_dim=1.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(x)</span><br><span class="line"><span class="comment">######</span></span><br><span class="line">model = MultiLinear()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-5</span>) </span><br><span class="line">nb_epochs = <span class="number">200</span></span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure>

<h2 id="KoBERT-분류기-클래스로-생성"><a href="#KoBERT-분류기-클래스로-생성" class="headerlink" title="KoBERT 분류기 클래스로 생성"></a>KoBERT 분류기 클래스로 생성</h2><ul>
<li><code>어텐션 마스크(attention mask)</code>는 BERT가 어텐션 연산을 할 때, 불필요하게 패딩 토큰에 대해서 어텐션을 하지 않도록 실제 단어와 패딩 토큰을 구분할 수 있도록 알려주는 입력입니다. 0과 1으로 이루어져있는데, 숫자 0은 해당 토큰은 패딩 토큰이므로 마스킹을 한다는 의미이고 숫자 1은 해당 토큰은 실제 단어이므로 마스킹을 하지 않는다라는 의미입니다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#KoBERT 학습모델 생성</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">KoBERTClassifier</span>(nn.Module):    <span class="comment">## nn.Module 클래스를 상속</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 bert,              <span class="comment">#bert모델</span></span></span><br><span class="line"><span class="params">                 hidden_size = <span class="number">768</span>, <span class="comment">#히든사이즈</span></span></span><br><span class="line"><span class="params">                 num_classes=<span class="number">6</span>,     <span class="comment">#클래스 수</span></span></span><br><span class="line"><span class="params">                 dr_rate=<span class="literal">None</span></span>):     <span class="comment">#dropout</span></span><br><span class="line">        <span class="built_in">super</span>(BERTClassifier, self).__init__() <span class="comment">#super함수 생성 시 self가 뒤에 와야한다.</span></span><br><span class="line">        self.bert = bert</span><br><span class="line">        self.dr_rate = dr_rate</span><br><span class="line"></span><br><span class="line">        <span class="comment">#inputsize, outputsize의 Linearmodel 만드는 함수     </span></span><br><span class="line">        self.classifier = nn.Linear(hidden_size , num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#dropout옵션을 사용한다면 아래 함수 생성</span></span><br><span class="line">        <span class="keyword">if</span> dr_rate:</span><br><span class="line">            self.dropout = nn.Dropout(p=dr_rate)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#어텐션마스크 생성을 위한 함수</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">gen_attention_mask</span>(<span class="params">self, token_ids, valid_length</span>):</span><br><span class="line">        attention_mask = torch.zeros_like(token_ids)</span><br><span class="line">        <span class="keyword">for</span> i, v <span class="keyword">in</span> <span class="built_in">enumerate</span>(valid_length):</span><br><span class="line">            attention_mask[i][:v] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> attention_mask.<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, token_ids, valid_length, segment_ids</span>):</span><br><span class="line">        <span class="comment">#어텐션마스크 생성</span></span><br><span class="line">        attention_mask = self.gen_attention_mask(token_ids, valid_length)</span><br><span class="line">        </span><br><span class="line">        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.<span class="built_in">float</span>().to(token_ids.device))</span><br><span class="line"></span><br><span class="line">        <span class="comment">#dropout옵션을 사용한다면 아래 함수 생성</span></span><br><span class="line">        <span class="keyword">if</span> self.dr_rate:</span><br><span class="line">            out = self.dropout(pooler)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.classifier(out)</span><br><span class="line"></span><br><span class="line"><span class="comment">######## 사용 예시 ########</span></span><br><span class="line">bertmodel, vocab = get_pytorch_kobert_model()</span><br><span class="line"></span><br><span class="line">model = KoBERTClassifier(bertmodel,  dr_rate=<span class="number">0.5</span>).to(device)</span><br></pre></td></tr></table></figure>

<h2 id="코드-분해하여-원리-파악하기"><a href="#코드-분해하여-원리-파악하기" class="headerlink" title="코드 분해하여 원리 파악하기"></a>코드 분해하여 원리 파악하기</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Linear(in_features,out_features,bias = <span class="literal">True</span>, device = <span class="literal">None</span>,dtype = <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train_dataloader의 리스트는</span></span><br><span class="line"><span class="comment"># [[token_ids], [valid_length], [segment_ids], [label]] 의 순서로 이루어져 있습니다.</span></span><br><span class="line"><span class="comment"># for i ,(token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader): 이런식으로 사용.</span></span><br></pre></td></tr></table></figure></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2022-12-01T15:00:00.000Z" title="2022. 12. 2. 오전 12:00:00">2022-12-02</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2022-12-04T15:00:00.000Z" title="2022. 12. 5. 오전 12:00:00">2022-12-05</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">6분안에 읽기 (약 848 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/12/02/Study_folder/Pytorch/2022-12-02-pytorch-Dataset(class)/">Pytorch Dataset 클래스(상속) 파악하기!!</a></h1><div class="content"><h2 id="BERT-Dataset"><a href="#BERT-Dataset" class="headerlink" title="BERT_Dataset"></a>BERT_Dataset</h2><ul>
<li>데이터를 정제하였다면, 각 데이터가 KoBERT 모델의 입력으로 들어갈 수 있는 형태가 되도록 토큰화, 정수 인코딩, 패딩 등을 해주어야 한다. 아래는 그를 수행할 클래스이다.</li>
<li>Dataset을 상속 받는 클래스의 구성</li>
<li>사용자 정의 Dataset 클래스는 반드시 3개 함수를 구현해야 합니다: <br><br>1.<code>__init__</code>,<br>2.<code>__len__</code>,<br>3.<code>__getitem__</code></li>
<li><code>__init__</code> 함수는 Dataset 객체가 생성(instantiate)될 때 한 번만 실행됩니다. </li>
<li><code>__len__</code> 함수는 데이터셋의 샘플 개수를 반환합니다.<br>len()함수를 사용 시 반환값이라고 생각하시면 됩니다.</li>
<li><code>__getitem__</code> 함수는 클래스의 인덱스에 접근할 때 자동으로 호출되는 메서드(함수)이다.<br>쉽게 표현하면 슬라이싱을 구현하려면 필요한 것은 <code>__getitem__</code>라는 메소드!</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> gluonnlp <span class="keyword">as</span> nlp</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BERT_Dataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># __init__ 함수는 Dataset 객체가 생성(instantiate)될 때 한 번만 실행됩니다. </span></span><br><span class="line">    <span class="comment"># 여기서는 dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair)를 불러옵니다.</span></span><br><span class="line">    <span class="comment"># 함수를 호출하게되면 nlp.data.BERTSentenceTransform을 하고, sentences와 labels의 리스트를 만들게 됩니다.</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,</span></span><br><span class="line"><span class="params">                 pad, pair</span>):</span><br><span class="line">        transform = nlp.data.BERTSentenceTransform(</span><br><span class="line">            bert_tokenizer, max_seq_length = max_len, pad = pad, pair = pair)</span><br><span class="line"></span><br><span class="line">        self.sentences = [transform([i[sent_idx]]) <span class="keyword">for</span> i <span class="keyword">in</span> dataset]</span><br><span class="line">        self.labels = [np.int32(i[label_idx]) <span class="keyword">for</span> i <span class="keyword">in</span> dataset]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 인덱스를 기반으로 문장과 라벨을 반환합니다. return에 슬라이싱 &quot;[]&quot;이 필요함.</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>):</span><br><span class="line">        <span class="keyword">return</span> (self.sentences[i] + (self.labels[i], ))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># __len__ 함수는 데이터셋의 레이블 개수를 반환합니다.</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> (<span class="built_in">len</span>(self.labels))</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#사용하기</span></span><br><span class="line">tokenizer = get_tokenizer()</span><br><span class="line">token = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 이런식으로 만든 함수를 사용합니다.</span></span><br><span class="line">data_train = BERTDataset(dataset = train_set_data, sent_idx = <span class="number">0</span>, label_idx = <span class="number">1</span>, bert_tokenizer = token, max_len = <span class="number">64</span>, pad = <span class="literal">True</span>, pair = <span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 아니면 아래와 같은 형식으로 &#x27;=&#x27; 기호를 빼고 사용하셔도 결과는 같습니다.</span></span><br><span class="line">data_train = BERTDataset(train_set_data, <span class="number">0</span>, <span class="number">1</span>, token, max_len, <span class="literal">True</span>, <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h2 id="코드-분해하여-원리-파악하기"><a href="#코드-분해하여-원리-파악하기" class="headerlink" title="코드 분해하여 원리 파악하기"></a>코드 분해하여 원리 파악하기</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> kobert.utils <span class="keyword">import</span> get_tokenizer</span><br><span class="line"><span class="keyword">import</span> gluonnlp <span class="keyword">as</span> nlp</span><br><span class="line">tokenizer = get_tokenizer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bertmodel, vocab = get_pytorch_kobert_model()</span><br><span class="line">token = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">data_train = BERTDataset(dataset = train_set_data, sent_idx = <span class="number">0</span>, label_idx = <span class="number">1</span>, bert_tokenizer = token, max_len = <span class="number">64</span>, pad = <span class="literal">True</span>, pair = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># data를 리스트[0]에는 문장을 넣고, 리스트[1]에는 감정(labels)으로 넣어서 리스트로 만들어줌.</span></span><br><span class="line">train_set_data = [[i, <span class="built_in">str</span>(j)] <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">zip</span>(train_set[<span class="string">&#x27;data&#x27;</span>], train_set[<span class="string">&#x27;label&#x27;</span>])]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(train_set_data[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 출력 결과 =&gt; [&#x27;큰아들이 결혼하는데 집을 사달라고 해서 화가 나.&#x27;, &#x27;4&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(data_train[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 출력 결과 =&gt;</span></span><br><span class="line">(array([   <span class="number">2</span>, <span class="number">4688</span>, <span class="number">6797</span>, <span class="number">5940</span>,  <span class="number">950</span>, <span class="number">7795</span>, <span class="number">4384</span>, <span class="number">7088</span>, <span class="number">2573</span>, <span class="number">5794</span>, <span class="number">5439</span>,</span><br><span class="line">        <span class="number">5007</span>, <span class="number">5112</span>, <span class="number">5330</span>, <span class="number">1370</span>,  <span class="number">517</span>,   <span class="number">54</span>,    <span class="number">3</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">           <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">           <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">           <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">           <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>], dtype=int32),</span><br><span class="line"> array(<span class="number">18</span>, dtype=int32),</span><br><span class="line"> array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,</span><br><span class="line">        <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,</span><br><span class="line">        <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       dtype=int32),</span><br><span class="line"> <span class="number">4</span>)</span><br><span class="line"><span class="comment"># array의 첫 번째는 패딩된 시퀀스, 두 번째는 길이와 타입에 대한 내용, 세 번재는 어텐션 마스크 시퀀스이다.</span></span><br></pre></td></tr></table></figure>

<ul>
<li>아래는 <code>from kobert.pytorch_kobert import get_pytorch_kobert_model</code>의 <code>get_kobert_model</code>함수 형식입니다.</li>
<li><blockquote>
<p><code>bertmodel, vocab = get_pytorch_kobert_model()</code>을<br> 실행 시 에러가 나온다면 <br><code>!pip install sentencepiece==0.1.91</code><br> <code>!pip install transformers==4.8.2</code><br>버전을 맞춰주면 해결이 될 수도 있습니다.</p>
</blockquote>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_kobert_model</span>(<span class="params">model_path, vocab_file, ctx=<span class="string">&quot;cpu&quot;</span></span>):</span><br><span class="line">    bertmodel = BertModel.from_pretrained(model_path)</span><br><span class="line">    device = torch.device(ctx)</span><br><span class="line">    bertmodel.to(device)</span><br><span class="line">    bertmodel.<span class="built_in">eval</span>()</span><br><span class="line">    vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(vocab_file,</span><br><span class="line">                                                         padding_token=<span class="string">&#x27;[PAD]&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> bertmodel, vocab_b_obj</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>KoBERT 함수 출처&lt;<a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/SKTBrain/KoBERT/blob/8df69ec6b588ae661bef98d28ec29448482bbe6e/kobert/pytorch_kobert.py#L5">KoBERT</a>&gt;</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2022-11-30T15:00:00.000Z" title="2022. 12. 1. 오전 12:00:00">2022-12-01</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2022-11-30T15:00:00.000Z" title="2022. 12. 1. 오전 12:00:00">2022-12-01</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">3분안에 읽기 (약 434 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/12/01/Study_folder/DL(Deep_Learning)/2022-12-01-Overfitting/">과적적합(Overfitting)을 막는 방법</a></h1><div class="content"><h2 id="과적합을-막는-방법"><a href="#과적합을-막는-방법" class="headerlink" title="과적합을 막는 방법"></a>과적합을 막는 방법</h2><p>1.데이터의 양을 늘리기</p>
<ul>
<li>만약, 데이터의 양이 적을 경우에는 의도적으로 기존의 데이터를 조금씩 변형하고 추가하여 데이터의 양을 늘리기도 하는데 이를 데이터 증식 또는 증강(Data Augmentation)이라고 합니다. <br></li>
<li>이미지의 경우에는 데이터 증식이 많이 사용되는데 이미지를 돌리거나 노이즈를 추가하고, 일부분을 수정하는 등으로 데이터를 증식시킵니다. <br></li>
<li>텍스트 데이터의 경우에는 데이터를 증강하는 방법으로 번역 후 재번역을 통해 새로운 데이터를 만들어내는 역번역(Back Translation) 등의 방법이 있습니다.</li>
</ul>
<p>2.모델의 복잡도를 줄이기</p>
<p>3.가중치 규제(Regularization) 적용하기</p>
<p>4.드롭아웃(Dropout)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#예시</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dropout, Dense</span><br><span class="line"></span><br><span class="line">max_words = <span class="number">10000</span></span><br><span class="line">num_classes = <span class="number">46</span></span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">256</span>, input_shape=(max_words,), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>)) <span class="comment"># 드롭아웃 추가. 비율은 50%  &lt;- 여기서 드롭아웃 50%만 사용한다는 의미(랜덤으로)</span></span><br><span class="line">model.add(Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>)) <span class="comment"># 드롭아웃 추가. 비율은 50%</span></span><br><span class="line">model.add(Dense(num_classes, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 코드 출처 : 위키독스(wikidocs)</span></span><br></pre></td></tr></table></figure>


<h3 id="pad-sequences-사용하는-이유"><a href="#pad-sequences-사용하는-이유" class="headerlink" title="pad_sequences 사용하는 이유"></a>pad_sequences 사용하는 이유</h3><ul>
<li>샘플의 길이가 서로 다를 수 있는데, 모델의 입력으로 사용하려면 모든 샘플의 길이를 통일해주는 함수</li>
<li>maxlen &#x3D; 모든 데이터에 대해서 정규화 할 길이</li>
<li>padding &#x3D; ‘post’로 입력 시 뒤에께 잘림, 보통 ‘post’를 사용함</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"></span><br><span class="line">pad_sequences([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]], maxlen=<span class="number">3</span>, padding=<span class="string">&#x27;pre&#x27;</span>) </span><br><span class="line"><span class="comment">#결과</span></span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">7</span>, <span class="number">8</span>]], dtype=int32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pad_sequences([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]], maxlen=<span class="number">3</span>, padding=<span class="string">&#x27;post&#x27;</span>)</span><br><span class="line"><span class="comment">#결과</span></span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">       [<span class="number">7</span>, <span class="number">8</span>, <span class="number">0</span>]], dtype=int32)</span><br></pre></td></tr></table></figure></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2022-11-30T15:00:00.000Z" title="2022. 12. 1. 오전 12:00:00">2022-12-01</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2022-11-30T15:00:00.000Z" title="2022. 12. 1. 오전 12:00:00">2022-12-01</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">1분안에 읽기 (약 147 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/12/01/Study_folder/DL(Deep_Learning)/2022-12-03-callbacks/">callbacks - EarlyStopping, ModelCheckpoint</a></h1><div class="content"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.callbacks <span class="keyword">import</span> EarlyStopping, ModelCheckpoint</span><br><span class="line"></span><br><span class="line"><span class="comment"># earlystopping == 더이상 개선이 안될 경우 스탑하는 기능.</span></span><br><span class="line"><span class="comment"># 관심 가지는 변수 monitor = &#x27;변수&#x27;</span></span><br><span class="line"><span class="comment"># 연속해서 4번의 변화까지는 참는다(계속 진행한다) patience = 4</span></span><br><span class="line">es = EarlyStopping(monitor=<span class="string">&#x27;val_loss&#x27;</span>, mode=<span class="string">&#x27;min&#x27;</span>, verbose=<span class="number">1</span>, patience=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 모델 임시 저장 modelcheckpoint</span></span><br><span class="line">mc = ModelCheckpoint(<span class="string">&#x27;best_model.h5&#x27;</span>, monitor=<span class="string">&#x27;val_acc&#x27;</span>, mode=<span class="string">&#x27;max&#x27;</span>, verbose=<span class="number">1</span>, save_best_only=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 콜백(callback) : 시스템이 어떤 상황이 되었을때 시스템에 의해 자동으로 호출되는 함수</span></span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>, metrics=[<span class="string">&#x27;acc&#x27;</span>])</span><br><span class="line">history = model.fit(X_train, y_train, batch_size=<span class="number">128</span>, epochs=<span class="number">2</span>, callbacks=[es, mc], validation_data=(X_test, y_test))</span><br></pre></td></tr></table></figure></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2022-11-29T15:00:00.000Z" title="2022. 11. 30. 오전 12:00:00">2022-11-30</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2022-11-30T15:00:00.000Z" title="2022. 12. 1. 오전 12:00:00">2022-12-01</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">4분안에 읽기 (약 652 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/11/30/Study_folder/Basic_study/2022-11-30-Class/">Class(클래스, 클래스 상속)</a></h1><div class="content"><h2 id="클래스-Class"><a href="#클래스-Class" class="headerlink" title="클래스(Class)"></a>클래스(Class)</h2><ul>
<li><p>클래스 : 클래스는 <code>데이터와 기능을 함께 묶는 방법</code></p>
</li>
<li><p>인스턴스 : 클래스로 정의된 객체를 프로그램 상에서 이용할 수 있게 만든 <code>변수</code></p>
</li>
<li><p>인스턴스를 사용하기 위해 클래스를 먼저 만들어야 함</p>
</li>
<li><p>클래스의 함수 : <code>메소드</code> &lt; 클래스 내부의 함수</p>
</li>
<li><p>클래스의 명명은 맨 앞자는 대분자를 쓰는게 일반적입니다. </p>
</li>
<li><p>아래는 간단한 예제 입니다.</p>
</li>
</ul>
<h3 id="예시1-클래스-생성"><a href="#예시1-클래스-생성" class="headerlink" title="예시1 - 클래스 생성"></a>예시1 - 클래스 생성</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Charicters</span> :</span><br><span class="line">    <span class="comment"># 클래스의 생성자</span></span><br><span class="line">    <span class="comment"># def __init__ 은 이와 유사한 형식을 따라야 함</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name, hobby</span>):</span><br><span class="line">        self.name = name <span class="comment">#클래스의 맴버</span></span><br><span class="line">        self.hobby = hobby <span class="comment">#클래스의 맴버</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#클래스의 메소드</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">xxx</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;이름 : <span class="subst">&#123;self.name&#125;</span>, 취미 : <span class="subst">&#123;self.hobby&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#setter 메소드</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_hobby</span>(<span class="params">self, hobby</span>):</span><br><span class="line">        self.hobby = hobby</span><br><span class="line"></span><br><span class="line">    <span class="comment">#클래스 계승을 위한 함수(밑의 예시용)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">nice</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;nice to meet you&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = Characters(<span class="string">&#x27;조인환&#x27;</span>, <span class="string">&#x27;클라이밍&#x27;</span>)</span><br><span class="line">a.xxx()</span><br><span class="line"><span class="comment"># 출력 결과</span></span><br><span class="line"><span class="comment"># 이름 : 조인환, 취미 : 클라이밍</span></span><br><span class="line"></span><br><span class="line">a.hobby</span><br><span class="line"><span class="comment"># 출력 결과</span></span><br><span class="line"><span class="comment"># 클라이밍</span></span><br><span class="line"></span><br><span class="line">a = Characters(<span class="string">&#x27;조인환&#x27;</span>, <span class="string">&#x27;클라이밍&#x27;</span>)</span><br><span class="line">a.set_hobby(<span class="string">&#x27;산책&#x27;</span>)</span><br><span class="line">a.xxx()</span><br><span class="line"><span class="comment"># 출력 결과</span></span><br><span class="line"><span class="comment"># 이름 : 조인환, 취미 : 산책</span></span><br></pre></td></tr></table></figure>

<h3 id="예시-2-클래스-상속-딥러닝-모델-생성-pytorch"><a href="#예시-2-클래스-상속-딥러닝-모델-생성-pytorch" class="headerlink" title="예시 2 - 클래스 상속 - 딥러닝 모델 생성(pytorch)"></a>예시 2 - 클래스 상속 - 딥러닝 모델 생성(pytorch)</h3><ul>
<li>부모와 자식 관계가 존재</li>
<li>부모 클래스 : 기존의 클래스</li>
<li>자식 클래스 : 부모 클래스를 상속 받은 클래스</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#클래스에서 클레스를 만드는것을 클래스 상속이라고 합니다.</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Baby</span>(<span class="title class_ inherited__">Characters</span>) :</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">hansome</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment">#super()를 사용하면 부모 클래스의 함수를 사용 가능합니다.</span></span><br><span class="line">    <span class="built_in">super</span>().nice()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">hobi_is</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;self.name&#125;</span>의 취미는 <span class="subst">&#123;self.hobby&#125;</span>입니다&#x27;</span>)</span><br><span class="line"></span><br><span class="line">b = Baby(<span class="string">&#x27;조인환&#x27;</span>,<span class="string">&#x27;클라이밍&#x27;</span>)</span><br><span class="line"></span><br><span class="line">b.nice()</span><br><span class="line"><span class="comment">#nice to meet you</span></span><br><span class="line"></span><br><span class="line">b.hobi_is()</span><br><span class="line"><span class="comment"># 조인환의 취미는 클라이밍입니다</span></span><br></pre></td></tr></table></figure>

<h3 id="예시-3-클래스-상속-딥러닝-모델-생성-pytorch"><a href="#예시-3-클래스-상속-딥러닝-모델-생성-pytorch" class="headerlink" title="예시 3 - 클래스 상속 - 딥러닝 모델 생성(pytorch)"></a>예시 3 - 클래스 상속 - 딥러닝 모델 생성(pytorch)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment">#NeuralNet이라는 클래스를 만듬(nn.Module을 상속)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNet</span>(nn.Module):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment"># torch.nn.Module의 NeuralNet을 사용한다는 의미</span></span><br><span class="line">    <span class="comment"># (from nn import NeuralNet)를 수행</span></span><br><span class="line">    <span class="built_in">super</span>(NeuralNet, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Conv2d를 2번, Linear를 2번 실행한다는 의미</span></span><br><span class="line">    self.conv1 = nn.Conv2d(<span class="number">1</span>,<span class="number">8</span>,<span class="number">2</span>)</span><br><span class="line">    self.conv2 = nn.Conv2d(<span class="number">8</span>,<span class="number">10</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    self.fc1 = nn.Linear(<span class="number">10</span>*<span class="number">5</span>*<span class="number">5</span>,<span class="number">60</span>)</span><br><span class="line">    self.fc2 = nn.Linear(<span class="number">60</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line"></span><br><span class="line">    x = F.max_pool2d(F.relu(self.conv1(x)), <span class="number">2</span>)</span><br><span class="line">    x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)    </span><br><span class="line"></span><br><span class="line">    <span class="comment">#Linear는 flatten한 데이터만 사용 가능해서 데이터 편집</span></span><br><span class="line">    x = x.view(-<span class="number">1</span>,self.num_flat_features(x))</span><br><span class="line">    x = F.relu(self.fc1(x))</span><br><span class="line">    x = self.fc2(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">num_flat_features</span>(<span class="params">self, x</span>):</span><br><span class="line">    size = x.size()[<span class="number">1</span>:]</span><br><span class="line">    num_features = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">      num_features *= s</span><br><span class="line">    <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line">net = NeuralNet()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2022-11-29T15:00:00.000Z" title="2022. 11. 30. 오전 12:00:00">2022-11-30</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2022-11-29T15:00:00.000Z" title="2022. 11. 30. 오전 12:00:00">2022-11-30</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">2분안에 읽기 (약 322 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/11/30/Study_folder/Basic_study/2022-11-30-save-list(pickle)/">pickle()-파일,리스트 저장</a></h1><div class="content"><h2 id="pickle"><a href="#pickle" class="headerlink" title="pickle"></a>pickle</h2><ul>
<li>파이썬에서 작업중이던 리스트, 딕셔너리 등을 저장해서 다른 위치에서 열고 싶을 경우</li>
<li>피클 파일을 이용하여 저장, 불러오기가 가능합니다.</li>
<li><code>with open(파일 이름, 파일 모드) as f:</code> # &lt;- f라는 이름의 파일로 파일 이름, 모드를 연다는 의미.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">list_save = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 피클 파일로 저장하는 겁니다. 파일 명은 &quot;list_ex.pkl&quot;로 저장</span></span><br><span class="line"><span class="comment"># 편집 가능하게 &#x27;wb&#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;list_ex.pkl&quot;</span>,<span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(list_save, f)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 불러오는 방법입니다</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;list_save.pkl&quot;</span>,<span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    list_load = pickle.load(f)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(list_load)</span><br><span class="line"><span class="comment"># [1, 2, 3, 4]</span></span><br></pre></td></tr></table></figure>


<ul>
<li>파일 모드 설명(보통 wb,rb를 많이 사용))</li>
</ul>
<p>|file_mode|기능|설명|<br>|’r’|읽기 전용|파일을 읽어오는 기능이며, 파일이 없으면 에러|<br>|’w’|쓰기 전용|파일이 있으면 내용을 덮어 씀|<br>|’a’|추가|파일이 없으면 파일을 생성|<br>|’b’|바이너리 모드|파일의 내용을 그대로 읽고, 값을 그대로 사용|</p>
<h3 id="joblib으로-저장-불러오기"><a href="#joblib으로-저장-불러오기" class="headerlink" title="joblib으로 저장, 불러오기"></a>joblib으로 저장, 불러오기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#joblib으로 dump하여 피클 형식으로 저장하기.</span></span><br><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line"></span><br><span class="line">joblib.dump((x_train, x_test, y_train, y_test), <span class="string">&#x27;review.pkl&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 불러오기</span></span><br><span class="line">x_train, x_test, y_train, y_test = joblib.load(<span class="string">&#x27;review.pkl&#x27;</span>)</span><br></pre></td></tr></table></figure></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2022-11-26T15:00:00.000Z" title="2022. 11. 27. 오전 12:00:00">2022-11-27</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2022-11-26T15:00:00.000Z" title="2022. 11. 27. 오전 12:00:00">2022-11-27</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">23분안에 읽기 (약 3495 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/11/27/Study_folder/Basic_study/2022-11-27-mini-project-first/">Portfolio(미니 프로젝트 1)</a></h1><div class="content"><h2 id="미니-프로젝트-1-문제"><a href="#미니-프로젝트-1-문제" class="headerlink" title="미니 프로젝트 1 문제"></a>미니 프로젝트 1 문제</h2><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">1. tmdb_5000_movies 데이터 셋 분석</span><br><span class="line"></span><br><span class="line">1) 예산과 장르 관계?</span><br><span class="line"></span><br><span class="line">2) 키워드로 많이 사용된 단어는? </span><br><span class="line"></span><br><span class="line">3) 장르와 키워드 관계는?</span><br><span class="line"></span><br><span class="line">4) 평균 평점과 장르 사이의 관계?</span><br><span class="line"></span><br><span class="line">5) 연도별로 많이 제작된 영화 장르는?</span><br><span class="line"></span><br><span class="line">6) 인기도와 예산 관계는?</span><br><span class="line"></span><br><span class="line">7) 영화 run time과 인기도 사이에 관계가 있을까?</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2. tmdb_5000_movies 데이터 기반 추천시스템 제작</span><br><span class="line"></span><br><span class="line">​</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">3. dataset 데이터 분석 및 연관 규칙 생성</span><br><span class="line"></span><br><span class="line">1) dataset 파일 분석</span><br><span class="line"></span><br><span class="line">2) 연도별 많이 / 적게 팔린 아이템은?</span><br><span class="line"></span><br><span class="line">3) member id 에 따른 구매 연(월, 요일)도, 아이템 분석</span><br><span class="line"></span><br><span class="line">- 무슨 요일에 와서 구매를 많이 했을까?</span><br><span class="line"></span><br><span class="line">4) 연관 규칙 생성</span><br><span class="line"></span><br><span class="line">5) 연관 규칙에 따른 vip member에게 어떤 상품을 추천할까?</span><br><span class="line"></span><br><span class="line">-vip는 매출횟수가 가장 많은 상위 100명 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">​</span><br><span class="line">4. 와인의 화학 조성을 사용하여 와인의 종류를 예측하기 위한 데이터이다. load_wine() 명령으로 로드하며 다음과 같이 구성되어 있다. 와인의 종류를 예측할 수 있는 모델을 생성하시오.</span><br><span class="line"></span><br><span class="line">from sklearn.datasets import load_wine</span><br></pre></td></tr></table></figure>

<h2 id="1-2번-문제"><a href="#1-2번-문제" class="headerlink" title="1,2번 문제"></a>1,2번 문제</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t_df = pd.read_csv(<span class="string">&#x27;../Downloads/tmdb_5000_movies.csv&#x27;</span>)</span><br><span class="line">g_df = pd.read_csv(<span class="string">&#x27;../Downloads/Groceries_dataset.csv&#x27;</span>)</span><br><span class="line">pd.options.display.max_columns=<span class="number">50</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t_df.drop(columns=[<span class="string">&#x27;homepage&#x27;</span>,<span class="string">&#x27;tagline&#x27;</span>,<span class="string">&#x27;status&#x27;</span>],inplace=<span class="literal">True</span>)</span><br><span class="line">t_df.dropna(inplace=<span class="literal">True</span>)</span><br><span class="line">t_df.reset_index(drop=<span class="literal">True</span>,inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># json 열을 name만 추출하여 전처리, keywords는 공백이 유의미한 단위이므로 따로 처리</span></span><br><span class="line">json_col = [<span class="string">&#x27;genres&#x27;</span>,<span class="string">&#x27;production_companies&#x27;</span>,<span class="string">&#x27;production_countries&#x27;</span>,<span class="string">&#x27;spoken_languages&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> json_col:</span><br><span class="line">    t_df[i] = t_df[i].apply(<span class="keyword">lambda</span> x : <span class="built_in">eval</span>(x))</span><br><span class="line">    t_df[i] = t_df[i].apply(<span class="keyword">lambda</span> x : [d[<span class="string">&#x27;name&#x27;</span>] <span class="keyword">for</span> d <span class="keyword">in</span> x]).apply(<span class="keyword">lambda</span> x : <span class="string">&#x27; &#x27;</span>.join(x))</span><br><span class="line">t_df[<span class="string">&#x27;keywords&#x27;</span>] = t_df[<span class="string">&#x27;keywords&#x27;</span>].apply(<span class="keyword">lambda</span> x : <span class="built_in">eval</span>(x))</span><br><span class="line">t_df[<span class="string">&#x27;keywords&#x27;</span>] = t_df[<span class="string">&#x27;keywords&#x27;</span>].apply(<span class="keyword">lambda</span> x : [d[<span class="string">&#x27;name&#x27;</span>] <span class="keyword">for</span> d <span class="keyword">in</span> x]).apply(<span class="keyword">lambda</span> x : <span class="string">&#x27;,&#x27;</span>.join(x))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1) 예산과 장르 관계?</span></span><br><span class="line"></span><br><span class="line">t_df[<span class="string">&#x27;budget_cat&#x27;</span>] = pd.qcut(t_df[<span class="string">&#x27;budget&#x27;</span>],<span class="number">5</span>,duplicates=<span class="string">&#x27;drop&#x27;</span>)</span><br><span class="line">t_df.pivot_table(index=<span class="string">&#x27;budget_cat&#x27;</span>,columns=<span class="string">&#x27;genres&#x27;</span>,aggfunc=<span class="string">&#x27;count&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 웨스턴풍은 저예산이 많고, 어드벤쳐쪽은 고예산이 많다.</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead tr th &#123;
    text-align: left;
&#125;

.dataframe thead tr:last-of-type th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="25" halign="left">budget</th>
      <th>...</th>
      <th colspan="25" halign="left">vote_count</th>
    </tr>
    <tr>
      <th>genres</th>
      <th></th>
      <th>Action</th>
      <th>Action Adventure</th>
      <th>Action Adventure Animation Comedy Family</th>
      <th>Action Adventure Animation Comedy Family Fantasy Romance</th>
      <th>Action Adventure Animation Comedy Family Fantasy Science Fiction</th>
      <th>Action Adventure Animation Comedy Science Fiction</th>
      <th>Action Adventure Animation Family</th>
      <th>Action Adventure Animation Family Fantasy</th>
      <th>Action Adventure Animation Fantasy Science Fiction</th>
      <th>Action Adventure Animation Science Fiction Thriller</th>
      <th>Action Adventure Comedy</th>
      <th>Action Adventure Comedy Crime</th>
      <th>Action Adventure Comedy Crime Drama</th>
      <th>Action Adventure Comedy Crime Mystery Thriller</th>
      <th>Action Adventure Comedy Crime Romance Thriller</th>
      <th>Action Adventure Comedy Crime Thriller</th>
      <th>Action Adventure Comedy Drama Family Music Romance</th>
      <th>Action Adventure Comedy Drama Foreign</th>
      <th>Action Adventure Comedy Drama Mystery</th>
      <th>Action Adventure Comedy Drama Science Fiction Thriller</th>
      <th>Action Adventure Comedy Family</th>
      <th>Action Adventure Comedy Family Fantasy</th>
      <th>Action Adventure Comedy Family Fantasy Science Fiction</th>
      <th>Action Adventure Comedy Family Science Fiction</th>
      <th>...</th>
      <th>War</th>
      <th>War Action</th>
      <th>War Action Adventure Drama Thriller</th>
      <th>War Action Drama History Thriller</th>
      <th>War Adventure Drama Romance</th>
      <th>War Comedy Drama</th>
      <th>War Crime Drama Mystery Romance Thriller</th>
      <th>War Drama</th>
      <th>War Drama Action</th>
      <th>War Drama History</th>
      <th>War Drama History Action</th>
      <th>War Drama History Action Romance</th>
      <th>War Drama Romance</th>
      <th>War History Action Adventure Drama Romance</th>
      <th>War History Drama</th>
      <th>War Western</th>
      <th>Western</th>
      <th>Western Action Drama History</th>
      <th>Western Adventure</th>
      <th>Western Animation Adventure Comedy Family</th>
      <th>Western Comedy</th>
      <th>Western Drama</th>
      <th>Western Drama Adventure Thriller</th>
      <th>Western History</th>
      <th>Western History War</th>
    </tr>
    <tr>
      <th>budget_cat</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>(-0.001, 7500000.0]</th>
      <td>27</td>
      <td>6</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>15</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>(7500000.0, 22000000.0]</th>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>(22000000.0, 50000000.0]</th>
      <td>0</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>5</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>(50000000.0, 380000000.0]</th>
      <td>0</td>
      <td>5</td>
      <td>7</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>8</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>4 rows × 18800 columns</p>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2) 키워드로 많이 사용된 단어는? </span></span><br><span class="line">t_df.keywords</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">a = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(t_df[<span class="string">&#x27;keywords&#x27;</span>])):</span><br><span class="line">    a.append(t_df[<span class="string">&#x27;keywords&#x27;</span>][i])</span><br><span class="line">a = <span class="string">&#x27;&#x27;</span>.join(a)</span><br><span class="line">words = a.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">counter = Counter(words)</span><br><span class="line"><span class="built_in">print</span>(counter.most_common(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># (&#x27;independent film&#x27;, 192), (&#x27;murder&#x27;, 172), (&#x27;violence&#x27;, 146), (&#x27;dystopia&#x27;, 120), (&#x27;duringcreditsstinger&#x27;, 116)</span></span><br></pre></td></tr></table></figure>

<pre><code>[(&#39;independent film&#39;, 192), (&#39;murder&#39;, 172), (&#39;violence&#39;, 146), (&#39;dystopia&#39;, 120), (&#39;duringcreditsstinger&#39;, 116)]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3) 장르와 키워드 관계는?</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1) Tf(Term Frequency)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">하나의 문서(문장)에서 특정 단어가 등장하는 횟수</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">2) Idf(Inverse Document Frequency)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Df(Document Frequency)는 문서 빈도. 특정 단어가 몇 개의 문서(문장)에서 등장하는지를 수치화 한 것. 그것의 역수가 idf다.</span></span><br><span class="line"><span class="string">보통 그냥 역수를 취하기 보다는 아래처럼 수식화한다. </span></span><br><span class="line"><span class="string">역수 개념을 사용하는 이유는, 적은 문서(문장)에 등장할수록 큰 숫자가 되게하고 반대로 많은 문서(문장)에 등장할수록 숫자를 작아지게 함으로써</span></span><br><span class="line"><span class="string">여러 문서(문장)에 의미 없이 사용되는 단어의 가중치를 줄이기 위해서다.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line"></span><br><span class="line">tfidf_vector = TfidfVectorizer()</span><br><span class="line"></span><br><span class="line"><span class="comment">#장르와 키워드를 백터화</span></span><br><span class="line">tfidf_matrix = tfidf_vector.fit_transform(t_df[<span class="string">&#x27;genres&#x27;</span>]+ <span class="string">&#x27; &#x27;</span> + t_df[<span class="string">&#x27;keywords&#x27;</span>]).toarray()</span><br><span class="line"></span><br><span class="line"><span class="comment">#벡터화되기 전 이름들을 추출</span></span><br><span class="line">tfidf_matrix_feature = tfidf_vector.get_feature_names_out()</span><br><span class="line"></span><br><span class="line"><span class="comment">#데이터 프레임으로 만들기</span></span><br><span class="line">tfidf_matrix = pd.DataFrame(tfidf_matrix, columns=tfidf_matrix_feature, index = t_df.title)</span><br><span class="line"></span><br><span class="line"><span class="comment">#코사인 유사도로 유사 관계 파악</span></span><br><span class="line">cosine_sim = cosine_similarity(tfidf_matrix)</span><br><span class="line">cosine_sim_df = pd.DataFrame(cosine_sim, index = t_df.title, columns = t_df.title)</span><br><span class="line">cosine_sim_df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>title</th>
      <th>Avatar</th>
      <th>Pirates of the Caribbean: At World's End</th>
      <th>Spectre</th>
      <th>The Dark Knight Rises</th>
      <th>John Carter</th>
      <th>Spider-Man 3</th>
      <th>Tangled</th>
      <th>Avengers: Age of Ultron</th>
      <th>Harry Potter and the Half-Blood Prince</th>
      <th>Batman v Superman: Dawn of Justice</th>
      <th>Superman Returns</th>
      <th>Quantum of Solace</th>
      <th>Pirates of the Caribbean: Dead Man's Chest</th>
      <th>The Lone Ranger</th>
      <th>Man of Steel</th>
      <th>The Chronicles of Narnia: Prince Caspian</th>
      <th>The Avengers</th>
      <th>Pirates of the Caribbean: On Stranger Tides</th>
      <th>Men in Black 3</th>
      <th>The Hobbit: The Battle of the Five Armies</th>
      <th>The Amazing Spider-Man</th>
      <th>Robin Hood</th>
      <th>The Hobbit: The Desolation of Smaug</th>
      <th>The Golden Compass</th>
      <th>King Kong</th>
      <th>...</th>
      <th>Rampage</th>
      <th>Slacker</th>
      <th>Dutch Kills</th>
      <th>Dry Spell</th>
      <th>Flywheel</th>
      <th>Backmask</th>
      <th>The Puffy Chair</th>
      <th>Stories of Our Lives</th>
      <th>Breaking Upwards</th>
      <th>All Superheroes Must Die</th>
      <th>Pink Flamingos</th>
      <th>Clean</th>
      <th>The Circle</th>
      <th>Tin Can Man</th>
      <th>Cure</th>
      <th>On The Downlow</th>
      <th>Sanctuary: Quite a Conundrum</th>
      <th>Bang</th>
      <th>Primer</th>
      <th>Cavite</th>
      <th>El Mariachi</th>
      <th>Newlyweds</th>
      <th>Signed, Sealed, Delivered</th>
      <th>Shanghai Calling</th>
      <th>My Date with Drew</th>
    </tr>
    <tr>
      <th>title</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Avatar</th>
      <td>1.000000</td>
      <td>0.033911</td>
      <td>0.017435</td>
      <td>0.004221</td>
      <td>0.248501</td>
      <td>0.035139</td>
      <td>0.024894</td>
      <td>0.059988</td>
      <td>0.023661</td>
      <td>0.022763</td>
      <td>0.043666</td>
      <td>0.022542</td>
      <td>0.022596</td>
      <td>0.010072</td>
      <td>0.088634</td>
      <td>0.043253</td>
      <td>0.077686</td>
      <td>0.102427</td>
      <td>0.143133</td>
      <td>0.092661</td>
      <td>0.021569</td>
      <td>0.029362</td>
      <td>0.045073</td>
      <td>0.022733</td>
      <td>0.013829</td>
      <td>...</td>
      <td>0.010297</td>
      <td>0.008659</td>
      <td>0.000000</td>
      <td>0.061893</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.023269</td>
      <td>0.000000</td>
      <td>0.031020</td>
      <td>0.074724</td>
      <td>0.030946</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.012150</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.048523</td>
      <td>0.000000</td>
      <td>0.006793</td>
      <td>0.056478</td>
      <td>0.026349</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Pirates of the Caribbean: At World's End</th>
      <td>0.033911</td>
      <td>1.000000</td>
      <td>0.021037</td>
      <td>0.034132</td>
      <td>0.012511</td>
      <td>0.111809</td>
      <td>0.011378</td>
      <td>0.017092</td>
      <td>0.044458</td>
      <td>0.027466</td>
      <td>0.029152</td>
      <td>0.027200</td>
      <td>0.474144</td>
      <td>0.012153</td>
      <td>0.029551</td>
      <td>0.068232</td>
      <td>0.038306</td>
      <td>0.208616</td>
      <td>0.008180</td>
      <td>0.028966</td>
      <td>0.037160</td>
      <td>0.029612</td>
      <td>0.030120</td>
      <td>0.027430</td>
      <td>0.142986</td>
      <td>...</td>
      <td>0.012424</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.015401</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.018194</td>
      <td>0.038199</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.008197</td>
      <td>0.000000</td>
      <td>0.021293</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Spectre</th>
      <td>0.017435</td>
      <td>0.021037</td>
      <td>1.000000</td>
      <td>0.078061</td>
      <td>0.070146</td>
      <td>0.060568</td>
      <td>0.030749</td>
      <td>0.111236</td>
      <td>0.017189</td>
      <td>0.057658</td>
      <td>0.103059</td>
      <td>0.549934</td>
      <td>0.022014</td>
      <td>0.018065</td>
      <td>0.062035</td>
      <td>0.076458</td>
      <td>0.062227</td>
      <td>0.023734</td>
      <td>0.012160</td>
      <td>0.023387</td>
      <td>0.021013</td>
      <td>0.024623</td>
      <td>0.018135</td>
      <td>0.096466</td>
      <td>0.024804</td>
      <td>...</td>
      <td>0.045032</td>
      <td>0.000000</td>
      <td>0.097970</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.012025</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.070458</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.123730</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>The Dark Knight Rises</th>
      <td>0.004221</td>
      <td>0.034132</td>
      <td>0.078061</td>
      <td>1.000000</td>
      <td>0.004502</td>
      <td>0.051288</td>
      <td>0.024913</td>
      <td>0.062652</td>
      <td>0.000000</td>
      <td>0.180473</td>
      <td>0.109040</td>
      <td>0.109994</td>
      <td>0.005329</td>
      <td>0.004373</td>
      <td>0.150406</td>
      <td>0.023430</td>
      <td>0.059022</td>
      <td>0.005746</td>
      <td>0.065193</td>
      <td>0.005662</td>
      <td>0.082179</td>
      <td>0.005961</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.012210</td>
      <td>...</td>
      <td>0.061043</td>
      <td>0.079586</td>
      <td>0.186559</td>
      <td>0.000000</td>
      <td>0.007272</td>
      <td>0.019680</td>
      <td>0.020162</td>
      <td>0.094137</td>
      <td>0.000000</td>
      <td>0.120491</td>
      <td>0.013621</td>
      <td>0.010274</td>
      <td>0.027136</td>
      <td>0.0</td>
      <td>0.019153</td>
      <td>0.004788</td>
      <td>0.032963</td>
      <td>0.010261</td>
      <td>0.093245</td>
      <td>0.023555</td>
      <td>0.033142</td>
      <td>0.000000</td>
      <td>0.007541</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>John Carter</th>
      <td>0.248501</td>
      <td>0.012511</td>
      <td>0.070146</td>
      <td>0.004502</td>
      <td>1.000000</td>
      <td>0.012965</td>
      <td>0.049956</td>
      <td>0.088164</td>
      <td>0.010223</td>
      <td>0.034291</td>
      <td>0.154023</td>
      <td>0.024046</td>
      <td>0.013092</td>
      <td>0.032782</td>
      <td>0.151277</td>
      <td>0.060955</td>
      <td>0.105641</td>
      <td>0.073203</td>
      <td>0.119286</td>
      <td>0.063651</td>
      <td>0.012497</td>
      <td>0.031320</td>
      <td>0.078151</td>
      <td>0.057371</td>
      <td>0.014752</td>
      <td>...</td>
      <td>0.010983</td>
      <td>0.009237</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.079708</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.034295</td>
      <td>0.012961</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.051759</td>
      <td>0.000000</td>
      <td>0.007246</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>El Mariachi</th>
      <td>0.006793</td>
      <td>0.008197</td>
      <td>0.123730</td>
      <td>0.033142</td>
      <td>0.007246</td>
      <td>0.008494</td>
      <td>0.000000</td>
      <td>0.009899</td>
      <td>0.000000</td>
      <td>0.008641</td>
      <td>0.009172</td>
      <td>0.053005</td>
      <td>0.008578</td>
      <td>0.007039</td>
      <td>0.009297</td>
      <td>0.000000</td>
      <td>0.009326</td>
      <td>0.009248</td>
      <td>0.011084</td>
      <td>0.009113</td>
      <td>0.008188</td>
      <td>0.009594</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.009665</td>
      <td>...</td>
      <td>0.056640</td>
      <td>0.000000</td>
      <td>0.146807</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.031677</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.032542</td>
      <td>0.010962</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.019168</td>
      <td>0.000000</td>
      <td>0.053056</td>
      <td>0.000000</td>
      <td>0.009552</td>
      <td>0.037913</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Newlyweds</th>
      <td>0.056478</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.053283</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.051092</td>
      <td>0.000000</td>
      <td>0.332759</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.411991</td>
      <td>0.000000</td>
      <td>0.549240</td>
      <td>0.000000</td>
      <td>0.036634</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.275376</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.154088</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Signed, Sealed, Delivered</th>
      <td>0.026349</td>
      <td>0.021293</td>
      <td>0.000000</td>
      <td>0.007541</td>
      <td>0.000000</td>
      <td>0.022065</td>
      <td>0.000000</td>
      <td>0.064265</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.060542</td>
      <td>0.000000</td>
      <td>0.008210</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.005281</td>
      <td>...</td>
      <td>0.009198</td>
      <td>0.007873</td>
      <td>0.033924</td>
      <td>0.051274</td>
      <td>0.006188</td>
      <td>0.000000</td>
      <td>0.080640</td>
      <td>0.080105</td>
      <td>0.084631</td>
      <td>0.000000</td>
      <td>0.005645</td>
      <td>0.008742</td>
      <td>0.023091</td>
      <td>0.0</td>
      <td>0.042630</td>
      <td>0.004074</td>
      <td>0.042432</td>
      <td>0.008732</td>
      <td>0.005635</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.154088</td>
      <td>1.000000</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Shanghai Calling</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>My Date with Drew</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.132800</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.044517</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
<p>4799 rows × 4799 columns</p>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#4) 평균 평점과 장르 사이의 관계?</span></span><br><span class="line"></span><br><span class="line">tfidf_vector = TfidfVectorizer()</span><br><span class="line"></span><br><span class="line"><span class="comment">#장르를 백터화</span></span><br><span class="line">tfidf_matrix = tfidf_vector.fit_transform(t_df[<span class="string">&#x27;genres&#x27;</span>]).toarray()</span><br><span class="line"></span><br><span class="line"><span class="comment">#벡터화되기 전 이름들을 추출</span></span><br><span class="line">tfidf_matrix_feature = tfidf_vector.get_feature_names_out()</span><br><span class="line"></span><br><span class="line"><span class="comment">#유사도 값이 아닌 이진화</span></span><br><span class="line">tfidf_matrix = pd.DataFrame(tfidf_matrix, columns=tfidf_matrix_feature)</span><br><span class="line">tfidf_matrix = tfidf_matrix.applymap(<span class="keyword">lambda</span> x : <span class="number">0</span> <span class="keyword">if</span> x == <span class="number">0</span> <span class="keyword">else</span> <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 평점을 범주화 (평점 5가 가장 높음)</span></span><br><span class="line">t_df[<span class="string">&#x27;cat_voteav&#x27;</span>] = pd.qcut(t_df[<span class="string">&#x27;vote_average&#x27;</span>],<span class="number">5</span>,labels=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">tfidf_matrix[<span class="string">&#x27;vote_average&#x27;</span>] = t_df[<span class="string">&#x27;cat_voteav&#x27;</span>]</span><br><span class="line">tfidf_matrix.groupby(<span class="string">&#x27;vote_average&#x27;</span>).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 호러*코미디가 평점이 낮고, 음악&amp;역사&amp;드라마가 평점이 높은 경향이 있다.</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>action</th>
      <th>adventure</th>
      <th>animation</th>
      <th>comedy</th>
      <th>crime</th>
      <th>documentary</th>
      <th>drama</th>
      <th>family</th>
      <th>fantasy</th>
      <th>fiction</th>
      <th>foreign</th>
      <th>history</th>
      <th>horror</th>
      <th>movie</th>
      <th>music</th>
      <th>mystery</th>
      <th>romance</th>
      <th>science</th>
      <th>thriller</th>
      <th>tv</th>
      <th>war</th>
      <th>western</th>
    </tr>
    <tr>
      <th>vote_average</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>266</td>
      <td>149</td>
      <td>31</td>
      <td>427</td>
      <td>95</td>
      <td>19</td>
      <td>273</td>
      <td>122</td>
      <td>99</td>
      <td>137</td>
      <td>7</td>
      <td>10</td>
      <td>203</td>
      <td>4</td>
      <td>22</td>
      <td>59</td>
      <td>152</td>
      <td>137</td>
      <td>288</td>
      <td>4</td>
      <td>7</td>
      <td>15</td>
    </tr>
    <tr>
      <th>2</th>
      <td>291</td>
      <td>203</td>
      <td>54</td>
      <td>458</td>
      <td>146</td>
      <td>9</td>
      <td>416</td>
      <td>124</td>
      <td>109</td>
      <td>122</td>
      <td>3</td>
      <td>21</td>
      <td>130</td>
      <td>1</td>
      <td>32</td>
      <td>70</td>
      <td>207</td>
      <td>122</td>
      <td>309</td>
      <td>1</td>
      <td>19</td>
      <td>12</td>
    </tr>
    <tr>
      <th>3</th>
      <td>279</td>
      <td>162</td>
      <td>45</td>
      <td>397</td>
      <td>190</td>
      <td>13</td>
      <td>503</td>
      <td>102</td>
      <td>83</td>
      <td>104</td>
      <td>5</td>
      <td>44</td>
      <td>95</td>
      <td>1</td>
      <td>39</td>
      <td>102</td>
      <td>192</td>
      <td>104</td>
      <td>321</td>
      <td>1</td>
      <td>29</td>
      <td>13</td>
    </tr>
    <tr>
      <th>4</th>
      <td>173</td>
      <td>144</td>
      <td>51</td>
      <td>267</td>
      <td>139</td>
      <td>24</td>
      <td>578</td>
      <td>92</td>
      <td>67</td>
      <td>98</td>
      <td>13</td>
      <td>54</td>
      <td>55</td>
      <td>1</td>
      <td>50</td>
      <td>55</td>
      <td>200</td>
      <td>98</td>
      <td>200</td>
      <td>1</td>
      <td>36</td>
      <td>16</td>
    </tr>
    <tr>
      <th>5</th>
      <td>145</td>
      <td>132</td>
      <td>53</td>
      <td>173</td>
      <td>126</td>
      <td>43</td>
      <td>526</td>
      <td>73</td>
      <td>66</td>
      <td>74</td>
      <td>6</td>
      <td>68</td>
      <td>36</td>
      <td>1</td>
      <td>42</td>
      <td>62</td>
      <td>143</td>
      <td>74</td>
      <td>156</td>
      <td>1</td>
      <td>53</td>
      <td>26</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5) 연도별로 많이 제작된 영화 장르는?</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 평점을 범주화 (평점 5가 가장 높음)</span></span><br><span class="line">t_df[<span class="string">&#x27;cat_year&#x27;</span>] = pd.qcut(t_df[<span class="string">&#x27;vote_average&#x27;</span>],<span class="number">10</span>,labels=<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">11</span>))</span><br><span class="line">tfidf_matrix[<span class="string">&#x27;cat_year&#x27;</span>] = t_df[<span class="string">&#x27;cat_year&#x27;</span>]</span><br><span class="line">tfidf_matrix.groupby(<span class="string">&#x27;cat_year&#x27;</span>).<span class="built_in">sum</span>(numeric_only=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#액션은 최근에 줄어드는 경향이 있음.</span></span><br><span class="line"><span class="comment">#드라마는 늘어나는 경향이 있음.</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>action</th>
      <th>adventure</th>
      <th>animation</th>
      <th>comedy</th>
      <th>crime</th>
      <th>documentary</th>
      <th>drama</th>
      <th>family</th>
      <th>fantasy</th>
      <th>fiction</th>
      <th>foreign</th>
      <th>history</th>
      <th>horror</th>
      <th>movie</th>
      <th>music</th>
      <th>mystery</th>
      <th>romance</th>
      <th>science</th>
      <th>thriller</th>
      <th>tv</th>
      <th>war</th>
      <th>western</th>
    </tr>
    <tr>
      <th>cat_year</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>135</td>
      <td>69</td>
      <td>12</td>
      <td>203</td>
      <td>45</td>
      <td>14</td>
      <td>136</td>
      <td>61</td>
      <td>42</td>
      <td>76</td>
      <td>2</td>
      <td>2</td>
      <td>114</td>
      <td>2</td>
      <td>12</td>
      <td>29</td>
      <td>69</td>
      <td>76</td>
      <td>145</td>
      <td>2</td>
      <td>3</td>
      <td>10</td>
    </tr>
    <tr>
      <th>2</th>
      <td>131</td>
      <td>80</td>
      <td>19</td>
      <td>224</td>
      <td>50</td>
      <td>5</td>
      <td>137</td>
      <td>61</td>
      <td>57</td>
      <td>61</td>
      <td>5</td>
      <td>8</td>
      <td>89</td>
      <td>2</td>
      <td>10</td>
      <td>30</td>
      <td>83</td>
      <td>61</td>
      <td>143</td>
      <td>2</td>
      <td>4</td>
      <td>5</td>
    </tr>
    <tr>
      <th>3</th>
      <td>127</td>
      <td>79</td>
      <td>24</td>
      <td>205</td>
      <td>59</td>
      <td>5</td>
      <td>156</td>
      <td>56</td>
      <td>44</td>
      <td>59</td>
      <td>1</td>
      <td>4</td>
      <td>56</td>
      <td>0</td>
      <td>17</td>
      <td>27</td>
      <td>83</td>
      <td>59</td>
      <td>136</td>
      <td>0</td>
      <td>3</td>
      <td>5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>164</td>
      <td>124</td>
      <td>30</td>
      <td>253</td>
      <td>87</td>
      <td>4</td>
      <td>260</td>
      <td>68</td>
      <td>65</td>
      <td>63</td>
      <td>2</td>
      <td>17</td>
      <td>74</td>
      <td>1</td>
      <td>15</td>
      <td>43</td>
      <td>124</td>
      <td>63</td>
      <td>173</td>
      <td>1</td>
      <td>16</td>
      <td>7</td>
    </tr>
    <tr>
      <th>5</th>
      <td>117</td>
      <td>68</td>
      <td>10</td>
      <td>154</td>
      <td>85</td>
      <td>3</td>
      <td>187</td>
      <td>37</td>
      <td>26</td>
      <td>43</td>
      <td>4</td>
      <td>13</td>
      <td>44</td>
      <td>1</td>
      <td>18</td>
      <td>43</td>
      <td>81</td>
      <td>43</td>
      <td>136</td>
      <td>1</td>
      <td>13</td>
      <td>4</td>
    </tr>
    <tr>
      <th>6</th>
      <td>162</td>
      <td>94</td>
      <td>35</td>
      <td>243</td>
      <td>105</td>
      <td>10</td>
      <td>316</td>
      <td>65</td>
      <td>57</td>
      <td>61</td>
      <td>1</td>
      <td>31</td>
      <td>51</td>
      <td>0</td>
      <td>21</td>
      <td>59</td>
      <td>111</td>
      <td>61</td>
      <td>185</td>
      <td>0</td>
      <td>16</td>
      <td>9</td>
    </tr>
    <tr>
      <th>7</th>
      <td>95</td>
      <td>72</td>
      <td>28</td>
      <td>130</td>
      <td>59</td>
      <td>7</td>
      <td>231</td>
      <td>41</td>
      <td>33</td>
      <td>52</td>
      <td>4</td>
      <td>27</td>
      <td>32</td>
      <td>0</td>
      <td>24</td>
      <td>27</td>
      <td>87</td>
      <td>52</td>
      <td>95</td>
      <td>0</td>
      <td>17</td>
      <td>8</td>
    </tr>
    <tr>
      <th>8</th>
      <td>78</td>
      <td>72</td>
      <td>23</td>
      <td>137</td>
      <td>80</td>
      <td>17</td>
      <td>347</td>
      <td>51</td>
      <td>34</td>
      <td>46</td>
      <td>9</td>
      <td>27</td>
      <td>23</td>
      <td>1</td>
      <td>26</td>
      <td>28</td>
      <td>113</td>
      <td>46</td>
      <td>105</td>
      <td>1</td>
      <td>19</td>
      <td>8</td>
    </tr>
    <tr>
      <th>9</th>
      <td>74</td>
      <td>59</td>
      <td>18</td>
      <td>88</td>
      <td>59</td>
      <td>17</td>
      <td>235</td>
      <td>24</td>
      <td>26</td>
      <td>31</td>
      <td>2</td>
      <td>31</td>
      <td>19</td>
      <td>0</td>
      <td>23</td>
      <td>32</td>
      <td>68</td>
      <td>31</td>
      <td>85</td>
      <td>0</td>
      <td>21</td>
      <td>10</td>
    </tr>
    <tr>
      <th>10</th>
      <td>71</td>
      <td>73</td>
      <td>35</td>
      <td>85</td>
      <td>67</td>
      <td>26</td>
      <td>291</td>
      <td>49</td>
      <td>40</td>
      <td>43</td>
      <td>4</td>
      <td>37</td>
      <td>17</td>
      <td>1</td>
      <td>19</td>
      <td>30</td>
      <td>75</td>
      <td>43</td>
      <td>71</td>
      <td>1</td>
      <td>32</td>
      <td>16</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 6) 인기도와 예산 관계는?</span></span><br><span class="line"></span><br><span class="line">t_df.pivot_table(index = t_df[<span class="string">&#x27;cat_voteav&#x27;</span>], columns=<span class="string">&#x27;budget_cat&#x27;</span>,aggfunc=<span class="string">&#x27;count&#x27;</span>)[<span class="string">&#x27;budget&#x27;</span>]</span><br><span class="line"><span class="comment">#평점 5가 높은거임.</span></span><br><span class="line"><span class="comment">#평점이 높다고 예산이 높은 영화이지는 않지만, 평점이 낮으면 저예산 영화일 확률은 높다.</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>budget_cat</th>
      <th>(-0.001, 7500000.0]</th>
      <th>(7500000.0, 22000000.0]</th>
      <th>(22000000.0, 50000000.0]</th>
      <th>(50000000.0, 380000000.0]</th>
    </tr>
    <tr>
      <th>cat_voteav</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>508</td>
      <td>174</td>
      <td>177</td>
      <td>138</td>
    </tr>
    <tr>
      <th>2</th>
      <td>352</td>
      <td>212</td>
      <td>250</td>
      <td>234</td>
    </tr>
    <tr>
      <th>3</th>
      <td>345</td>
      <td>217</td>
      <td>249</td>
      <td>214</td>
    </tr>
    <tr>
      <th>4</th>
      <td>375</td>
      <td>202</td>
      <td>188</td>
      <td>157</td>
    </tr>
    <tr>
      <th>5</th>
      <td>341</td>
      <td>180</td>
      <td>139</td>
      <td>147</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 7) 영화 run time과 인기도 사이에 관계가 있을까?</span></span><br><span class="line"></span><br><span class="line">t_df[<span class="string">&#x27;cat_runtime&#x27;</span>] = pd.qcut(t_df[<span class="string">&#x27;runtime&#x27;</span>],<span class="number">5</span>,labels=<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">6</span>))</span><br><span class="line">t_df.pivot_table(index = t_df[<span class="string">&#x27;cat_runtime&#x27;</span>], columns=<span class="string">&#x27;cat_voteav&#x27;</span>,aggfunc=<span class="string">&#x27;count&#x27;</span>)[<span class="string">&#x27;budget&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 확실히 런타임이 길면 길 수록 평점이 높은 경향이 있다.</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>cat_voteav</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
    </tr>
    <tr>
      <th>cat_runtime</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>389</td>
      <td>229</td>
      <td>156</td>
      <td>111</td>
      <td>87</td>
    </tr>
    <tr>
      <th>2</th>
      <td>278</td>
      <td>241</td>
      <td>202</td>
      <td>158</td>
      <td>87</td>
    </tr>
    <tr>
      <th>3</th>
      <td>195</td>
      <td>248</td>
      <td>240</td>
      <td>169</td>
      <td>123</td>
    </tr>
    <tr>
      <th>4</th>
      <td>90</td>
      <td>217</td>
      <td>238</td>
      <td>245</td>
      <td>167</td>
    </tr>
    <tr>
      <th>5</th>
      <td>45</td>
      <td>113</td>
      <td>189</td>
      <td>239</td>
      <td>343</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">genre_recommendations</span>(<span class="params">df , mat, items</span>):</span><br><span class="line">    a = mat.loc[df].sort_values(ascending=<span class="literal">False</span>)[<span class="number">1</span>:<span class="number">21</span>]</span><br><span class="line">    scale_score = pd.DataFrame(a)</span><br><span class="line">    scale_score = scale_score.reset_index()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">        scale_score.loc[i,[<span class="string">&#x27;score&#x27;</span>]] = (items[items[<span class="string">&#x27;title&#x27;</span>] == a.index[i]][<span class="string">&#x27;vote_average&#x27;</span>]).values</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#스케일링</span></span><br><span class="line">    scale_score[<span class="string">&#x27;total_score&#x27;</span>] = scale_score.iloc[:,<span class="number">1</span>]+(scale_score.iloc[:,<span class="number">2</span>]/<span class="number">0.8</span>)</span><br><span class="line">    <span class="keyword">return</span> scale_score.sort_values(by=<span class="string">&#x27;total_score&#x27;</span>,ascending=<span class="literal">False</span>)[:<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 추천 시스템 예시 - Avatar와 유사한 영화 10개 추천</span></span><br><span class="line">genre_recommendations(<span class="string">&#x27;Avatar&#x27;</span>,cosine_sim_df,t_df).reset_index(drop=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>Avatar</th>
      <th>score</th>
      <th>total_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Alien</td>
      <td>0.377950</td>
      <td>7.9</td>
      <td>10.252950</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Aliens</td>
      <td>0.427672</td>
      <td>7.7</td>
      <td>10.052672</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Star Trek Into Darkness</td>
      <td>0.420840</td>
      <td>7.4</td>
      <td>9.670840</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Gravity</td>
      <td>0.417716</td>
      <td>7.3</td>
      <td>9.542716</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Treasure Planet</td>
      <td>0.320138</td>
      <td>7.2</td>
      <td>9.320138</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Stargate: The Ark of Truth</td>
      <td>0.315874</td>
      <td>6.9</td>
      <td>8.940874</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Spaceballs</td>
      <td>0.329561</td>
      <td>6.7</td>
      <td>8.704561</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Starship Troopers</td>
      <td>0.298873</td>
      <td>6.7</td>
      <td>8.673873</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Silent Running</td>
      <td>0.410423</td>
      <td>6.3</td>
      <td>8.285423</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Space Dogs</td>
      <td>0.391469</td>
      <td>6.3</td>
      <td>8.266469</td>
    </tr>
  </tbody>
</table>
</div>


<h2 id="3번-문제"><a href="#3번-문제" class="headerlink" title="3번 문제"></a>3번 문제</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># #3. dataset 데이터 분석 및 연관 규칙 생성</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1) dataset 파일 분석</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2) 연도별 많이 / 적게 팔린 아이템은?</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3) member id 에 따른 구매 연(월, 요일)도, 아이템 분석</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># - 무슨 요일에 와서 구매를 많이 했을까?</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4) 연관 규칙 생성</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5) 연관 규칙에 따른 vip member에게 어떤 상품을 추천할까?</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -vip는 매출횟수가 가장 많은 상위 100명 </span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Date열을 datetime으로 변경</span></span><br><span class="line">g_df[<span class="string">&#x27;Date&#x27;</span>] = pd.to_datetime(g_df[<span class="string">&#x27;Date&#x27;</span>],<span class="built_in">format</span>=<span class="string">&#x27;%d-%m-%Y&#x27;</span> )</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># na.nan데이터 없음</span></span><br><span class="line">g_df.info()</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 38765 entries, 0 to 38764
Data columns (total 3 columns):
 #   Column           Non-Null Count  Dtype         
---  ------           --------------  -----         
 0   Member_number    38765 non-null  int64         
 1   Date             38765 non-null  datetime64[ns]
 2   itemDescription  38765 non-null  object        
dtypes: datetime64[ns](1), int64(1), object(1)
memory usage: 908.7+ KB
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2) 연도별 많이 / 적게 팔린 아이템은?</span></span><br><span class="line">g_df[<span class="string">&#x27;year&#x27;</span>] = g_df[<span class="string">&#x27;Date&#x27;</span>].dt.year</span><br><span class="line">a = g_df.pivot_table(index=<span class="string">&#x27;year&#x27;</span>,columns=<span class="string">&#x27;itemDescription&#x27;</span>,aggfunc=<span class="string">&#x27;count&#x27;</span>)[<span class="string">&#x27;Date&#x27;</span>]</span><br><span class="line">a.<span class="built_in">max</span>(axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>year
2014    1038.0
2015    1464.0
dtype: float64
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a</span><br><span class="line"><span class="comment">#Whole milk가 연도별로 가장 높고,</span></span><br><span class="line"><span class="comment">#bags, toilet cleaner 가장 적다</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>itemDescription</th>
      <th>Instant food products</th>
      <th>UHT-milk</th>
      <th>abrasive cleaner</th>
      <th>artif. sweetener</th>
      <th>baby cosmetics</th>
      <th>bags</th>
      <th>baking powder</th>
      <th>bathroom cleaner</th>
      <th>beef</th>
      <th>berries</th>
      <th>beverages</th>
      <th>bottled beer</th>
      <th>bottled water</th>
      <th>brandy</th>
      <th>brown bread</th>
      <th>butter</th>
      <th>butter milk</th>
      <th>cake bar</th>
      <th>candles</th>
      <th>candy</th>
      <th>canned beer</th>
      <th>canned fish</th>
      <th>canned fruit</th>
      <th>canned vegetables</th>
      <th>cat food</th>
      <th>...</th>
      <th>sparkling wine</th>
      <th>specialty bar</th>
      <th>specialty cheese</th>
      <th>specialty chocolate</th>
      <th>specialty fat</th>
      <th>specialty vegetables</th>
      <th>spices</th>
      <th>spread cheese</th>
      <th>sugar</th>
      <th>sweet spreads</th>
      <th>syrup</th>
      <th>tea</th>
      <th>tidbits</th>
      <th>toilet cleaner</th>
      <th>tropical fruit</th>
      <th>turkey</th>
      <th>vinegar</th>
      <th>waffles</th>
      <th>whipped/sour cream</th>
      <th>whisky</th>
      <th>white bread</th>
      <th>white wine</th>
      <th>whole milk</th>
      <th>yogurt</th>
      <th>zwieback</th>
    </tr>
    <tr>
      <th>year</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2014</th>
      <td>37.0</td>
      <td>160.0</td>
      <td>12.0</td>
      <td>13.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>67.0</td>
      <td>11.0</td>
      <td>177.0</td>
      <td>128.0</td>
      <td>109.0</td>
      <td>319.0</td>
      <td>504.0</td>
      <td>17.0</td>
      <td>323.0</td>
      <td>273.0</td>
      <td>126.0</td>
      <td>54.0</td>
      <td>34.0</td>
      <td>136.0</td>
      <td>266.0</td>
      <td>70.0</td>
      <td>8.0</td>
      <td>49.0</td>
      <td>109.0</td>
      <td>...</td>
      <td>25.0</td>
      <td>110.0</td>
      <td>37.0</td>
      <td>125.0</td>
      <td>20.0</td>
      <td>10.0</td>
      <td>23.0</td>
      <td>48.0</td>
      <td>155.0</td>
      <td>34.0</td>
      <td>13.0</td>
      <td>19.0</td>
      <td>12.0</td>
      <td>5.0</td>
      <td>364.0</td>
      <td>27.0</td>
      <td>29.0</td>
      <td>166.0</td>
      <td>365.0</td>
      <td>3.0</td>
      <td>213.0</td>
      <td>82.0</td>
      <td>1038.0</td>
      <td>640.0</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>2015</th>
      <td>23.0</td>
      <td>163.0</td>
      <td>10.0</td>
      <td>16.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>55.0</td>
      <td>6.0</td>
      <td>339.0</td>
      <td>199.0</td>
      <td>142.0</td>
      <td>368.0</td>
      <td>429.0</td>
      <td>21.0</td>
      <td>248.0</td>
      <td>261.0</td>
      <td>137.0</td>
      <td>39.0</td>
      <td>32.0</td>
      <td>83.0</td>
      <td>451.0</td>
      <td>46.0</td>
      <td>13.0</td>
      <td>33.0</td>
      <td>68.0</td>
      <td>...</td>
      <td>21.0</td>
      <td>100.0</td>
      <td>35.0</td>
      <td>115.0</td>
      <td>9.0</td>
      <td>1.0</td>
      <td>17.0</td>
      <td>52.0</td>
      <td>110.0</td>
      <td>35.0</td>
      <td>8.0</td>
      <td>8.0</td>
      <td>10.0</td>
      <td>NaN</td>
      <td>668.0</td>
      <td>53.0</td>
      <td>22.0</td>
      <td>114.0</td>
      <td>297.0</td>
      <td>5.0</td>
      <td>149.0</td>
      <td>94.0</td>
      <td>1464.0</td>
      <td>694.0</td>
      <td>36.0</td>
    </tr>
  </tbody>
</table>
<p>2 rows × 167 columns</p>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3</span></span><br><span class="line"></span><br><span class="line">g_df[<span class="string">&#x27;weekday&#x27;</span>] = g_df[<span class="string">&#x27;Date&#x27;</span>].dt.weekday<span class="comment">#0이 월요일 6이 일요일</span></span><br><span class="line">g_df[<span class="string">&#x27;month&#x27;</span>] = g_df[<span class="string">&#x27;Date&#x27;</span>].dt.month</span><br><span class="line">g_df.groupby(<span class="string">&#x27;Member_number&#x27;</span>).<span class="built_in">sum</span>(numeric_only=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>year</th>
      <th>weekday</th>
      <th>month</th>
    </tr>
    <tr>
      <th>Member_number</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1000</th>
      <td>26192</td>
      <td>43</td>
      <td>76</td>
    </tr>
    <tr>
      <th>1001</th>
      <td>24175</td>
      <td>35</td>
      <td>51</td>
    </tr>
    <tr>
      <th>1002</th>
      <td>16116</td>
      <td>46</td>
      <td>36</td>
    </tr>
    <tr>
      <th>1003</th>
      <td>16114</td>
      <td>24</td>
      <td>30</td>
    </tr>
    <tr>
      <th>1004</th>
      <td>42296</td>
      <td>27</td>
      <td>150</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>4996</th>
      <td>20150</td>
      <td>24</td>
      <td>90</td>
    </tr>
    <tr>
      <th>4997</th>
      <td>12088</td>
      <td>36</td>
      <td>50</td>
    </tr>
    <tr>
      <th>4998</th>
      <td>4030</td>
      <td>4</td>
      <td>20</td>
    </tr>
    <tr>
      <th>4999</th>
      <td>32236</td>
      <td>58</td>
      <td>62</td>
    </tr>
    <tr>
      <th>5000</th>
      <td>14101</td>
      <td>27</td>
      <td>34</td>
    </tr>
  </tbody>
</table>
<p>3898 rows × 3 columns</p>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">g_df_dummies = pd.get_dummies(g_df,columns=[<span class="string">&#x27;weekday&#x27;</span>,<span class="string">&#x27;month&#x27;</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">g_df_dummies.groupby(<span class="string">&#x27;Member_number&#x27;</span>).<span class="built_in">sum</span>(numeric_only=<span class="literal">True</span>).iloc[:,<span class="number">1</span>:].<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment">#0이 월요일 6이 일요일</span></span><br><span class="line"><span class="comment">#요일은 상관 없이 구매량은 비슷하다.</span></span><br><span class="line"><span class="comment">#월도 크게 차이가 나는 편은 아니다.</span></span><br></pre></td></tr></table></figure>




<pre><code>weekday_0    5382
weekday_1    5558
weekday_2    5562
weekday_3    5620
weekday_4    5562
weekday_5    5551
weekday_6    5530
month_1      3324
month_2      2997
month_3      3133
month_4      3260
month_5      3408
month_6      3264
month_7      3300
month_8      3496
month_9      3059
month_10     3261
month_11     3254
month_12     3009
dtype: int64
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4,5 </span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 매출횟수 상위 100명에게 vip타이틀 부여</span></span><br><span class="line">b = g_df.groupby(<span class="string">&#x27;Member_number&#x27;</span>)[<span class="string">&#x27;itemDescription&#x27;</span>].size().sort_values(ascending=<span class="literal">False</span>)[:<span class="number">100</span>]</span><br><span class="line">g_df[<span class="string">&#x27;vip&#x27;</span>]= g_df[<span class="string">&#x27;Member_number&#x27;</span>].apply(<span class="keyword">lambda</span> x : <span class="number">1</span> <span class="keyword">if</span> x <span class="keyword">in</span> b.index <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#vip</span></span><br><span class="line">vip = g_df[g_df[<span class="string">&#x27;vip&#x27;</span>]==<span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<pre><code>/var/folders/pr/27tft1vj6396wqnj02ngz5p80000gn/T/ipykernel_28378/1060107556.py:2: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.
  b = g_df.groupby(&#39;Member_number&#39;)[&#39;itemDescription&#39;].size().sort_values(ascending=False)[:100]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mlxtend.preprocessing <span class="keyword">import</span> TransactionEncoder</span><br><span class="line"><span class="keyword">from</span> mlxtend.frequent_patterns <span class="keyword">import</span> apriori, association_rules</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#vip들이 구매한 상품을 그룹화(장바구니화)</span></span><br><span class="line">vip[<span class="string">&#x27;itemDescription&#x27;</span>] = vip[<span class="string">&#x27;itemDescription&#x27;</span>].apply(<span class="keyword">lambda</span> x : x+<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">vip_item = vip.groupby(<span class="string">&#x27;Member_number&#x27;</span>)[<span class="string">&#x27;itemDescription&#x27;</span>].<span class="built_in">sum</span>().to_list()</span><br><span class="line"></span><br><span class="line">items = [<span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(vip_item))]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(vip_item)):</span><br><span class="line">    items[i] = vip_item[i].split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TransactionEncoder(아이템 더미화)</span></span><br><span class="line">te = TransactionEncoder()</span><br><span class="line">te_result = te.fit(items).transform(items)</span><br><span class="line">df = pd.DataFrame(te_result, columns=te.columns_ ,dtype=<span class="string">&#x27;int&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># apriori</span></span><br><span class="line">apr_result = apriori(df, min_support=<span class="number">0.09</span>,  use_colnames=<span class="literal">True</span>)</span><br><span class="line">lift_based = association_rules(apr_result, metric=<span class="string">&#x27;lift&#x27;</span>, min_threshold=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # #lift와 confidence를 소수점 1,2자리까지만 출력 -&gt; 정렬</span></span><br><span class="line">lift_based[<span class="string">&#x27;lift&#x27;</span>] = lift_based[<span class="string">&#x27;lift&#x27;</span>].apply(<span class="keyword">lambda</span> x : <span class="string">f&#x27;<span class="subst">&#123;x:<span class="number">.1</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">lift_based[<span class="string">&#x27;confidence&#x27;</span>] = lift_based[<span class="string">&#x27;confidence&#x27;</span>].apply(<span class="keyword">lambda</span> x : <span class="string">f&#x27;<span class="subst">&#123;x:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#frozenset -&gt; object로 변경 (잘못된 공백값을 제거하기 위해)</span></span><br><span class="line">lift_based[<span class="string">&#x27;antecedents&#x27;</span>] = lift_based[<span class="string">&#x27;antecedents&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="string">&#x27;, &#x27;</span>.join(<span class="built_in">list</span>(x))).<span class="built_in">str</span>.replace(<span class="string">&#x27; &#x27;</span>,<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">lift_based[<span class="string">&#x27;consequents&#x27;</span>] = lift_based[<span class="string">&#x27;consequents&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="string">&#x27;, &#x27;</span>.join(<span class="built_in">list</span>(x))).<span class="built_in">str</span>.replace(<span class="string">&#x27; &#x27;</span>,<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># , 제거</span></span><br><span class="line">lift_based[<span class="string">&#x27;antecedents&#x27;</span>] = lift_based[<span class="string">&#x27;antecedents&#x27;</span>].apply(<span class="keyword">lambda</span> x : x <span class="keyword">if</span> x[<span class="number">0</span>] != <span class="string">&#x27;,&#x27;</span> <span class="keyword">else</span> x[<span class="number">1</span>:])</span><br><span class="line">lift_based[<span class="string">&#x27;consequents&#x27;</span>] = lift_based[<span class="string">&#x27;consequents&#x27;</span>].apply(<span class="keyword">lambda</span> x : x <span class="keyword">if</span> x[<span class="number">0</span>] != <span class="string">&#x27;,&#x27;</span> <span class="keyword">else</span> x[<span class="number">1</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment">#lift와 confidence로 정렬</span></span><br><span class="line">lift_sort = lift_based.sort_values(by = [<span class="string">&#x27;lift&#x27;</span>,<span class="string">&#x27;confidence&#x27;</span>], ascending=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># yogurt,tropicalfruit,rolls/buns &amp; wholemilk,othervegetables,cannedbeer</span></span><br><span class="line"><span class="comment"># 이것들은 여기 상점 VIP의 연관성이 가장 높은 물품들이다. 이 들의 동선을 살짝 멀찍이 띄어두고 </span></span><br><span class="line"><span class="comment"># 그 사이에 이벤트 상품들을 판매하면 다른 상품들의 매출도 늘 것 같다.</span></span><br><span class="line">lift_sort</span><br></pre></td></tr></table></figure>

<pre><code>/var/folders/pr/27tft1vj6396wqnj02ngz5p80000gn/T/ipykernel_28378/1298349058.py:6: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  vip[&#39;itemDescription&#39;] = vip[&#39;itemDescription&#39;].apply(lambda x : x+&#39;,&#39;)
/opt/homebrew/lib/python3.10/site-packages/mlxtend/frequent_patterns/fpcommon.py:111: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type
  warnings.warn(
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>antecedents</th>
      <th>consequents</th>
      <th>antecedent support</th>
      <th>consequent support</th>
      <th>support</th>
      <th>confidence</th>
      <th>lift</th>
      <th>leverage</th>
      <th>conviction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>8</th>
      <td>yogurt,tropicalfruit,rolls/buns</td>
      <td>othervegetables,cannedbeer,wholemilk</td>
      <td>0.11</td>
      <td>0.21</td>
      <td>0.09</td>
      <td>0.82</td>
      <td>3.9</td>
      <td>0.0669</td>
      <td>4.345000</td>
    </tr>
    <tr>
      <th>14</th>
      <td>yogurt,tropicalfruit,rolls/buns</td>
      <td>othervegetables,cannedbeer,wholemilk</td>
      <td>0.11</td>
      <td>0.21</td>
      <td>0.09</td>
      <td>0.82</td>
      <td>3.9</td>
      <td>0.0669</td>
      <td>4.345000</td>
    </tr>
    <tr>
      <th>20</th>
      <td>yogurt,tropicalfruit,rolls/buns</td>
      <td>othervegetables,cannedbeer,wholemilk</td>
      <td>0.11</td>
      <td>0.21</td>
      <td>0.09</td>
      <td>0.82</td>
      <td>3.9</td>
      <td>0.0669</td>
      <td>4.345000</td>
    </tr>
    <tr>
      <th>9</th>
      <td>othervegetables,cannedbeer,wholemilk</td>
      <td>yogurt,tropicalfruit,rolls/buns</td>
      <td>0.21</td>
      <td>0.11</td>
      <td>0.09</td>
      <td>0.43</td>
      <td>3.9</td>
      <td>0.0669</td>
      <td>1.557500</td>
    </tr>
    <tr>
      <th>15</th>
      <td>othervegetables,cannedbeer,wholemilk</td>
      <td>yogurt,tropicalfruit,rolls/buns</td>
      <td>0.21</td>
      <td>0.11</td>
      <td>0.09</td>
      <td>0.43</td>
      <td>3.9</td>
      <td>0.0669</td>
      <td>1.557500</td>
    </tr>
    <tr>
      <th>21</th>
      <td>othervegetables,cannedbeer,wholemilk</td>
      <td>yogurt,tropicalfruit,rolls/buns</td>
      <td>0.21</td>
      <td>0.11</td>
      <td>0.09</td>
      <td>0.43</td>
      <td>3.9</td>
      <td>0.0669</td>
      <td>1.557500</td>
    </tr>
    <tr>
      <th>6</th>
      <td>yogurt,rolls/buns,tropicalfruit,wholemilk</td>
      <td>othervegetables,cannedbeer</td>
      <td>0.10</td>
      <td>0.25</td>
      <td>0.09</td>
      <td>0.90</td>
      <td>3.6</td>
      <td>0.0650</td>
      <td>7.500000</td>
    </tr>
    <tr>
      <th>12</th>
      <td>tropicalfruit,wholemilk,yogurt,rolls/buns</td>
      <td>othervegetables,cannedbeer</td>
      <td>0.10</td>
      <td>0.25</td>
      <td>0.09</td>
      <td>0.90</td>
      <td>3.6</td>
      <td>0.0650</td>
      <td>7.500000</td>
    </tr>
    <tr>
      <th>17</th>
      <td>yogurt,rolls/buns,tropicalfruit,wholemilk</td>
      <td>othervegetables,cannedbeer</td>
      <td>0.10</td>
      <td>0.25</td>
      <td>0.09</td>
      <td>0.90</td>
      <td>3.6</td>
      <td>0.0650</td>
      <td>7.500000</td>
    </tr>
    <tr>
      <th>11</th>
      <td>othervegetables,cannedbeer</td>
      <td>yogurt,rolls/buns,tropicalfruit,wholemilk</td>
      <td>0.25</td>
      <td>0.10</td>
      <td>0.09</td>
      <td>0.36</td>
      <td>3.6</td>
      <td>0.0650</td>
      <td>1.406250</td>
    </tr>
    <tr>
      <th>18</th>
      <td>othervegetables,cannedbeer</td>
      <td>yogurt,rolls/buns,tropicalfruit,wholemilk</td>
      <td>0.25</td>
      <td>0.10</td>
      <td>0.09</td>
      <td>0.36</td>
      <td>3.6</td>
      <td>0.0650</td>
      <td>1.406250</td>
    </tr>
    <tr>
      <th>23</th>
      <td>othervegetables,cannedbeer</td>
      <td>tropicalfruit,wholemilk,yogurt,rolls/buns</td>
      <td>0.25</td>
      <td>0.10</td>
      <td>0.09</td>
      <td>0.36</td>
      <td>3.6</td>
      <td>0.0650</td>
      <td>1.406250</td>
    </tr>
    <tr>
      <th>0</th>
      <td>yogurt,tropicalfruit,rolls/buns</td>
      <td>othervegetables,cannedbeer</td>
      <td>0.11</td>
      <td>0.25</td>
      <td>0.09</td>
      <td>0.82</td>
      <td>3.3</td>
      <td>0.0625</td>
      <td>4.125000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>yogurt,tropicalfruit,rolls/buns</td>
      <td>othervegetables,cannedbeer</td>
      <td>0.11</td>
      <td>0.25</td>
      <td>0.09</td>
      <td>0.82</td>
      <td>3.3</td>
      <td>0.0625</td>
      <td>4.125000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>yogurt,tropicalfruit,rolls/buns</td>
      <td>othervegetables,cannedbeer</td>
      <td>0.11</td>
      <td>0.25</td>
      <td>0.09</td>
      <td>0.82</td>
      <td>3.3</td>
      <td>0.0625</td>
      <td>4.125000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>othervegetables,cannedbeer</td>
      <td>yogurt,tropicalfruit,rolls/buns</td>
      <td>0.25</td>
      <td>0.11</td>
      <td>0.09</td>
      <td>0.36</td>
      <td>3.3</td>
      <td>0.0625</td>
      <td>1.390625</td>
    </tr>
    <tr>
      <th>3</th>
      <td>othervegetables,cannedbeer</td>
      <td>yogurt,tropicalfruit,rolls/buns</td>
      <td>0.25</td>
      <td>0.11</td>
      <td>0.09</td>
      <td>0.36</td>
      <td>3.3</td>
      <td>0.0625</td>
      <td>1.390625</td>
    </tr>
    <tr>
      <th>5</th>
      <td>othervegetables,cannedbeer</td>
      <td>yogurt,tropicalfruit,rolls/buns</td>
      <td>0.25</td>
      <td>0.11</td>
      <td>0.09</td>
      <td>0.36</td>
      <td>3.3</td>
      <td>0.0625</td>
      <td>1.390625</td>
    </tr>
    <tr>
      <th>10</th>
      <td>yogurt,cannedbeer,othervegetables</td>
      <td>rolls/buns,tropicalfruit,wholemilk</td>
      <td>0.13</td>
      <td>0.22</td>
      <td>0.09</td>
      <td>0.69</td>
      <td>3.1</td>
      <td>0.0614</td>
      <td>2.535000</td>
    </tr>
    <tr>
      <th>16</th>
      <td>yogurt,cannedbeer,othervegetables</td>
      <td>rolls/buns,tropicalfruit,wholemilk</td>
      <td>0.13</td>
      <td>0.22</td>
      <td>0.09</td>
      <td>0.69</td>
      <td>3.1</td>
      <td>0.0614</td>
      <td>2.535000</td>
    </tr>
    <tr>
      <th>22</th>
      <td>yogurt,cannedbeer,othervegetables</td>
      <td>rolls/buns,tropicalfruit,wholemilk</td>
      <td>0.13</td>
      <td>0.22</td>
      <td>0.09</td>
      <td>0.69</td>
      <td>3.1</td>
      <td>0.0614</td>
      <td>2.535000</td>
    </tr>
    <tr>
      <th>7</th>
      <td>rolls/buns,tropicalfruit,wholemilk</td>
      <td>yogurt,cannedbeer,othervegetables</td>
      <td>0.22</td>
      <td>0.13</td>
      <td>0.09</td>
      <td>0.41</td>
      <td>3.1</td>
      <td>0.0614</td>
      <td>1.472308</td>
    </tr>
    <tr>
      <th>13</th>
      <td>rolls/buns,tropicalfruit,wholemilk</td>
      <td>yogurt,cannedbeer,othervegetables</td>
      <td>0.22</td>
      <td>0.13</td>
      <td>0.09</td>
      <td>0.41</td>
      <td>3.1</td>
      <td>0.0614</td>
      <td>1.472308</td>
    </tr>
    <tr>
      <th>19</th>
      <td>rolls/buns,tropicalfruit,wholemilk</td>
      <td>yogurt,cannedbeer,othervegetables</td>
      <td>0.22</td>
      <td>0.13</td>
      <td>0.09</td>
      <td>0.41</td>
      <td>3.1</td>
      <td>0.0614</td>
      <td>1.472308</td>
    </tr>
  </tbody>
</table>
</div>


<h2 id="4번-문제"><a href="#4번-문제" class="headerlink" title="4번 문제"></a>4번 문제</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4. 와인의 화학 조성을 사용하여 와인의 종류를 예측하기 위한 데이터이다. load_wine() 명령으로 로드하며 다음과 같이 구성되어 있다. </span></span><br><span class="line"><span class="comment"># 와인의 종류를 예측할 수 있는 모델을 생성하시오.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_wine</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">wine = load_wine()</span><br><span class="line"></span><br><span class="line">w_df = wine.data</span><br><span class="line">w_target = wine.target</span><br><span class="line"></span><br><span class="line"><span class="comment">#표준화</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">w_df = scaler.fit_transform(w_df)</span><br><span class="line">xtrain , xtest, ytrain, ytest = train_test_split(w_df,w_target,test_size= <span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#테스트가 값이 3종류라 카테고리화하기</span></span><br><span class="line">ytrain = to_categorical(ytrain)</span><br><span class="line">ytest = to_categorical(ytest)</span><br><span class="line"></span><br><span class="line">model=Sequential()</span><br><span class="line">model.add(Dense(<span class="number">3</span>, input_dim=<span class="number">13</span>, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">hist = model.fit(xtrain, ytrain, epochs=<span class="number">300</span>, batch_size=<span class="number">1</span>, validation_data=(xtest, ytest))</span><br><span class="line"></span><br><span class="line">epochs = <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(hist.history[<span class="string">&#x27;accuracy&#x27;</span>]) + <span class="number">1</span>)</span><br><span class="line">plt.plot(epochs, hist.history[<span class="string">&#x27;loss&#x27;</span>])</span><br><span class="line">plt.plot(epochs, hist.history[<span class="string">&#x27;val_loss&#x27;</span>])</span><br><span class="line">plt.title(<span class="string">&#x27;model loss&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>], loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># accuracy: 1.0000</span></span><br></pre></td></tr></table></figure>

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2022-11-25T15:00:00.000Z" title="2022. 11. 26. 오전 12:00:00">2022-11-26</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2022-11-26T15:00:00.000Z" title="2022. 11. 27. 오전 12:00:00">2022-11-27</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">3분안에 읽기 (약 420 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/11/26/Study_folder/Pandas,%20Numpy/2022-11-26-pd.pivot_table()/">데이터프레임 필터링 피봇테이블 - pd.pivot_table()</a></h1><div class="content"><h2 id="pivot-table"><a href="#pivot-table" class="headerlink" title="pivot_table()"></a>pivot_table()</h2><ul>
<li>아래와 같은 데이터 프레임이 있을 때, ‘Age_category’를 엑셀 필터를 거는것 처럼 만들고 싶다면, 다양한 방법이 있지만 가장 보기 편한 것 중 하나가 피봇테이블이다.</li>
<li>엑셀이나 스프레드시트의 피봇테이블과 유사하며 방법도 그리 어렵지 않다.</li>
<li>아래는 간단한 예시입니다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#index는 맨 왼쪽에 gropby할 열이름을 입력,</span></span><br><span class="line"><span class="comment">#columns는 보고 싶은 열의 이름을 입력,</span></span><br><span class="line"><span class="comment">#fill_value는 연산하고 싶은 열의 이름을 입력(int,float타입만 가능) - 입력하지 않으면 default값으로 수치화할 수 있는 열들이 들어감.</span></span><br><span class="line"><span class="comment">#aggfunc 에는 연산하고 싶은 방식 선택 ex) &#x27;count&#x27;, &#x27;sum&#x27;, &#x27;mean&#x27;, np.mean 등이 있다.</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;Age&#x27;</span> : <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">101</span>,<span class="number">10</span>),<span class="string">&#x27;Number&#x27;</span> :<span class="built_in">range</span>(<span class="number">100</span>,<span class="number">1001</span>,<span class="number">100</span>)&#125;)</span><br><span class="line">df[<span class="string">&#x27;Age_category&#x27;</span>] = pd.qcut(df[<span class="string">&#x27;Age&#x27;</span>],<span class="number">3</span>,labels=[<span class="string">&#x27;young&#x27;</span>,<span class="string">&#x27;medium&#x27;</span>,<span class="string">&#x27;older&#x27;</span>]) </span><br><span class="line">df</span><br><span class="line">	Age	Number	Age_category</span><br><span class="line"><span class="number">0</span>	<span class="number">1</span>	<span class="number">100</span>	    young</span><br><span class="line"><span class="number">1</span>	<span class="number">11</span>	<span class="number">200</span>	    young</span><br><span class="line"><span class="number">2</span>	<span class="number">21</span>	<span class="number">300</span>	    young</span><br><span class="line"><span class="number">3</span>	<span class="number">31</span>	<span class="number">400</span> 	    young</span><br><span class="line"><span class="number">4</span>	<span class="number">41</span>	<span class="number">500</span>	    medium</span><br><span class="line"><span class="number">5</span>	<span class="number">51</span>	<span class="number">600</span>	    medium</span><br><span class="line"><span class="number">6</span>	<span class="number">61</span>	<span class="number">700</span>	    medium</span><br><span class="line"><span class="number">7</span>	<span class="number">71</span>	<span class="number">800</span>	    older</span><br><span class="line"><span class="number">8</span>	<span class="number">81</span>	<span class="number">900</span>	    older</span><br><span class="line"><span class="number">9</span>	<span class="number">91</span>	<span class="number">1000</span>	    older</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">df.pivot_table(index=<span class="string">&#x27;Age&#x27;</span>,columns=<span class="string">&#x27;Age_category&#x27;</span>,aggfunc=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Number</span><br><span class="line">Age_category	young   	medium	    older</span><br><span class="line">Age			</span><br><span class="line"><span class="number">1</span>	            <span class="number">100.0</span>	    NaN           NaN</span><br><span class="line"><span class="number">11</span>	            <span class="number">200.0</span>	    NaN           NaN</span><br><span class="line"><span class="number">21</span>	            <span class="number">300.0</span>	    NaN           NaN</span><br><span class="line"><span class="number">31</span>	            <span class="number">400.0</span>	    NaN           NaN</span><br><span class="line"><span class="number">41</span>                NaN         <span class="number">500.0</span>       NaN</span><br><span class="line"><span class="number">51</span>	            NaN         <span class="number">600.0</span>       NaN</span><br><span class="line"><span class="number">61</span>	            NaN         <span class="number">700.0</span>       NaN</span><br><span class="line"><span class="number">71</span>                NaN       NaN           <span class="number">800.0</span></span><br><span class="line"><span class="number">81</span>                NaN       NaN           <span class="number">900.0</span></span><br><span class="line"><span class="number">91</span>                NaN       NaN           <span class="number">1000.0</span></span><br></pre></td></tr></table></figure>

<h2 id="아래는-판다스-메뉴얼의-예시입니다"><a href="#아래는-판다스-메뉴얼의-예시입니다" class="headerlink" title="아래는 판다스 메뉴얼의 예시입니다"></a>아래는 판다스 메뉴얼의 예시입니다</h2><ul>
<li>공식 메뉴얼을 보면, 확실히 이해 할 수 있을겁니다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">&quot;A&quot;</span>: [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;foo&quot;</span>,</span><br><span class="line">                         <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;bar&quot;</span>],</span><br><span class="line">                   <span class="string">&quot;B&quot;</span>: [<span class="string">&quot;one&quot;</span>, <span class="string">&quot;one&quot;</span>, <span class="string">&quot;one&quot;</span>, <span class="string">&quot;two&quot;</span>, <span class="string">&quot;two&quot;</span>,</span><br><span class="line">                         <span class="string">&quot;one&quot;</span>, <span class="string">&quot;one&quot;</span>, <span class="string">&quot;two&quot;</span>, <span class="string">&quot;two&quot;</span>],</span><br><span class="line">                   <span class="string">&quot;C&quot;</span>: [<span class="string">&quot;small&quot;</span>, <span class="string">&quot;large&quot;</span>, <span class="string">&quot;large&quot;</span>, <span class="string">&quot;small&quot;</span>,</span><br><span class="line">                         <span class="string">&quot;small&quot;</span>, <span class="string">&quot;large&quot;</span>, <span class="string">&quot;small&quot;</span>, <span class="string">&quot;small&quot;</span>,</span><br><span class="line">                         <span class="string">&quot;large&quot;</span>],</span><br><span class="line">                   <span class="string">&quot;D&quot;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">                   <span class="string">&quot;E&quot;</span>: [<span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">9</span>]&#125;)</span><br><span class="line">df</span><br><span class="line">	A	B	C	D	E</span><br><span class="line"><span class="number">0</span>	foo	one	small	<span class="number">1</span>	<span class="number">2</span></span><br><span class="line"><span class="number">1</span>	foo	one	large	<span class="number">2</span>	<span class="number">4</span></span><br><span class="line"><span class="number">2</span>	foo	one	large	<span class="number">2</span>	<span class="number">5</span></span><br><span class="line"><span class="number">3</span>	foo	two	small	<span class="number">3</span>	<span class="number">5</span></span><br><span class="line"><span class="number">4</span>	foo	two	small	<span class="number">3</span>	<span class="number">6</span></span><br><span class="line"><span class="number">5</span>	bar	one	large	<span class="number">4</span>	<span class="number">6</span></span><br><span class="line"><span class="number">6</span>	bar	one	small	<span class="number">5</span>	<span class="number">8</span></span><br><span class="line"><span class="number">7</span>	bar	two	small	<span class="number">6</span>	<span class="number">9</span></span><br><span class="line"><span class="number">8</span>	bar	two	large	<span class="number">7</span>	<span class="number">9</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#pivot_table</span></span><br><span class="line">pd.pivot_table(df, values=[<span class="string">&#x27;D&#x27;</span>, <span class="string">&#x27;E&#x27;</span>], index=[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;C&#x27;</span>],</span><br><span class="line">                    aggfunc=&#123;<span class="string">&#x27;D&#x27;</span>: np.mean,</span><br><span class="line">                             <span class="string">&#x27;E&#x27;</span>: [<span class="built_in">min</span>, <span class="built_in">max</span>, np.mean]&#125;)</span><br><span class="line">                             </span><br><span class="line">            D           E</span><br><span class="line">            mean        <span class="built_in">max</span>	 mean	    <span class="built_in">min</span></span><br><span class="line">A     C				</span><br><span class="line">bar   large <span class="number">5.500000</span>	<span class="number">9</span>   	<span class="number">7.500000</span>	<span class="number">6</span></span><br><span class="line">small            <span class="number">5.500000</span>	<span class="number">9</span>   	<span class="number">8.500000</span>	<span class="number">8</span></span><br><span class="line">foo	large   <span class="number">2.000000</span>	<span class="number">5</span>   	<span class="number">4.500000</span>	<span class="number">4</span></span><br><span class="line">small           <span class="number">2.333333</span>	<span class="number">6</span>   	<span class="number">4.333333</span>	<span class="number">2</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2022-11-24T15:00:00.000Z" title="2022. 11. 25. 오전 12:00:00">2022-11-25</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2022-11-24T15:00:00.000Z" title="2022. 11. 25. 오전 12:00:00">2022-11-25</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">1분안에 읽기 (약 223 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/11/25/Study_folder/Pandas,%20Numpy/2022-11-25-cut()-qcut()/">범주화(카테고리화) - pd.cut(), pd.qcut()</a></h1><div class="content"><blockquote>
<p>cut, qcut은 수치형 데이터를 카테고리화 하는 함수입니다.</p>
</blockquote>
<h2 id="pd-cut"><a href="#pd-cut" class="headerlink" title="pd.cut()"></a>pd.cut()</h2><ul>
<li>pd.cut()은 구간(bin, 나누는 개수)을 특정 범위로 설정 가능합니다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;Age&#x27;</span> : <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">100</span>,<span class="number">7</span>)&#125;)</span><br><span class="line">df[<span class="string">&#x27;Age_category&#x27;</span>] = pd.cut(df[<span class="string">&#x27;Age&#x27;</span>],bins=[<span class="number">0</span>,<span class="number">10</span>,<span class="number">70</span>,<span class="number">100</span>],labels=[<span class="string">&#x27;young&#x27;</span>,<span class="string">&#x27;medium&#x27;</span>,<span class="string">&#x27;older&#x27;</span>]) </span><br><span class="line">df</span><br><span class="line"><span class="comment"># 출력 결과</span></span><br><span class="line">	Age	Age_category</span><br><span class="line"><span class="number">0</span>	<span class="number">1</span>	young</span><br><span class="line"><span class="number">1</span>	<span class="number">8</span>	young</span><br><span class="line"><span class="number">2</span>	<span class="number">15</span>	medium</span><br><span class="line"><span class="number">3</span>	<span class="number">22</span>	medium</span><br><span class="line"><span class="number">4</span>	<span class="number">29</span>	medium</span><br><span class="line"><span class="number">5</span>	<span class="number">36</span>	medium</span><br><span class="line"><span class="number">6</span>	<span class="number">43</span>	medium</span><br><span class="line"><span class="number">7</span>	<span class="number">50</span>	medium</span><br><span class="line"><span class="number">8</span>	<span class="number">57</span>	medium</span><br><span class="line"><span class="number">9</span>	<span class="number">64</span>	medium</span><br><span class="line"><span class="number">10</span>	<span class="number">71</span>	older</span><br><span class="line"><span class="number">11</span>	<span class="number">78</span>	older</span><br><span class="line"><span class="number">12</span>	<span class="number">85</span>	older</span><br><span class="line"><span class="number">13</span>	<span class="number">92</span>	older</span><br><span class="line"><span class="number">14</span>	<span class="number">99</span>	older</span><br></pre></td></tr></table></figure>

<h2 id="pd-qcut"><a href="#pd-qcut" class="headerlink" title="pd.qcut()"></a>pd.qcut()</h2><ul>
<li>qcut은 cut과 달리 구간 설정을 정확히 나눌 경우 사용되어 임의의 계산이 필요 없다.</li>
<li>아래와 같은 경우 라벨이 young은 df[‘Age’].quantile(0.33)이하의 값이고 medium은 0.33 ~ 0.66이며, older는 df[‘Age’].quantile(0.66)의 값이 bin이다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;Age&#x27;</span> : <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">100</span>,<span class="number">7</span>)&#125;)</span><br><span class="line">df[<span class="string">&#x27;Age_category&#x27;</span>] = pd.qcut(df[<span class="string">&#x27;Age&#x27;</span>],<span class="number">3</span>,labels=[<span class="string">&#x27;young&#x27;</span>,<span class="string">&#x27;medium&#x27;</span>,<span class="string">&#x27;older&#x27;</span>]) </span><br><span class="line">df</span><br><span class="line"><span class="comment"># 만약,duplicates오류가 나온다면 duplicates=&#x27;drop&#x27;을 추가 옵션에 넣으면 됩니다.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 출력 결과</span></span><br><span class="line">	Age	Age_category</span><br><span class="line"><span class="number">0</span>	<span class="number">1</span>	young</span><br><span class="line"><span class="number">1</span>	<span class="number">8</span>	young</span><br><span class="line"><span class="number">2</span>	<span class="number">15</span>	young</span><br><span class="line"><span class="number">3</span>	<span class="number">22</span>	young</span><br><span class="line"><span class="number">4</span>	<span class="number">29</span>	young</span><br><span class="line"><span class="number">5</span>	<span class="number">36</span>	medium</span><br><span class="line"><span class="number">6</span>	<span class="number">43</span>	medium</span><br><span class="line"><span class="number">7</span>	<span class="number">50</span>	medium</span><br><span class="line"><span class="number">8</span>	<span class="number">57</span>	medium</span><br><span class="line"><span class="number">9</span>	<span class="number">64</span>	medium</span><br><span class="line"><span class="number">10</span>	<span class="number">71</span>	older</span><br><span class="line"><span class="number">11</span>	<span class="number">78</span>	older</span><br><span class="line"><span class="number">12</span>	<span class="number">85</span>	older</span><br><span class="line"><span class="number">13</span>	<span class="number">92</span>	older</span><br><span class="line"><span class="number">14</span>	<span class="number">99</span>	older</span><br></pre></td></tr></table></figure>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/7/">이전</a></div><div class="pagination-next"><a href="/page/9/">다음</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/7/">7</a></li><li><a class="pagination-link is-current" href="/page/8/">8</a></li><li><a class="pagination-link" href="/page/9/">9</a></li><li><a class="pagination-link" href="/page/10/">10</a></li></ul></nav></div><div class="column column-left is-3-tablet is-3-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.png" alt="Inhwan Cho"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Inhwan Cho</p><p class="is-size-6 is-block">Inhwan&#039;s Digital Space</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Seoul in Korea</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">포스트</p><a href="/archives"><p class="title">98</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">카테고리</p><a href="/categories"><p class="title">12</p></a></div></div></nav><div class="level"><a class="level-item button is-info is-outlined is-rounded" href="/" target="_self" rel="noopener">Home</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="external nofollow noopener noreferrer" title="Github" href="https://github.com/InhwanCho"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="external nofollow noopener noreferrer" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">카테고리</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Blog/"><span class="level-start"><span class="level-item">Blog</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Colab/"><span class="level-start"><span class="level-item">Colab</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Command/"><span class="level-start"><span class="level-item">Command</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Errors/"><span class="level-start"><span class="level-item">Errors</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/Git/"><span class="level-start"><span class="level-item">Git</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Mac/"><span class="level-start"><span class="level-item">Mac</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/OpenCV/"><span class="level-start"><span class="level-item">OpenCV</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/Portfolio/"><span class="level-start"><span class="level-item">Portfolio</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Project/"><span class="level-start"><span class="level-item">Project</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Review/"><span class="level-start"><span class="level-item">Review</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Samples/"><span class="level-start"><span class="level-item">Samples</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Study/"><span class="level-start"><span class="level-item">Study</span></span><span class="level-end"><span class="level-item tag">60</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">최근 글</h3><article class="media"><div class="media-content"><p class="date"><time datetime="2023-01-18T06:23:12.874Z">2023-01-18</time></p><p class="title"><a href="/2023/01/18/Study_folder/OpneCV/2023-01-18-haarscascade/">OpenCV 얼굴, 눈 등 특정 객체 검출</a></p><p class="categories"><a href="/categories/OpenCV/">OpenCV</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2023-01-18T05:11:30.071Z">2023-01-18</time></p><p class="title"><a href="/2023/01/18/Study_folder/OpneCV/2023-01-18-contour/">OpenCV 윤곽선 검출</a></p><p class="categories"><a href="/categories/OpenCV/">OpenCV</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2023-01-17T13:20:05.900Z">2023-01-17</time></p><p class="title"><a href="/2023/01/17/Study_folder/OpneCV/2023-01-18-opencv/">OpenCV 여러장의 이미지를 한장으로 출력하는 방법</a></p><p class="categories"><a href="/categories/OpenCV/">OpenCV</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2023-01-15T15:00:00.000Z">2023-01-16</time></p><p class="title"><a href="/2023/01/16/Blogs_folder/2023-01-16-customizeing/">Icarus 테마 커스터마이징</a></p><p class="categories"><a href="/categories/Errors/">Errors</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2023-01-14T15:00:00.000Z">2023-01-15</time></p><p class="title"><a href="/2023/01/15/Study_folder/OpneCV/2023-01-15-webcam-in-colab/">OpenCV 코랩에서 웹캠 사용하기</a></p><p class="categories"><a href="/categories/OpenCV/">OpenCV</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">아카이브</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">1월 2023</span></span><span class="level-end"><span class="level-item tag">25</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">12월 2022</span></span><span class="level-end"><span class="level-item tag">50</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">11월 2022</span></span><span class="level-end"><span class="level-item tag">23</span></span></a></li></ul></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">업데이트 소식 받기</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="구독"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/yo_img.jpg" alt="Inhwan&#039;s Digital Space" height="28"></a><p class="is-size-7"><span>&copy; 2023 InhwanCho</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="external nofollow noopener noreferrer">Icarus</a><br><span id="busuanzi_container_site_uv"><span id="busuanzi_value_site_uv">0</span>명의 사용자가 방문 함</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Download on GitHub" href="https://github.com/InhwanCho"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "이 웹 사이트는 귀하의 경험을 향상시키기 위해 Cookie를 사용합니다.",
          dismiss: "무시",
          allow: "허용",
          deny: "거부",
          link: "더 알아보기",
          policy: "Cookie 정책",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>